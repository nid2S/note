# Word to Vector
- 워드 임베딩을 위한 대표적 방법.
- CBOW 와 Skip-gram으로 나뉨.
- 학습속도를 위해 계층적 softmax나 네거티브샘플링 등의 기법들을 이용.

## CBOW
- 가까이 위치하는 단어는 비슷한 의미를 지닐거라는 아이디어에 기반.
- 근처의 단어들을 입력으로 중심 단어를 예측.  

- 근처의 단어(앞뒤로 윈도우사이즈 만큼)들의 원핫벡터(word_num(V), 1 | V.T)를 가져옴
- 각각의 벡터들과 입력-투사층(V*M)의 가중치를 곱함(해당 행을 가져옴, look up).
- 이 벡터들을 평균냄(전부더함/벡터의 전체개수(윈도우사이즈*2)).
  
- 평균벡터(M)를 투사-출력층 의 가중치와 곱해 원핫벡터와 차원이 동일한 벡터를 얻음.
- 그 벡터에 softmax를 취해 스코어 벡터를 얻음.
- 스코어벡터가 레이블(중심단어의 원핫벡터)에 가까워지게 훈련시킴(손실(cross-entropy)을 구함 > 역전파를 통해 가중치 업데이트. | 손실함수를 가중치에 대해 편미분 > )
- 두개의 가중치중 하나(떄로는 둘의 평균)를 임베딩벡터로 사용((문장의 단어 개수, 임베딩차원)크기를 갖게 됨). 

![image_CBOW](/NLP/image/w2v_cbow.png)

## skipGram
- 중심 단어로부터 주변 단어를 예측, 이 외엔 CBOW와 동일.
- 전반적으로 CBOW보다 성능이 좋다고 알려짐.

![image_SkipGram](/NLP/image/w2v_skipgram.png)


