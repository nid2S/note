# GPT
- GPT : Generative Pre Training of a language model 의 약어. 트랜스포머 디코더의 구조를 사용. open-AI 제작. (조건부)LM을 핵심으로 하고 있음.
- BPE(바이트 페어 인코딩) : 자주 함께 사용되는 char 를 하나의 묶음으로 사용(최소한의 단어). 워드 임베딩과 캐릭터 임베딩의 장점을 모두 가지고 있음. 
  (word)단어간의 유사도와 (char)처음보는 문자의 예측 모두가 가능함. GPT에서 그 이전의 모델들과 달리 사용됨.

## GPT-1
- GPT-1 : 문장간 관계 유추, 질의 응답, 문장유사도, 분류등에 뛰어난 성능을 보임. 
  언어 모델로 학습, 파인 튜닝(linear와 softmax, 추가적인 레이어 없이 적은 양의 레이블 만으로도 가능)두 단계를 거침. 트랜스포머의 디코더기반 모델. 
## GPT-2
- GPT-2 : 1과 달리 파인튜닝을 없애고, 사이즈가 커짐. 입력된 값과 수행해야할 task(다음단어, 번역, 응답 등)를 함께 입력받아 다음 단어를 출력. 
## GPT-3
- GPT-3 : 파인튜닝 제거(가능하긴 하나 필요 X)를 핵심으로 하고 있음. 제로샷(한번도 볼 필요 없음)이나 원샷(한장만 보면 됨)이 아닌 퓨샷러닝(몇장의 이미지만 사용)만을 이용. 
  여러 분야에서 뛰어난 성능을 보이나 단방향으로 학습해 문맥 파악에 약하다는 단점을 가짐.     

## GPT의 변형(디코더/자기회귀 모델)
- GPT(original) : Book Corpus데이터셋에서 사전훈련된 트랜스포머 아키텍쳐를 기반으로 하는 자기회귀모델.
- CTRL : GPT와 동일하나 제어코드의 개념을 추가함. 텍스트는 프롬프트와 텍스트생성에 영향을 미치는데 사용되는 제어코드중 하나(이상)에서 생성.
- Transformer-XL : 일반 GPT와 동일하나 두개의 연속 세그먼트에 대한 반복 매커니즘을 도입함. 이전 세그먼트의 은닉상태를 현 입력에 연결해 어텐션 점수를 계산한 뒤,
  위치임베딩을 위치상대임베딩으로 변경하고 어텐션스코어의 계산방식을 약간 바꿈.
- REformer : 메모리 풋프린트와 계산시간을 줄이기 위한 많은 트릭이 있는 자동회귀 트랜스포머 모델.
  축 위치 인코딩, LSH어텐션, backward pass동안 리버시블 트랜스포머층을 사용해 각 레이어의 중간결과를 저장/재계산함, 전체배치 > 청크 의 트릭을 사용함.
- XLNet : 자기회귀모델 기반 훈련. 모델이 마지막 n개의 토큰을 사용해 n+1의 토큰을 예측하게 하며, 이 과정을 모두 마스크에서 진행함.  
