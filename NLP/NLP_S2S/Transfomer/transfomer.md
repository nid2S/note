# 트랜스포머
- Transformer : 인코더 디코더 구조를 발전시킨 딥러닝 모델. RNN 대신 Self-Attention만을 이용하며, 현재 대부분의 SOTA모델이 이 구조에 기반해 구현되어있음.
  어텐션만으로 입력데이터에서 중요한 정보를 찾아내 인코딩. 어텐션 뿐 아니라 다양한 기술을 사용해 성능을 올림.
- 특징 : 인코더-디코더 구조, 안코더/디코더가 여러개 존재가능, 임베딩과 포지셔널 인코딩을 거친 행렬을 입력으로 받음, 병렬화(행렬연산을 통한)의 특징이 있음.
- VS RNN : RNN의 순차적인 계산을 단순 행렬연산으로 한번에 처리. 한번에 연산으로 중요정보를 각 단어에 인코딩. 

- PositionalEncoding : CNN도 RNN도 사용하지 않는 트랜스포머에서 위치정보의 주입을 위한 인코딩. 토큰의 상대적 위치와 관련된 정보를 얻을 수 있음. 
  sin과 cos을 활용하며, 항상 값이 -1 ~ 1 사이에 있게 되고, 학습 데이터보다 더 긴 문장이 들어와도 문제가 없게 됨. 
- 과정 : 각 단어의 임베딩 벡터가 모인 문장벡터행렬에 포지셔널인코딩 벡터(위치정보를 가짐)를 더하며, 이를 통해 같은 단어라도 위치에 따라 임베딩벡터의 값이 달라지게 됨. 
- 수식 : `PE(pos, 2i) = sin(pos/10000^(2i/d)), PE(pos, 2i) = cos(pos/10000^(2i/d))`.
- PaddingMask : 패딩된 토큰(<pad>등)을 어텐션에서 제외하기위해 마스킹(매우매우 작은 값을 곱해 학습에 반영되지 않게 함)함. 

- Multi Head Attention : d_model의 차원을 헤드 수로 나눠 병렬 어텐션을 수행. 다른 시각으로의 정보수집(사람의 말이 모호할 때가 많기 때문)이 가능하며, 
  모든 어텐션헤드를 연결 후 또 다른 가중치행렬을 곱해 최종 출력을 만들어 내고, 이게 다시 FC에 들어가 입력과 동일한 크기의 행렬이 생성됨.
- Position-wise Feed-Forward Neural Network : 두개의 선형변환으로 구성되어 있으며, 그 사이에 ReLU를 적용. `FFNN(x) = MAX(0, xW_1+b_1)W_2 + b_2`. 
  각 벡터가 멀티헤드어텐션을 지나 FFNN을 지나가면서도 원래 크기는 보존되며, 이는 한 인코더의 결과가 다음 인코더로 그대로 들어가기 때문임. Bidirectional로 학습됨.

- Encoder : 레이어층 개수(하이퍼파라미터)만큼 인코더층을 쌓음. 인코더층은 크게 두개의 서브층(셀프어텐션, 피드포워드)으로 구성되어있으며, [패딩마스크 > 인코더 > 드롭아웃/층정규화]순서로 진행됨.
- Decoder : 인코더 마지막층의 출력을 각 디코더층 연산에 사용하며, 학습시 교사강요를 사용. 크게 세개의 서브층(멀티헤드셀프 어텐션/인코더-디코더 어텐션/피드포워드)으로 구성되어있음.
- 룩-어헤드 마스크 : RNN없이 문장행렬 전체를 입력으로 받아, 미래 단어를 참고할 위험이 있어 이를 막기위해 사용. 첫번째 서브층의 어텐션스코어 행렬에 마스킹을 적용해 수행됨. 디코더에서 사용됨.
- 인코더-디코더 어텐션 : 멀티헤드어텐션을 수행하지만 셀프어텐션이 아님. K와 V는 인코더 마지막 층에서, Q는 디코더 첫번째 서브층의 결과로 얻음. 이후는 첫 층과 동일하며, 패딩마스크를 입력으로 받음.
- label-smoothing : 트랜스포머에서 softmax를 지나 확률을 추출하는 과정에서 성능을 높이기 위해 원핫인코딩 대신 사용하는 방법. 사용하는 방법. 0과 가깝지만 0이 아니고, 1과 가깝지만 1이 아닌 값으로 표현됨. 
  모델이 너무 학습데이터에 치중하지 못하도록 보완. 학습데이터가 메우 깔끔하고 예측해야 할 값이 명확할 경우는 별 효과 없을 수 있으나, 그렇지 않으면 큰 효과가 있음. 한가지 데이터에 둘 이상 레이블이 있다면 학습이 잘 되지 않아 사용됨.
