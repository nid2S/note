# Sequence-to-Sequence
- S2S(Seq2Seq) : 시퀀스를 입력받아 시퀀스를 출력하는, 방대한 시퀀스 데이터의 처리에 특화된 모델. 번역기나 챗봇등에 주로 사용됨. 인코더와 디코더 두개의 RNN셀로 구성되어 있음. 

- 인코더 : RNN. 입력된 시퀀스를 시점마다 입력받아, 컨텍스트벡터(RNN의 마지막 은닉상태, 단어의 정보가 압축된 하나의 벡터)를 생성 후 디코더로 넘겨줌.
- 디코더 : RNNLM. 인코더에서 입력받은 컨텍스트벡터로부터 바뀐 단어들을 하나씩 출력. 기본적으로 이전 시점의 출력만을 입력으로 함.

- 입력 : 인코더 입력, 디코더 입력, 디코더 출력의 세가지 입력를 사용하는데, 디코더 입력은 (<sos> + sent, 교사강요시 사용), 출력은 (sent + <eos>)형태임. 
- 훈련/테스트 : 디코더에 입력을 넣어 정답을 알려주고(교사강요), 출력을 예측하게 훈련시키며, 테스트때는 <sos>없이 컨택스트 벡터만 입력받아 예측을 수행함.

- 변형 : 컨택스트 벡터를 디코더의 초기 은닉상태로 사용(기본)/벡터를 매시점마다 하나의 입력으로 사용/어텐션을 통해 더욱 문맥을 반영하는 벡터 생성 후 매시점 입력으로 사용.

- 사용 : 
- 훈련시와 사용시는 사용법이 달라(입력부터 다름), 하나의 모델로 만들기 위해선 subclassinf API를 통해 predict혹은 call을 재정의 해야 함.
- 사용에도 decoder의 출력을 vocab사이즈 차원의 softmax를 vocabsize를 거쳐 예측값을 뽑아내다 eos가 나오거나 최대길이에 도달하면 종료하는 형식이라 같이 커스텀해야 함.
- 인코더 동작방식의 이해 : 
- 1. 최초 문장을 인코더에서 변환 > 컨텍스트 벡터 생성 > 디코더에서 해당 벡터를 RNN으로 돌리며 결과 생성 > eos뒤로 다 잘라서 반환?
- 2. 현재는? > 최초문장 인코더 > 은닉/셀 상태 반환 > 디코더에서 셀상태와 입력(sos)을 입력 > 아웃풋과 셀 상태를 반환 > 다시 디코더에 > eos까지 반복
            
- [참고](https://wikidocs.net/24996)

# S2S with Attention
- S2S with Attention : 고정된 크기의 컨텍스트벡터를 사용하지 않으며, 각 시점마다 동적으로 컨텍스트 벡터를 생성해 사용함.

- 순서 : 시퀀스를 입력으로 받아 모든 스텝에서 은닉상태를 출력하고, 이 은닉상태들을 FullyConnect(Dense)에 넣음 ->
  이 후 인코더의 마지막 출력상태를 Dense에 넣고, 두 결과를 더함 ->
  이를 하이퍼볼릭 탄젠트(tanh) 활성화 함수에 넣어 시퀀스 길이에 맞게(각 스텝마다)score를 얻음 ->
  여기에 Softmax를 적용해 Attention weight를 얻어, 처음의 은닉상태들과 곱해(어텐션)모두 더하는 식으로 어텐션값(=컨텍스트벡터)을 획득함 ->
  이렇게 얻은 컨텍스트 벡터와 <start>를 연결(concatenate)후 tanh함수를 적용, 첫번째 아웃풋을 얻음 ->
  그렇게 얻은 아웃풋을 FC층에 넣어 인코더의 은닉상태들과 어텐션, 그렇게 얻은 컨텍스트벡터와 마지막 아웃풋을 어텐션, 다음 아웃풋을 얻음 ->
  <eos>토큰이 나오거나 최대길이에 도달할 때 까지 반복함. -> concat 부분과 예측을 생성하는 부분에서 공부 필요.
- 학습 : 학습시에는 교사강요(teacher forcing)를 사용함.

- 식 : Dense(tanh(Dense(h1, h2, ..., h)+Dense(output))) -> sum(Softmax * (h1, h2, ..., h))(각 벡터들을 더함) 
  -> tanh(concat(context_vector, last_output)) -> 반복.
