# 언어 모델
- 언어 모델(LM) : 문장에 확률을 할당하는 모델. 문장에 대해 그 문장이 적절한지, 말이 되는지(문법,문맥 등) 등을 판단. 
  확률 할당을 위해 보편적으로 이전 단어가 주어지면 다음 단어를 예측하도록 함. 혹은 빈칸 추론을 시킴. 통계를 이용하거나 ANN 을 이용해 만들 수 있음.
  p("I love you") = p("I", "love", "you") = P("you" | "I", "love") * P("love" | "I") * P("I")같은 과정을 거쳐 문장의 가능도(likelihood)를 측정함.
- LM 특징 : 모델 학습에 따로 레이블링 할 필요 없다는 장점을 가짐. 순방향과 역방향이 있으며 둘을 모두 합친걸 biLM 이라고 한다. 
  
- 언어 모델링 : 주어진 단어들로 아직 모르는 단어를 예측하는 것.  
- 언어모델의 기능 : 기계번역에서 문맥을 파악하거나, 오타를 교정하거나, 음성인식을 더 잘 되게 도와주는 등 보다 적절한 문장을 판단함.  
- 조건부 확률의 연쇄 법칙 : 조건부 확률(사건 B가 일어난 경우 사건 A가 일어날 확률)이 가지는 특징. P(x1,x2...xn)=P(x1)P(x2|x1)...P(xn|x1...xn−1). 
  이를 문장에 적용해 다음 단어에 대한 예측 확률을 모두 곱하여 문장의 확률을 구할 수 있음. 
  
- N-그램 : n개의 토큰이 연속적으로 모인것. 한개면 유니그램, 두개면 바이그램등으로 지칭. 문맥의 파악이 가능. 문장 벡터화의 방법 중 하나인 bag of words 의 단점을 해소 할 수 있음.

- SLM(Statistical Language Model) : 통계적 언어모델. 카운트에 기반해 확률을 구함(count(현 문장+예측할 단어)/count(현 문장)). 
  데이터 부족시 희소문제(데이터 부족으로 정확한 모델링이 불가능한 문제)가 발생하며 이를 완화하기 위해 n-gram,스무딩,백오프등 일반화 기법이 있으나 결국 NN 언어 모델로 넘어가게 됨.  
- N-gram 언어 모델 : SLM 의 일종. 일부 단어(N-1 개)만 고려. 몇개의 단어만 고려하기에 의도와 다른 단어를 고를 수 있다는 것과 여전히 남아있는 희소 문제, 
  n을 몇으로 선택 할지(trade-off) 등의 문제를 가짐. 훈련에 사용되는 코퍼스에 따라 성능이 비약적으로 달라진다는 SLM 의 특징또한 가지고 있음.

- 한국어 언어모델 : 한국어는 다음 단어의 예측이 훨싼 까다로운데, 어순이 크게 중요하지 않고, 교착어(형태소가 붙어 단어 형성)이며, 띄어쓰기가 잘 지켜지지 않는 한국어의 특성 때문이다.

###### 질의응답
- 질의응답 : 질문에 대한 답변 출력. 
- 메모리 네트워크 : 두개의 입력(스토리/질문 문장)을 받아 스토리는 두개(A,C), 질문은 하나(B)의 임베딩층을 통해 각각 단어임베딩이 되고,
  이것들에 어텐션(V-B,Q-C,K-A)이 적용됨. 이를 질문표현(질문문장임베딩)과 연결해 LSTM과 밀집층의 입력으로 사용.
- 이를 이용해 스토리 단어들과 질문 단어의 유사도를 구해 가장 자연스러운 답변을 생성하는 과정을 거쳐 질의응답기능을 만들 수 있음.
- 챗봇 데이터 : [urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv", filename="ChatBotData.csv")]
