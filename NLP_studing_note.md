# have to study
> RNN -> Conv1d -> Attention -> transformer -> transferlearning

> -데이터 전처리
>     - bpe
>     - 형태소 분석기(konlpy, mecab)
>     .... 등

> variation of RNN
>      - RNN
>     -GRU
>     -LSTM

> variation of transformer
> encoder
>     - Bert
> decoder 
>     -gpt
> Bert + gpt -> Bart(generator)

> 그리고 2번째는 mnist-> 영화 감성 분류 (IMDB) -> 캐글 재난 분류(이건 바뀔수도)

# 텍스트 데이터
- 수치형, 범주형과 구분되는 또 다른 데이터 유형. 총 4종류 중 하나로 표현되며 알고리즘 적용 전에 전처리가 필요하다.
- 범주형 텍스트 데이터 : 고정된 목록으로 구성. 메뉴 중 하나를 고르는 설문조사 등에서 볼 수 있는 데이터.
- 범주에 의미를 연결가능한 문자열 : 말그대로. 텍스트 필드로 응답을 받는 설문조사 등에서 볼 수 있는 데이터.
- 구조화 된 문자열 : 주소나 장소, 이름, 날짜, 전화번호등 일정한 구조를 가지는 문자열.
- 텍스트 데이터 : 자유로운 형태의 절과 문장으로 구성된 데이터. 

# 전처리
- 어간 추출 : 일일히 어미를 찾아 제거하는 규칙기반 방식. 더 좋은 일반화를 위해 각 단어를 단어의 어간으로 표현해 같은 어간을 가진 단어를 구분하기(합치기) 위해 사용한다.
- 표제어 추출 : 알려진 단어의 형태사전을 사용하고 문장에서 단어의 역할을 고려하는 처리 방식. 둘다 단어의 일반 형태를 추출하는 정규화의 한 형태로 볼 수 있음. 어간 추출보다 훨씬 복잡한 처리를 거치지만 토큰 정규화시 더 좋은 성능을 냄.

- BOW(bag of wards) : 가장 간단하지만 효과적이고 널리 쓰이는 방법. 토큰화 > 어휘사전 구축(모든 어휘를 모으고 알파벳 순으로 번호를 매김) > 인코딩(단어가 얼마나 나오는지 셈, 희소행렬로 만들기 등) 의 과정을 거친다.
- tf-idf : 얼마나 의미있는 특성인지 계산해서 스케일을 조정하는 방식. 다른 문서보다 특정 문서에서 자주 나타나는 단어에 높은 가중치를 주는 방법.
- n-그램 : BOW 를 사용할 때 문맥까지 같이 고려하는 방법으로 옆에 있는 토큰 몇개를 함께 고려한다. 두개는 바이그램, 세개는 트라이 그램이며(혼자있는 토큰은 유니그램) 일반적으로 연속된 토큰을 n-그램이라고 한다.  

- 토픽 모델링 : 비지도 학습으로 분서를 하나 또는 그 이상의 토픽에 할당하는 작업.
- LDA(잠재 디리클레 할당) : 토픽 모델링시 사용하는 특정한 성분 분해 방법. 자주 자타나는 단어의 그룹을 찾음.


# Gensim
- 토픽 모델링과 자연어 처리등을 수행할 수 있게 함.



# NLTK
- 자연어 처리를 할 수 있게 함.
- nltk.download() : NLTK 데이터 세트 다운로드. 안에 'treeback' 식으로 데이터셋의 이름을 넣으면 그것만 다운로드한다.



# sklearn
- sklearn.feature_extraction.text.CountVectorizer() : BOW 표현을 하게 해주는 변환기 로드. .fit(문자열이 담긴 리스트)로 사용, .vocabulary_ 속성에서 반환된 {단어:등장횟수} 형태의 딕셔너리를 볼 수 있음.  tf-idf 와 함께 ngram_range=(연속 토큰 최소길이, 최대길이) 로 연속된 토큰을 고려할 수 있다. 보통은 하나만 하지만 많을 때 바이그램정도로 추가하면 도움이 된다.  
- Bow 표현을 만드려면 .transform(list), Scipy 희소 행렬로 저장되어 있으며, .get_feature_names()로 각 특성에 해당하는 단어들을 볼 수 있음. min_df 매개변수로 토큰이 나타날 최소 문서 개수를 지정할 수 있고, max_df 매개변수로 자주 나타나는 단어를 제거할 수 있다. stop_words 매개변수에 "english" 를 넣으면 내장된 불용어를 사용한다.
- sklearn.feature_extraction.text.TfidVectorizer(min_df=i) : 텍스트 데이터를 입력받아 BOW 특성 추출과 tf-idf 를 실행하고 L2정규화(스케일 조정)까지 적용하는 모델로드. 훈련데이터의 통걔적 속성을 사용하므로 파이프 라인을 이용한 그리드 서치를 해 주어야 한다. .idf_ 에서 훈련세트의 idf 값을 볼 수 있다. idf 값이 낮으면 자주 나타나 덜 중요하다 생각되는 것이다.

# spacy
- 영어와 독일어를 지원하는 NLP 파이썬 패키지. 표제어 추출 방식이 구현되어 있음. 
- python -m spacy download en 으로 언어의 모델을 먼저 다운받아야 함.
- spacy.load('en') : spacy 의 영어 모델 로드. 
- 모델(document) : 문서 토큰화. 찾은 표제어들 반환.

# nltk
- 포터 어간 추출기가 구현되어 있음.
- nlty.stem.PorterStemmer() : PorterStemmer 객체 생성.
- 객체.stem(토큰.norm_.lower()) > 토큰(어간) 찾기.

# KoNLpy
- 한글 분석을 가능하게 함.
- konlpy.tag.Okt() >  Okt 클래스 객체 생성. .morphs(text)로 형태소 분석이 가능하다.







