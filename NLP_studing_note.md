# have to study
> RNN -> Conv1d -> Attention -> transformer -> transferlearning

> -데이터 전처리
>     - bpe
>     - 형태소 분석기(konlpy, mecab)
>     .... 등

> variation of RNN - RNN, GRU, LSTM
> variation of transformer
> encoder - Bert
> decoder - gpt
> Bert + gpt -> Bart(generator)
> mnist-> 영화 감성 분류 (IMDB) -> 캐글 재난 분류
> 
> 음성인식(ASR), 음성이해(NLU), 음성합성(TTS)
> 챗봇 : 의도 분류(Intent Classification)는 개체명 인식(Named Entity Recognition)과 더불어 챗봇의 중요 모듈로서 사용
> RNN, DL chapter review

# 자연어 처리
- [참고자료](https://wikidocs.net/21667)

- 자연어와 컴퓨터간 상호작용에 대함. 인공지능과 컴퓨터 언어학의 주요 분야중 하나.
- 하이퍼 파라미터 : 사용자가 직접 값을 선택해 성능에 영향을 주는 매개 변수. 
- EDA(Exploratory Data Analysis) : 탐색적 데이터 분석. 데이터 내 값의 분포, 변수간 관계, 결측값 존재 유뮤 등을 확인하는 데이터 파악 과정.

- 자연어 처리 언어모델의 발전 과정 : 
  사전 훈련된 워드 임베딩 -> W2V, GloVe등 워드 임베딩 방법 등장, 임베딩 사용 방법 중 임베딩층을 랜덤 초기화 해 처음부터 학습하는 방법보다 이미 학습된 벡터를 가져오는 방법, 
  문맥 고려가 불가하단 문제점이 있었음.
  사전 훈련된 언어 모델 -> 우선 LSTM을 학습하고, 학습한 LSTM을 다른 태스크에 추가 학습. 레이블이 없는 데이터 사용. ELMo(양방향 언어모델 학습, 거기서 임베딩 값을 얻음)등도 존재.
  트랜스포머 -> 언어모델 학습시 LSTM 대신 트랜스포머 사용. 디코더를 총 12층 쌓은 후 방대한 텍스트 데이터를 이용해 GPT-1이 탄생.
  언어모델 구조 변경 -> 마스크드 언어 모델(입력 단어 집합의 15%를 랜덤으로 마스킹, 이 단어들을 예측하게 함)을 이용해 단방향 언어모델을 기존 예측할 단어를 이미 관측해버려 
  잘 쓰이지 않았던 양방향 언어모델로 바꿈. 

- TPU 사용 : TPU초기화(tf.config... | tf.tpu...) > Strategy셋팅(tf.distribute) > 모델정의시 strategy.scope내에서 이뤄져야 함(함수를 만들고 내부에서 호출, 컴파일 하는 방식).

### 전처리
- 순서 : 데이터 생성(로드) > 데이터 확인(길이/개수/null) > 토큰화 > 단어집합생성 > 정수인코딩 > 패딩 > 벡터화
- 정제 : 등장 빈도가 낮은 단어 제외, 문자가 아닌것을 제외, 소문자화, 불용어 처리, 본문(글자형식)이 아닌것을 제외, 구두점 제거, null 행 제거 등 

##### 토큰화 
- 토큰화 : 텍스트를 토큰이라는 작은 부분으로 분할하는 과정. 문장 토큰화와 단어(단어, 단어구, 형태소)토큰화로 나뉨.

- 인코딩 : 문서의 인코딩이 utf-8 등 처리가 불가한 것으로 되어있을 경우, s.encode("utf8").decode("ascii",'ignore')식으로 바뀌줘야 한다
- 토큰화의 기준 : 토큰화 중 토큰화의 기준(축약형 등)을 선택해야 하거나, 구두점/특수문자등이 고유한 뜻을 가지거나 단어의 해석에 도움이 되는 경우도 있어 단순 제외는 피해야 함.
- 토큰화 함수(단어) : 단어 내에 뛰어쓰기나 구두점이 있는 경우도 있어 토큰화 함수는 그런 단어들을 하나로 인식할 수 있는 능력 또한 필요함.  
- 토큰화 함수(문장) : 문장 토큰화의 경우에도 문장 내에 구두점이 있는 경우가 있기에 단순 구두점을 기준으로 구분해서는 안 됨.   

- 마침표의 처리를 위해 입력에 따라 두 클래스로 분류하는 이진(문장의 끝, 이외)분류기를 사용하기도 하며, 이를 위해 약어사전이 유용하게 쓰임.
- 한국어는 조사까지 분리해줄 필요가 있어 단순 공백으로는 구분할 수 없(형태소 토큰화 필요)고, 띄어쓰기가 비교적 잘 지켜지지 않는 등 토큰화가 어려움.

###### 서브워드 토큰화
- 서브워드 분리(Subword segmentation) : 한 단어를 여러 서브워드(더 작은 단위의 의미있는 단어)로 분리해 인코딩 및 임베딩. OOV나 희귀, 신조어 등의 문제를 완화가능.
  
- BPE(Byte Pair Encoding) : 대표적인 서브워드 분리 알고리즘. 원래는 데이터 압축 알고리즘. 연속적으로 가장 많이 등장한 글자의 쌍을 찾아 하나의 글자로 병합. 
  더이상 병합할 바이트의 쌍이 없을때까지 병합.   
- BPE in NLP : 글자단위에서 점차적으로 단어집합을 만드는 bottom up 방식의 접근을 사용. 훈련데이터의 단어들을 글자/유니코드 단위로 단어 집함을 만들어, 
  가장 많이 등장하는 유니그램을 한 유니그램으로 통합. 
- BPE 알고리즘 : 딕셔너리(등장하는 단어의 집합)를 글자단위로 분리(초기단어집합), 알고리즘의 동작(가장 빈도수가 높은 유니그램(바이트)의 쌍을 한 유니그램으로 통합)을 몇 번 반복할지 결정.

- WPM(WordPieceModel) : BPE의 변형 알고리즘. 코퍼스의 가능도(우도, likelihood)를 가장 높이는 쌍을 병합. 
  모든 단어의 앞에 _를 붙이고 단어는 서브워드의 통계에 기반해 띄어쓰기로 구분. 모든 띄어쓰기를 제거하고 _를 띄어쓰기로 바꿔 디코딩 가능.
- WPM 아이디어 : 자주 등장하는 단어는 그대로 단어 집합에 추가, 자주 등장하지 않으면 서브워드로 분리해 집합.  

- Unigram Language Model Tokenizer : 각 서브워드들에 대해 (해당 서브워드가 단어집합에서 제거되면 우도가 감소하는 정도)을 계산, 
  이렇게 측정된 서브워드를 손실정도로 정렬해 최악의 영향을 주는 10~20%의 토큰을 제거. 이를 원하는 단어 집합의 크기까지 반복.

##### 정규화
- 정규화 : 표현 방법이 다른 같은 의미의 단어들을 같은 단어로 만드는 작업. 
- 문장부호(구두점)제거, 전체 대/소문자화, 숫자 단어화, 약어 전개 등의 자연어 텍스트 처리 수행을 위한 과정.
- 문장부호 제거는 반복문이나 기타 클래스/메서드, strip(string.punctuation(구두점모음))으로, 대/소문자화는 str.upper()/lower()로 가능하다.
- 단, 단어 내에 문장 부호가 있거나 단어가 대소문자 구분을 필요로 하기도 하기에 무턱대고 수행해서는 안된다.
###### 텍스트 대체  
- 축약이나 줄임말 등을 본래대로 풀어 처리를 효과적이게 하는 과정.
- 정규 표현식 이용 텍스트 대체:  import re >  [(r'won\'t', 'will not'), (바꿀단어, 바꿜 단어)\]식으로 제작 > re.compile(바꿀 단어) > 
  re.subn(컴파일, 바뀔 단어, text)[0\] 의 과정을 거쳐 할 수 있음.
###### 반복 문자 처리
- 무의미하고 오류를 일으키는 반복문자를 일반 문자로 변환.
- 반복문자를 포함하는 단어를 역참조 방식을 사용해 제거.
###### 단어 동의어 대체
- 더 훌륭한 성능과 적은 오류를 위해 같은 의미의 단어를 하나로 변환.

##### 정제 (불용어)
- 정제 : 갖고 있는 코퍼스에서 노이즈 데이터를 제거. 토큰화보다 앞서 이뤄지기도, 이후에도 남아있는 노이즈 제거를 위해 지속적으로 이훠지기도 함.
- 노이즈 데이터 : 무의미하거나, 목적과 다른 불필요 단어이거나, 특수문자 등을 뜻함.
- 등장 빈도가 적은 단어, 길이가 짧은 단어(영어), 빈 값(data.isnull().values.any()으로 확인가능)등도 제거해야함.
###### 불용어 처리
- 불용어 처리 : 문장의 전체적 의미에 크게 기여하지 않는 불용어를 검색 공간 줄이기 등의 이유로 제거하는 과정.
- 지주 등장하나 도움이 되지 않는 조사, 접미사 등을 제거함. 미리 정의된 불용어 사전이나 직접 정의한 불용어 들을 제거할 수 있음.

##### 태깅
- 시퀀스 레이블링 : 입력시퀀스에 대해 레이블 시퀀스를 각각 부여하는 작업([X1,X2],[Y1,Y2]). 태깅이 대표적.

- 태깅 작업 : 각 단어가 어떤 유형에 속해 있는지를 알아냄. RNN의 다대다 작업이며 양방향 RNN(모든은닉상태 출력)을 사용함. 챗봇, 기계번역 등의 전처리 작업으로써 필요할 때가 많음.
- 개체명 인식 : 각 단어의 유형이 사람, 장소, 단체등의 유형인지 알아냄. 주어진 유형중 단어가 어떤 유형인지 예측하며, 모델헤 따하선 품사 정보를 입력으로 요구하기도 함. 
  도메인이나 목적에 특화시키려면 직접 만들어야 함.
- 품사 태깅 : 각 단어의 품사가 명사, 동사, 형용사등 인지 알아냄.  품사에 따라 뜻이 달라지는 단어를 위해 토큰화 과정에서 각 단어가 어떤 단어로 쓰였는지 구분해 놓는 것.
- 사전에 추가 : 사람 이름 등 고유한 단어로 토큰화 되어야 하는 것들은 형태소 분석기에 사용자 사전을 추가해 분리되지 않게 할 수 있음.

- BIO(IOB, begin inside outside)표현 : 코퍼스로부터 개체명을 인식하기 위한 보편적 방법. 개체명의 시작은 B, 개채명의 내부는 I, 개체명의 외부는 O로 표기. 
  B/I-개체명 식으로 어떤 객체인지도 함께 태깅.

##### 어간 추출 (표제어 추출)
- 표제어 추출 : 단어들을 표제어(기본 사전형 단어)로 바꿔가는(찾아가는) 과정.
- 어간 : 단어의 의미를 담고 있는 핵심 부분. 
- 형태학적 파싱 : 어간과 접사를 분리하는 작업.
- 한국어 : 한국어는 5언 9품사 구조를 가지고 있으며, 이중 용언(동사, 형용사)이 어간과 어미의 결합으로 이뤄져 있음.

##### 토픽 모델링 (주제 찾기)
- 토픽 모델링 : 문서의 주제를 발견하기 위한 텍스트마이닝 기법.
- 행렬 용어 : 전치행렬 - 원래 행렬에서 행과 열을 바꿈 | 직교행렬 - 원래 행렬\*전치행렬,n\*n 행렬에서 원 행렬*전치행렬 = 단위행렬을 만족해야 함. | 
  단위행렬 - 대각행렬(정사각 아니여도 됨, 아니여도 (i,i) 가 1), 대각선은 1, 나머지는 0인 정사각 행렬. | 역행렬 - A\*어떤 행렬 이 단위 행렬일 때 어떤 행렬. 
- SVD(특이값 분해) : m\*n 차원의 행렬 A 를 UΣV^T 로 분해하는 행렬분해 방법. U(m\*m 직교행렬), V(n\*n 직교행렬), Σ(m*n 직사각 대각 행렬) 로 분해한다.
- Truncated SVD(절단된 SVD) : 대각 행렬 Σ의 원소값 중 상위 t 개만 남음. t 는 클수록 많은 정보를 가져 갈 수 있지만 작을수록 노이즈 제거 가능. 원 행렬 복구 불가.
- LSA(latent Semantic Analysis, 잠재 의미 분석) : 단어의 의미를 고려. DTM이나 TF-IDF와 같은 각 문서에서 각 단어의 빈도수를 카운트한 행렬을 입력으로 받아 차원을 축소해 의미를 추출함. 
  쉽고 빠른 구현과 단어의 잠재 의미를 이끌어 낼 수 있단 장점이 있음.
- LSA 자세히 : 단어를 행, 문장을 열로 나타낸 뒤, SVD 를 사용해 세개의 매트릭스(토픽을 위한 단어, 토픽 강도, 문장)로 나타냄. 뒤의 두 매트릭스를 곱해 단어간 유사도를 파악. 
  새 정보의 업데이트가 어렵고, 단어 의미 유추 작업에서 성능이 떨어진단 문제점을 가짐. 
- LDA(Latent Dirichlet Allocation, 잠재 디레클레 할당) : 토픽 모델링 대표 알고리즘. DTM 이나 TF-IDF 행렬을 입력으로 해 역공학(문사 작성 과정 역추적)을 해 토픽 추출. 
  단어가 특정 토픽에 존재할 확률과 특정 토픽이 존재할 확률을 결합확룰로 추정해 토픽을 추출한다.

##### 통계적 모델링
- 통계적 모델링 : 확률/수학적 모형을 가지고 현실의 데이터형성과정을 모방한 것. 현실을 그대로 반영하지는 못하지만, 적절한 가정들 하에 유용하게 쓰임.
- 통계적 모델링의 목적 : 불확실성의 측정, 통계쩍 추론, 가설 검증을 위한 도구 제공, 머신러닝에서의 예측.
- 과정 : 문제 이해 > 계획수립/데이터수집 > 자료탐색 > 모델상정 > 모델적용 > 모델확인 > 더 나은 모델을 위해 상정~확인 반복 > 모델사용

- 마르코프 확률 과정 : 현재에 대한 조건부로 과거와 미래가 서로 독립인 확률과정(미래를 유추하려 한다면 과거는 아무 정보 제공을 못하고, 오직 현재의 값만 쓸모가 있음). 
- HMM(hidden Markov model, 은닉 마르코프 모델) : 통계적 마르코프 모형의 하나, 시스템이 은닉상태와 관찰가능한 결과 두가지 요소로 이뤄져있다 보는 모델. 마르코프 과정을 통해 도출된 결과만 관찰가능.
- CRF(conditional random field, 조건부 무작위장) : 통계적 모델링 방법중 하나. 구조적 예측에 사용. 이웃하는 표본을 고려. 입력시퀀스에 대한 출력시퀀스의 조건부확률.


##### 문장 벡터화 (정수 인코딩)
- 국소표현 : 해당 단어만 봄. 이산표현 이라고도 함.
- 분산표현 : 주변을 참고해 그 단어를 표현. 연속표현 이라고도 함.
  
- 단어집합(사전) : 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓은 것. 전체 단어 뿐 아니라 특별 토큰(pad, unk 등)도 등록해두어야 함. 
- 정수인코딩 : 단어집합에 고유한 숫자를 부여(각 단어에 빈도수 등을 기준으로, 순서대로 고유한 정수를 부여)하는 작업.
- 문장 벡터화 : 정수 인코딩된 단어집합의 단어들을 모델이 처리하기 쉽도록 벡터화 하는 것. 원-핫 인코딩(희소행렬), 워드 임베딩(밀집벡터)등을 이용함.  

- 카운트 기반 단어 표현 : 정수 인코딩 방법중 하나. 이름대로 등장 빈도 등 횟수에 따라 인덱스를 부여.
- 빈도수로 벡터화 : (토큰화>정제,불용어 제거>등장 단어,빈도수 기록)의 과정을 거쳐 각 단어의 빈도수가 높은 단어부터 낮은 인덱스를 부여하는 방법으로도 벡터화가 가능하다.  
- 원 핫 인코딩을 이용한 벡터화 : 단어집합의 크기를 차원으로, 표현하고 싶은 단어의 인덱스엔 1, 나머지는 0을 부여하는 표현방식. 결과 벡터를 원-핫벡터(희소행렬임)라고 함.
- 원-핫 인코딩 단점 : 단어의 개수가 늘어날 수록 벡터 저장을 위한 공간이 늘어나고, 단어의 유사도를 표현하지 못한다는 문제점을 가짐.
- OOV(Out-Of-Vocabulary) : 정제, 불용어 처리 과정을 거쳐 생긴 단어집합(모든 단어를 중복을 허용하지 않고 모아놓음)에 없는 단어. 따로 OOV 의 레이블을 만들어 OOV 를 인코딩 해줘야 한다.
 
- bag of words : 정수인코딩 방법 중 하나. 단어의 출연 빈도로 문장을 나타냄. 문장의 유사도파악 등 에도 사용됨.
- bag of words 단점 : Sparsity(단어의 개수가 많다보니 0이 많아 계산량이 많아짐), 흔한 단어의 힘이 세짐, 단어의 순서를 완전 무시, 처음 보는 단어는 처리 불가 등의 단점이 있다.
- DTM(Document Term Matrix, 문서 단어 행렬) : 서로 다른 문서의 Bow 들을 결합한 표현 방법. 각 BoW 문서를 행으로, 단어를 열으로 하는 행렬로 만듦. 
  희소표현(원핫벡터와 마찬가지, 0이 많아져 공간에 문제), 단순빈도수접근(불용어 처리 불가)등의 문제점을 가짐. 
- TF-IDF(term frequency inverse doc freq) : DTM, 문서에서 각 단어별 문서 연관성(문서에서 가진 정보)을 파악. 단어빈도와 역문서빈도를 사용해 각 단어들마다 중요도를 가중치로 부여. 
  모든 문서에서 자주 등장시 중요도를 낮게, 특정 문서에서 자주 등장시 중요도를 높게 함. 
  TF(특정 문서 d 에서 특정 단어의 등장 횟수)*IDF(log(n/1+(t가 등장한 문서 수), 종종 자연 상수(ln)))로 이뤄짐.
- IDF : 자주 등장하는 단어(불용어)에 패널티를 주어 위의 것을 보완한 방법. log(총 문장 개수/단어 출현 문장 개수(+1, DIV_0를 피하기 위해)) 의 공식을 사용함.
  TF-IDF 의 결과는 (데이터 개수, 사용 단어 수)로 나타난다.

- Recommendation System using Document Embedding : 문서간 비교를 위해 각 문서를 고정된 길이의 벡터로 변환. 
- 문서 벡터 변환 : Doc2Vec, Sent2Vec 등을 사용할 수 도 있으나, 가장 간단한 방법은 문서에 존재하는 단어 벡터들의 평균을 구하는 것임.
###### 패딩
- 패딩 : 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업. 이를 통해 문장들을 하나의 행렬로 보고 병렬처리를 가능하게 함.
- 방법 : 정수 인코딩시 문장별로 단어를 모아 두어 문장의 길이를 확인할 수 있는데, 이때 가장 긴 문장의 길이에 맞게 다른 문장에 0을 삽입한다.

##### word embedding (정수 인코딩 + 단어간 유사도 정보)
- Word Embeddings : 텍스트 내 단어를 밀집 벡터로 만드는 과정. 단어의 유사도(의미)정보도 벡터화 가능. 유사도에 따라 단어가 유사한 값을 띄게 됨. 
  정수 인코딩(원핫인코딩)된 벡터를 입력으로 받음.
- 임베딩 벡터 : 사용자가 설정한 임베딩벡터 사이즈(차원)에 따라 임베딩 벡터가 생성됨. 임베딩 벡터 안에는 각 단어의 정보를 가지고 있는 (임베딩 사이즈)개의 실수가 들어 있음.  
- 밀집벡터 : 희소벡터(대부분의 값이 0인 벡터)와 달리 대부분의 값이 실수이고 상대적으로 저차원인 벡터. 사용자가 설정한 값으로 벡터의 차원을 맞춤. 
  특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로브터 임베딩 벡터값을 가져오는 룩업테이블.
- 분산표현 : 분포 가설(비슷한 위치에서 등장하는 단어는 비슷한 의미를 가짐)에 기초해 만들어진 표현 방법. 단어의 의미를 여러 차원에 분산하여 표현. 단어간 유사도 계산 가능.

- 임베딩 층 : 정수 인코딩된 단어들을 입력으로 받아 밀집벡터(임베딩벡터)로 변환. 입력 정수에 대해 밀집벡터로 맵핑하고, 이 벡터는 가중치가 학습되는 것과 같은 방식으로 훈련됨. 
  원-핫 벡터를 입력으로 받으면 룩업테이블과 내적곱을 해 임베딩 벡터를 가져오고, 정수인코딩된 단어를 입력으로 받으면 인덱스로 가져옴.

- 워드 임베딩 평균 : 임베딩층 뒤에 GlobalAveragePooling1D()를 사용해 할 수 있음. 기타 은닉층의 사용 없어도 높은 정확도의 분류가 가능함.
###### char embedding (글자 임베딩)
- 글자 임베딩 : 워드 임베딩과는 다르게 단어의 벡터 표현을 얻어 OOV문제를 해결함. 모르는 단어도 뜻을 추측(이해)해서 예측함. 워드 임베딩의 대체로도 사용할 수 있지만, 
  워드 임베딩과 연결해 신경망의 입력으로 사용하기도 함. 글자단위의 정수 인코딩이 필요.
  
- 1D CNN 이용 : FastText가 글자의 N-gram조함을 이용하듯이, 전체입력 내부의 더 작은 시퀀스로부터(N-gram) 정보를 얻음.
- 1D CNN 방법 : 단어를 글자단위로 분리, 글자에 대해 임베딩(이후 패딩 가능), 그 후 1D CNN을 적용. 나온 벡터를 풀링층을 거쳐 스칼라로 만들고, 이를 연결해 하나의 벡터로 만듦. 
  이 벡터가 단어의 벡터.
  
- BiLSTM 이용 : 단어를 글자로 쪼갠 뒤 임베딩, 정방향/역방향 LSTM을 사용, 둘의 마지막 은닉 상태를 연결해 벡터화, 이를 단어의 벡터로 사용.
###### sentence Embedding(문장임베딩) 
- 문장임베딩 : 다수의 문장간 비교를 위해 각 문장을 고정된 길이의 벡터로 변환하는 것. 여러 방법이 있으나, 간단하게는 문장의 단어벡터평균을 구하는 방법이 있음.
- 과정 : 모든 문장에 대해 문장벡터를 만든 뒤, 문장벡터간 코사인 유사도 행렬을 만들고, 이를 페이지랭크 알고리즘의 입력으로 사용해 각 문장의 점수를 구한 뒤, 
  점수가 가장 높은 문장을 상위n개 선택해 문서의 요약문으로 함. 

###### Word2Vec
- word to vector : 워드 임베딩을 하는 대표적 방법. 은닉층에 활성화 함수가 존재하지 않으며, 룩업테이블(투사층)이 그 역할을 대신함. 
  일반적으로 SGNS(Skip-Gram with Negative Sampling)을 사용. 동음이의어를 잘 반영하지 못한다는 문제점이 있음. 추천시스템에도 사용.
- W2V 자세히 : 투사층(임베딩 벡터의 차원(M)), 입력층과 투사층 사이(단어집합크기(V)*M), 투사층과 출력층 사이(M\*V) 의 크기를 가짐. 
  윈도우 크기 내에서만 주변 단어를 고려해 전체적 통계 정보를 반영하지 못한다는 문제점을 가짐. 
- VS NNLM : NNLM의 느린 속도와 정확도를 개선해 탄생. 다음단어가 아닌 중심 단어를 예측, 이전단어만 참고하는게 아닌 전후 단어 모두 참고, 활성화 함수와 은닉층 제거, 
  계층적 softmax와 네거티브 샘플링 등을 이용해 은닉층 제거 이외에도 속도를 개선.   

- 네거티브 샘플링(Negative Sampling) : W2V가 학습과정에서 전체 단어가 아닌 일부 단어에만 집중할 수 있도록 하는 방법. 
  무작위로 주변단어가 아닌 단어를 선택하고, 그것들을 기준으로 단어 집합을 만들어 긍정과 부정의 이진 분류 문제로 만듦.
- 자세히 : 일반 W2V(Skip-Gram)에는 속도(마지막 단계에서 softmax를 적용하고 오차를 구하며 임베딩 벡터를 조정하는 작업을 관계없는 단어에도 적용)문제를 가짐.
  해결(일부집합에 대해서만 고려)을 위해, 주변 단어를 가져와 집합을 만들고, 임의로 주변단어가 아닌 단어를 가져와 집합을 만든 뒤 마지막 단계를 이진분류 문제로 바꿈.
  이는 다중클래스 분류 문제를 이진분류문제로 바꾸면서도 훨씬 효율적인 연산량을 가짐.

- CBOW(Continuous Bag of Words) : word2vec의 두가지 방법중 하나. 주변 단어를 이용해 중간의 단어를 예측. 
  윈도우(앞뒤로 몇개의 단어를 볼 지)를 계속 움직여 주변과 중심 단어 선택을 바꿔가며 데이터 셋을 만드는(슬라이딩 윈도우)것이 가능. 
- CBOW 자세히 : 두개의 가중치 행렬(입력-투사, 투사-출력)을 학습해 나감. 입력된 원 핫 벡터(윈도우 안의 단어들)들을 가중치 행렬과 곱(i번째 행을 가져옴)해 그 결과 벡터들의 평균을 구함.
  그 평균 벡터는 두번째 가중치 행렬과 곱해져 원-핫 벡터와 차원이 동일한 벡터를 생성하고, 여기에 softmax와 손실함수로 cross-entropy를 사용해 
  각 단어가 중간 단어일 확률을 가진 스코어 벡터를 생성.
- Skip-gram : word2vec의 또다른 방법. 중심 단어에서 주변 단어(윈도우 안의 단어들)를 예측. CBOW와 유사한 과정을 통해 구함. CBOW보다 성능이 좋다고 알려져 있음.
- SGNG(Skip-Gram with Negative Sampling) : 네거티브 샘플링을 사용하는 Skip-Gram. 중심단어와 주변단어 둘 다 입력이 되고, 두 단어가 실제 윈도우 내에 존재하는 이웃일 확률을 계산. 

- 사전훈련된 W2V(GloVe)코드 : [urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", 
  filename="GoogleNews-vectors-negative300.bin.gz")
  word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True))]

- LSA(잠재 의미 분석) : 각 단어의 빈도수를 카운트한 행렬(DTM)을 입력으로 받아 차원을 축소(Truncated SVD)해 잠재된 의미를끌어내는 방법론. 토픽모델링 기법이자 워드 임베딩 방법.
- 단점 : 카운트 기반으로 코퍼스의 전체적인 통계정보를 고려하기는 하지만, 단어의 의미 유치 작업에서 성능이 떨어짐.  

###### GloVe
- GloVe : 또 다른 워드 임베딩 방법. 카운트 기반과 예측 기반 모두 사용하는 방법론. 카운트 기반의 LSA(잠재 의미 분석)과 W2V의 단점을 지적, 
  보완한다는 목적으로 나와 W2V와 비슷한 성능을 보여줌.  
- GloVe 아이디어 : 임베딩된 중심 단어와 주변 단어 벡터의 내적곱이 전체 데이터에서의 동시 등장 확률(log(P(i,j)))이 되도록 만드는 것. 
- 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix) : 행과 열을 전체 단어의 집합으로 구성한 뒤, i단어의 윈도우 안에서 j단어가 등장한 횟수를 (i,j)에 나타낸 행렬. 
  전치해도 동일한 행렬.
- 동시 등장 확률(Co-occurrence Probability) : 동시 등장 행렬에서 특정 단어(i)의 전체 등장 횟수와 특정 단어(i)가 등장했을때 특정 단어(j)가 
  등장한 횟수를 카운트해 계산((i,j)/sum(i))한 조건부 확률. 

###### FastText
- FastText : 페이스북 개발 워드 임베딩 방법. 메커니즘은 W2V와 비슷하지만, 하나의 단어 안에도 여러 단어들이 존재한다고 생각(내부단어 고려)함. 
  W2V, LSA와 달리 OOV와 RaerWord에 대한 대응이 가능함.
- 내부단어 고려 : 각 단어를 글자단위 n-gram의 구성으로 취급. n을 몇으로 하냐에 따라 단어들이 얼마나 분리될지 결정. 
- 토큰 분리 : 시작과 끝을 의미하는 <, > 에 기존 단어에 <>를 추가한 토큰까지 총 (글자수 - n + 4)개의 토큰이 나옴. 
  이때 n의 범위를 지정할 수 있는데, 이 경우 범위 안의 정수가 모두, 차례대로 n으로 사용됨.

- OOV(Out Of Vocabulary)대응 : 데이터 셋 내 모든 단어의 n-gram에 대해 워드 임베딩이 되어, 데이터셋만 충분하면 모르는 단어(OOV)에 대해서도 다른 단어와 유사도 계산이 가능. 
- RareWord(빈도수가 적은 단어)대응 : 단어가 희귀단어라도 그 단어의 n-gram이 다른 단어와 겹친다면 비교적 높은 임베딩 벡터값(유사도)을 얻음. 
  노이즈가 많은 코퍼스에서도 같은 이유로 강점을 가짐. 

###### ELMo
- ELMo(Embeddings from Language Model) : 새로운 워드 임베딩 방법. 사전 훈련된 언어 모델 사용. 같은 표기의 단어라도 문맥을 반영해 임베딩 할 수 있음.
- biLM(Bidirectional Language Model) : 양쪽 방향 RNN을 모두 사용하는 언어 모델. 단어단위로 입력을 받음. 다층구조. 다음 층으로 보내기 전이 아니라 훈련 후 표현을 위해 은닉상태를 연결. 
- biLM 이용 임베딩 : 각 층을 연결 > 출력별로 가중치 > 출력값을 전부 더함 > 벡터의 크기(스칼라)를 곱함. 이렇게 완성된 벡터를 ELMo표현이라고 하고, 
  이를 기존 임베딩 벡터와 함께(기존 벡터와 연결, 사전 흔련된 언어 모델 가중치는 고정)사용할 수 있음.


### 모델, 평가

##### 유사척도 (평가)
- 벡터 유사도 : 각 문서의 단어들을 어떤 방법으로 수치화해 표현 했는지, 문서간 단어 차이를 어떤 방법으로 계산했는지에 따라 달라짐. 
  이런 유사도르 이용해 영화추천(본 영화와 비슷한 영화를 추천)등을 구현할 수 있음.
- 코사인 유사도 : 두 벡터간 코사인 각도를 이용해 구할 수 있는 두 벡터의 우사도. 문자열을 축으로 해 축과 문장간의 각도로 유사도를 측정. 둘의 곱/둘의 거리로 구할 수 있음. 
- 자카드 유사도(자카드 계수, 타니모토 계수) : 두 세트의 곱집합 / 합집합. 동일하면 1, 완전 다르면 0. 두 문서의 총 단어 중 두 문서 전부 등장한 단어 비율.
- 유클리디안(유클리드) 거리 : 단어간의 거리가 짧으면 더욱 유사. 벡터 공간에서 유사도 측정. 자카드나 코사인보다는 효과가 떨어짐.

- 이진 거리 : 문자열 유사도 메트릭. 두 라벨이 동일하면 0, 다르면 1을 반환.
- 매시 거리 : 부분 일치에 기초. 1 - (곱집합 길이/합집합 길이)*(두 세트의 길이차에 따른 점수(1, 0.67, 0.33, 0))

- 지프의 법칙 : 토큰이 언어로 배포되는 방법을 설명. 토큰 빈도가 정렬된 목록의 순위와 정비례하게 함. 
- 편집 거리 : 두 문자열을 동일하게 하려면 삽입,대체,삭제해야 하는 문자의 수를 계산. 
- 스미스 워터맨 거리 : 편집거리와 유사. 관련된 단백질서열 및 광학정렬을 검출하기 위해 개발됨. 
  
- 문서의 유사도 구하기 : bag of ward 에 코사인 유사도를 적용하거나(bow 각 요소들의 곱의 합/bow 합의 제곱근)TF-IDF 를 적용하는 방법으로도 구할 수 있음. 
  구현이 쉽고 불용어를 잘 거른다는 장점이 있지만, 단어단위로 보기에 동음이의어를 잡지 못하고 토픽은 알 수 없다는 단점이 있음.
- WMD(word mover's distance) : 문서의 유사도를 구하는 방법. word2vec 근간으로 하여 단어들간의 유클리디안 거리를 사용. 
  w2v 로 임베딩된 단어에 대해 문서 단어들의 유사도를 판단한다. 거리가 다양하다면 단어를 여러 벡터에 대입하여 판단. 이것을 보완한 RWMD(속도 빠름, 결과 비슷)도 존재한다. 

- 혼동행렬 : 머신러닝에서 정확도 측정시 자세한 내용을 알기 위해 사용. TP/FN/FP/TN 으로 이뤄진 행렬. 정밀도, 재현율 등이 등장.

- 펄플렉서티(Perplexity, PPL) : 언어 모델 평가를 위한 내부 평가 지표. 낮을수록 성능이 좋음. 테스트 데이터에 의존. 분기계수(선택할 수 있는 가능한 경우의 수)임.

- unigram precision : 기계번역, 단어계수 카운트로 측정. 후보 문장 중 reference에서 등장한 단어의 개수를 전체 단어의 개수로 나눔(count/cand_ngram_count). 
  같은 단어가 여러번 등장하면 정확도가 높아져 중복을 고려한 측정이 필요.
- 보정된 유니그램 정밀도(Modified Unigram Precision) : 단어의 중복을 고려하기 위해, 유니그램이 한 ref에서 최대 몇번 등장했는지를 센 후 
  기존 count보다 작다면(min(count,max_ref_count))그것을 사용해 정밀도를 계산한다. 문맥의 고려가 불가.

- BLEU(Bilingual Evaluation Understudy)Score : 기계번역의 성능측정을 위해 사용. 높을수록 성능이 좋음. 언어에 구애받지 않고, 빠름. 
  보정된 유니그램 정밀도에서 문맥을 위해 유니그램을 n-gram으로 교체하고,
  후보 문장의 길이가 점수에 주는 영향을 상쇄하기 위해 (길이가 가장 비슷한)참조 문장의 길이가 후보보다 크면 패널티를 줌.
- BLEU 식 : BLEU=BP×exp(^N∑_(n=1)w_n log p_n). BP = (c>r)? 1 : e^(1−r/c).   C = Candidate 길이, r : Candidate와 가장 적은 길이 차이의 Reference 길이.

##### 언어 모델
- 언어 모델(LM) : 문장에 확률을 할당하는 모델. 문장에 대해 그 문장이 적절한지, 말이 되는지(문법,문맥 등) 등을 판단. 
  확률 할당을 위해 보편적으로 이전 단어가 주어지면 다음 단어를 예측하도록 함. 혹은 빈칸 추론을 시킴. 통계를 이용하거나 ANN 을 이용해 만들 수 있음.
- LM 특징 : 모델 학습에 따로 레이블링 할 필요 없다는 장점을 가짐. 순방향과 역방향이 있으며 둘을 모두 합친걸 biLM 이라고 한다. 
  
- 언어 모델링 : 주어진 단어들로 아직 모르는 단어를 예측하는 것.  
- 언어모델의 기능 : 기계번역에서 문맥을 파악하거나, 오타를 교정하거나, 음성인식을 더 잘 되게 도와주는 등 보다 적절한 문장을 판단함.  
- 조건부 확률의 연쇄 법칙 : 조건부 확률(사건 B가 일어난 경우 사건 A가 일어날 확률)이 가지는 특징. P(x1,x2...xn)=P(x1)P(x2|x1)...P(xn|x1...xn−1). 
  이를 문장에 적용해 다음 단어에 대한 예측 확률을 모두 곱하여 문장의 확률을 구할 수 있음. 
  
- N-그램 : n개의 토큰이 연속적으로 모인것. 한개면 유니그램, 두개면 바이그램등으로 지칭. 문맥의 파악이 가능. 문장 벡터화의 방법 중 하나인 bag of words 의 단점을 해소 할 수 있음.

- SLM(Statistical Language Model) : 통계적 언어모델. 카운트에 기반해 확률을 구함(count(현 문장+예측할 단어)/count(현 문장)). 
  데이터 부족시 희소문제(데이터 부족으로 정확한 모델링이 불가능한 문제)가 발생하며 이를 완화하기 위해 n-gram,스무딩,백오프등 일반화 기법이 있으나 결국 NN 언어 모델로 넘어가게 됨.  
- N-gram 언어 모델 : SLM 의 일종. 일부 단어(N-1 개)만 고려. 몇개의 단어만 고려하기에 의도와 다른 단어를 고를 수 있다는 것과 여전히 남아있는 희소 문제, 
  n을 몇으로 선택 할지(trade-off) 등의 문제를 가짐. 훈련에 사용되는 코퍼스에 따라 성능이 비약적으로 달라진다는 SLM 의 특징또한 가지고 있음.

- 한국어 언어모델 : 한국어는 다음 단어의 예측이 훨싼 까다로운데, 어순이 크게 중요하지 않고, 교착어(형태소가 붙어 단어 형성)이며, 띄어쓰기가 잘 지켜지지 않는 한국어의 특성 때문이다.

###### 질의응답
- 질의응답 : 질문에 대한 답변 출력. 
- 메모리 네트워크 : 두개의 입력(스토리/질문 문장)을 받아 스토리는 두개(A,C), 질문은 하나(B)의 임베딩층을 통해 각각 단어임베딩이 되고,
  이것들에 어텐션(V-B,Q-C,K-A)이 적용됨. 이를 질문표현(질문문장임베딩)과 연결해 LSTM과 밀집층의 입력으로 사용.
- 이를 이용해 스토리 단어들과 질문 단어의 유사도를 구해 가장 자연스러운 답변을 생성하는 과정을 거쳐 질의응답기능을 만들 수 있음.
- 챗봇 데이터 : [urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv", filename="ChatBotData.csv")]

#### 머신러닝

##### 텐서
- 텐서의 종류 : 기본적으로 행렬. scalar - 요소 하나만 가지고 있는 0차원(랭크 0)의 텐서. Vector - 1차원(벡터도 요소 수에 따라 차원이 있음, 이건 텐서의 차원), 
  matrix - 2차원, 그 이후부터는 N-tensor 이다.
- 이미지는 (이미지 수, 행, 렬, rgb)의 4차원 텐서로(흑백은 3차원), 영상은(프레임,이미지,행,렬,rgb)의 5차원 텐서로 이뤄진다. 

##### DL
- MLP(다층 퍼셉트론) : 단층 퍼셉트론에서 은닉층이 1개 이상 추가된 신경망. FF 신경망의 가장 기본적 형태.
- NNLM(신경망 언어모델) : n-gram 언어 모델의 희소문제 해결을 위해 등장. 기계가 단어간 유사도를 파악. n 개의 단어 벡터를 이용해 목표 단어를 예측. 
- NNLM 특징 : 밀집 벡터로 인해 희소문제를 해결했고, 모든 n-gram 을 저장하지 않아도 된다는 이점이 있지만, 마찬가지로 정해진 n개의 단어만 확인하고 
  입력의 길이가 고정되어 있다는 문제점이 있다.

##### RNN
- 순환 신경망 : 자연어 처리에서 단어의 품사 추축등 여러 분야에서 대표적으로 이용됨. 입력과 출력의 길이를 다르게 셜계할 수 있음. 가장 기본적인 시퀀스 모델.
  뉴런단위 보단 입력, 출력벡터와 은닉상태(층)라는 표현을 주로 씀. 모든 은닉층의 상태를 출력 > 각 스텝마다 cost를 계산해 하위 스텝으로 전파 > 
  각 가중치를 업데이트하게 하면 many-to-many 문제를 해결할 수 있음.

- Ht=tanh(WxXt+WhHt−1+b) 의 식을 사용함. 두 입력이 각각의 가중치와 곱해져 메모리 셀의 입력이 되고, 이를 활성화 함수(tanh)의 입력으로 사용, 그 값이 은닉층의 출력(은닉상태)이 됨.
- Wx는 (은닉 상태의 크기 × 입력의 차원), Wh는 (은닉 상태의 크기 × 은닉 상태의 크기), b는 (은닉 상태의 크기)의 크기를 가짐.
- 장기 의존성 문제 : Vanilla RNN(Simple RNN)의 단점. 시점이 길어질수록 앞의 정보고 뒤로 전달되질 못해 비교적 짧은 시퀀스에서만 효과를 보이는 것.
  
- 메모리셀 : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드. 이전의 값을 기억하려는 일종의 메모리 역할을 수행. RNN 셀 또는 그냥 셀 이라고 함. 
  이전 은닉층의 셀에서 나온 값은 자신의 입력으로 사용하는 재귀적 활동도 함.
- 은닉상태 : 메모리셀이 츨력층이나 다음 시점의 자신에게 보내는 값. 현재 시점의 메모리 셀은 이전 시점의 메모리셀이 보낸 은닉값을 현시점의 은닉 상태 계산을 위한 입력값으로 사욯함.
- RNN 층 : (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받아 설정에 따라 메모리 셀 최종지점의 은닉상태((batch_size, output_dim), many_to_one 문제)나 
  각 시점의 상태값을 모아 전체 시퀀스를((batch_size, timesteps, output_dim), many-to-many 문제)출력한다.

- DRNN(Deep Recurrent Neural Network) : 깊은 순환 신경망. 다수의 은닉층을 가지는 RNN.
- BRNN(Bidirectional Recurrent Neural Network) : 양방향 순환 신경망. 어느 시점의 값 예측을 위해 이전 데이터 뿐 아닌 이후 데이터도 사용. 두개의 메모리셀 사용.
- CRNN(Char Recurrent Neural Network) : 글자단위 RNN. 입출력의 단위가 단어가 아닌 글자.

- RNN 이용 텍스트 분류 : 텍스트를 입력으로 박아 텍스트가 어떤 종류의 범주에 속하는지 구분. 감성분석, 의도분석 등으로 나뉨. 
  다대일(Many-to-one, 모든 시점에 대해 입력을 받아 최종 지점의 셀만 출력)문제에 속함.

###### RNNLM
- RNNLM(RNN 언어 모델) : RNN을 이용한 언어 모델. 입력의 길이를 고정하지 않을 수 있음.  
- 교사강요 : RNN, RNNLM의 훈련방법. 훈련 시 t시점에서 예측한 값을 다음 시점의 입력으로 사용하는게 아닌 t시점의 레이블(정답)을 다음 시점의 입력으로 사용한다.

- Char RNNLM : 글자단위 RNN언어모델. 글자단위를 입출력으로 하여 임베딩 층을 사용하지 않음. 
  
```python clone
# 단어 예측 코드
def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수
    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장
    sentence = ''
    for _ in range(n): # n번 반복
        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩
        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩
        result = model.predict_classes(encoded, verbose=0)
    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.
        for word, index in t.word_index.items(): 
            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면
                break # 해당 단어가 예측 단어이므로 break
        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경
        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장
    # for문이므로 이 행동을 다시 반복
    sentence = init_word + sentence
    return sentence
```

###### LSTM
- LSTM : 장단기 메모리(Long Short-Term Memory). 장기 의존성 문제를 해결하기 위해, 메모리 셀에 입력게이트, 망각 게이트, 출력게이트를 추가해 불필요한 기억을 지움. 
  셀 상태 라는 값이 추가됨. 긴 시퀀스의 입력을 처리하는데 탁월.
- LSTM 처리 : 각 단어가 벡터로 면환된 문장 행렬로 입력을 받아(DTM등)시점마다 한 행식 입력으로 받아 처리.
- BoLSTM : 양방향 LSTM. 두개의 독립적 LSTM 아키텍쳐를 함께 사용. 

- 셀 상태(장기상태) : LSTM 에서 추가된 값. 이전시점의 셀 상태가 다음의 셀 상태를 구하기 위한 입력으로 사용됨. 입력게이트에서 나온 두 값을 원소곱(같은 위치끼리 곱)을 하고, 
  이걸 삭제 게이트 결과값에 더헤 현재 셀 상태를 구함.
- 게이트 : 은닉 상태(단기상태)값과 셀 상태 값을 구하기 위해 3개의 게이트(삭제,입력,출력. 모두 시그모이드 함수 존재)가 사용됨. 
- 입력 게이트 : 현재 정보 기억을 위한 게이트. (현재시점 x값 * 가중치)+(이전시점 은닉상태 * 가중치)가 시그모이드 함수를 지나고, 같은 값이 tanh 을 지남. 
  결과로 나온 두 값을 가지고 선택된 기억할 정보의 양을 정함. 시그모이드 함수의 값이 0이라면 현재의 입력값이 출력값에 영향을 줄 수 없음. 현시점 입력값의 반영도를 뜻함. 
- 삭제 게이트 : 기억 삭제를 위한 게이트. 현재시점 x값, 이전시점 은닉상태가 시그모이드 함수를 지남. 나온 값이 삭제 과정을 거친 정보의 양. 
  나온 값이 0이라면 이전의 상태값이 현재의 결과에 영향을 줄 수 없음. 이전시점 입력값의 반영도를 뜻함.
- 출력 게이트 : 현 시점의 은닉상태 결정을 위한 게이트. 현 시점 x값, 이전 은닉상태가 시그모이드를 지난 값. 

- 양방향 LSTM + CRF : CRF(Conditional Random Field)와 양방향 LSTM을 함께 사용. 레이블 사이의 의존성(예측 개체명)을 고려할 수 있음. 
  활성화 함수의 결과를 CRF층으로 전달, 레이블의 시퀀스에 대해 가장 높은 시퀀스를 가지는 시퀀스를 예측. 출력 레이블에 대한 양방향 문맥을 반영. 
  keras_contrib의 설치가 필요 ([!pip install git+https://www.github.com/keras-team/keras-contrib.git]).

###### GRU
- GRU(Gated Recurrent Unit) : 게이트 순환 유닛. LSTM과 성능은 유사하며 복잡했던 구조를 간단화 시킴. 업데이트 게이트와 리셋 단 두개의 게이트 만을 사용함. 
- 성능 : 데이터 양이 적을때는 GRU 가, 많을때는 LSTM이 낫다고 알려져 있음. 

##### CNN
- CNN(Convolution Neural Network) : 합성곱 신경망. 원래는 비전 분야에서 주로 사용되지만 NLP에 사용하기 위한 다양한 시도가 있었음.

- 1D CNN : 자연어 처리에 활용되는 합성곱. 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음(LSTM과 동일). 
- 1D CNN 커널 : 커널은 너비를 임베딩 벡터 차원과 동일하게 설정하고, 높이만 따로 설정해 커널의 사이즈라고 칭함. 너비가 임베딩 벡터와 동일하니 높이 방향으로만 움직임.
- 학습 방법 : 사이즈가 같거나 다른 커널 여러개를 사용해 벡터를 여러개 얻고, 거기에 풀링을 거쳐 스칼라화 한 다음, 그것들을 모두 연결해 하나의 벡터로 만듦. 
  이를 출력층에 연결해 텍스트 분류를 실행.


##### seq2seq
- Sequence-to-Sequence : 번역기등 입력 시퀀스에서 다른 시퀀스를 출력하는 기능에서 대표적으로 사용되는 모델. 크게 인코더와 디코더 두개의 아키텍쳐로 구성. 
  고정크기의 벡터에 모든 정보를 압축하려 하니 생기는 정보의 손실과, RNN의 기울기 소실 문제가 있음. 
  컨택스트 벡터를 디코더의 초기 은닉상태로 사용하거나(기본), 매시점마다 하나의 입력으로 사용하거나, 
  어텐션매커니즘을 통해 더욱 문맥을 반영하는 컨택스트 벡터를 생성해 매시점 입력으로 사용하는 등의 변형이 있음.
- S2S 훈련 : 두개의 쌍의 길이가 같지 않아도 됨. 훈련시에는 디코더에 원래 문장을 입력(<sos> + sent)받으면 정답(sent + <eos>)이 나와야 된다 알려주며(+ 교사강요로 디코더 입력도 사용)
  훈련하나, 테스트(실제)과정에선 컨텍스트 벡터와 <sos>만을 입력받아 단어를 예측하게(교사강요 사용 X)함. 따라서 decoder_input(<sos>+sent)/decoder_tar(sent+<eos>)따로 인코딩 필요.
- RNN : 인코더와 디코더의 내부는 전부 RNN(LSTM/GRU)으로 구성되어 있음. 입력 문장을 받는 RNN셀이 인코더, 문장을 출력하는 RNN셀이 디코더. 
  
- S2S 인코더 : 입력 문장의 모든 단어(임베딩 벡터)를 순차적으로 입력받아 마지막에 모든 단어 정보를 압축, 하나의 벡터(컨텍스트 벡터)로 만듦. 이를 디코더로 전송.
- 인코더 자세히 : 입력 문장이 단어토큰화를 거쳐, 각 토큰 각각이 RNN셀 각 시점의 입력이 됨. 마지막 시점의 은닉상태를 컨텍스트 벡터로 함.
- S2S 디코더 : 인코더로부터 전달받은 컨텍스트 벡터에서 번역된 단어를 한개씩 순차적으로 출력. 기본적으로 RNNLM, 기본적으로 이전시점의 출력만을 입력값으로 함. 
  초기 입력으로 문장의 시작을 의미하는 <sos>가 들어오고, 문장의 끝을 의미하는 <eos>가 나오면 반복종료.
- 디코더 자세히 : <sos>가 들어온 후 다음 올 단어를 예측하고, 그 단어를 다믐 시점의 입력으로 넣는 것을 <eos>가 다음 단어로 예측될 때 까지 반복(테스트 과정). 
  <sos>와 <eos>는 문장의 시작과 끝을 의미하는 어떤 단어(\t, <sos>등)도 될 수 있음. 실제값 시퀀스에서는 시작토큰을 제거해야 함.
- 컨텍스트 벡터(context vector) : 인코더에서 생성된 입력 단어들의 정보가 압축된 벡터. 인코더 마지막 시점의 은닉상태이며 디코더의 첫번째 은닉상태로 사용됨. 

- 교사강요 : 디코더 셀, 훈련과정에서 이전의 예측이 틀렸음에도 그걸 현재의 입력으로 사용하면 현재의 예측도 잘못될 가능성이 높기에, 
  이전 디코더셀의 출력 대신 실제값을 현 디코더셀의 입력으로 사용하는 방법. 

###### 텍스트 요약
- 텍스트 요약 : 큰 원문에서 핵심내용만 간추려 요약문으로 변환. Seq2Seq에 어텐션을 적용해 구현할 수 있음.

- 추출적 요약 : 원문에서 핵심 문장 또는 단어구를 몇 개 뽑아 이로 구성된 요약문을 만드는 방법. 모델의 언어표현능력이 제한된다는 단점이 있음.
- 텍스트랭크 : 추출적 요약의 대표적 알고리즘. 페이지랭크(검색엔진에서 웹페이지의 순위를 정하기 위해 사용되던 알고리즘)알고리즘이 기반이 됨. 
  노드는 문장, 간선의 가중치는 문장간 유사도를 의미함.

- 추상적 요약 : 원문에 없던 문장이라도 핵심문맥을 반영한 새로운 문장을 생성해 원문을 요약하는 방법. 사람이 요약하는것 같은 방법, 대표적으로 S2S. 
  지도학습이라 실제 요약문도 필요로 한다는 단점이 있음.
- 한국어 추상적 요약 : [링크](https://www.slideshare.net/BOAZbigdata/deeptitle?fbclid=IwAR2GHvEwLC-cvnlbjhvbKc0dxT7Xyof0CjooXvrcejqJoPN_cFLk6ewbMjQ)

##### 트렌스포머
- 어텐션 메커니즘 : 신경망(S2S)의 성능증가를 위한 매커니즘이었고, 트랜스포머의 기반이 됨. 입력시퀀스의 길이가 길어지면 출력의 정확도가 떨어지는 현상을 보정하기 위해 등장. 
  이 외에도 텍스트분류 등에서 손실된 정보의 복구를 위해(양방향LSTM등과함께)사용되기도 함. 
- 어텐션 아이디어 : 디코더에서 출력을 예측하는 매 시점마다 인코더의 전체 문장을 다시 참고, 이때 해당 시점에서 예측해야할 단어와 연관있는 부분을 더 집중해서 봄. 
  쿼리에서 모든 키 벡터에 대해 유사도를 구함. 
- 어텐션 함수 : 디코더에서 동작. 주어진 쿼리(현 디코더 은닉상태)에 대해 모든 키(인코더 모든시점 은닉상태)와의 유사도를 각각 구해 키와 맵핑된 각 값(인코더 모든시점 은닉상태)에 반영하고, 
  유사도가 반영된 값을 모두 더해(어텐션값)반환함. 
- 어텐션 함수 자세히 : 이전시점의 은닉상태를 softmax 함수에 통과시켜 각 입력 단어가 출력단어 예측에 얼마나 도움되는지를 구함. 이를 하나의 정보로 담아 디코더에 전송. 
  어텐션값과 현 시점의 은닉상태를 결합해 하나의 벡터로 만들고, 이를(tanh을 거치기도)예측연산의 입력으로 사용함.
- 어텐션 함수 종류 : dot(루옹, 은닉상태 전치 후 내적곱)외에도 scale dot, general, concat(바다나우, 바로이전의 은닉상태를 이용해 어텐션값을 구함), 
  location-base 등의 어텐션 스코어 함수가 있음. 
- 어텐션 스코어 : 현재 디코더의 시점에서 단어 예측을 위해, 인코더의 모든 은닉상태가 현시점의 은닉상태와 얼마나 유사한지 판단하는 스코어값. 
  각 인코더의 은닉상태와 어텐션 가중치값을 곱해 모두 더함. 컨텍스트 벡터(S2S에선 인코더의 마지막 은닉상테)라고도 불림.
- 어텐션 분포 : 모든 은닉상태의 어텐션스코어 모음값에, softmax를 적용한 스키마. 각 값을 어텐션가중치라고 함.

- 셀프 어텐션 : 쿼리, 키, 벨류가 입력문장의 모든 단어벡터들로 동일. 인코더(Encoder) 혹은 디코더에서(Masked decoder) 이뤄짐. 
  입력문장내 단어들끼리 유사도를 구해 단어의 의미(it 등이 무엇을 뜻하는지)를 찾아냄.
- 셀프 어텐션 실행 : 각 단어벡터들에 가중치행렬(단어벡터차원*(단어벡터차원/num_heads)의 크기를 지님)을 곱해 일정크기(단어벡터차원/num_heads)의 쿼리, 키, 벨류 벡터를 얻음.
- 셀프 어텐션 실행 : 이를 이용해 스코어(q*k/√(k벡터 차원), 트랜스포머는 Scaled dot-product Attention을 사용)를 구한 뒤 softmax를 지나 어텐션 분포를 구하고, 
  이를 가중합해 어텐션값을 구함.
- 셀프 어텐션 행렬연산 : 위 과정은 벡터 연산이 아닌 행렬연산을 사용하면 일괄계산이 가능해 행렬연산으로 구현됨. 헤드의 수만큼 병렬을 수행함.
- 셀프 어텐션 행렬연산 과정 : 문장행렬에 가중치행렬을 곱해 Q,K,V 행렬을 구하고, Q와 K를 내적곱(Q*K^t)하고 (q\*k/√(k벡터 차원))로 나눠 어텐션스코어를 얻으며, 
  여기에 softmax를 지나게 하고(어텐션 분포) V행렬을 곱해 어텐션값 행렬을 만들 수 있음.
 
- 트랜스포머 : S2S의 단점을 개선하면서도 인코더-디코더의 구조는 유지. RNN을 사용하지 않고(단어입력 순차적X) 셀프 어텐션을 이용해 문장을 이해. 
  인코더와 디코더가 여러개 존재할 수 있음(병렬화). 각 단어의 임베딩 벡터에서 조정된 값을 얻음. 디코더, 인코더 둘 다 임베딩,포지셔널인코딩을 거친 행렬을 입력으로 받음. 
- 트랜스포머 하이퍼 파라미터 : 입력과 출력의 크기, 인코더와 디코더의 층 수, 어텐션 사용시 사용될 분할 병렬 어텐션의 수, 트랜스포머 내부 신경망의 은닉층 크기 등의 
  하이퍼 파라미터를 사용할 수 있음. 학습률을 경과에 따라 변하도록, 인코더-디코더 뒤에 출력용 전밀집층을 추가해 설정.
- 포지셔널 인코딩 : 각 단어의 임베딩 벡터에 위치 정보를 더해 입력으로 사용, 임베딩벡터가 모인 문장벡터행렬과 포지셔널인코딩 벡터행렬 간의 덧셈을 통해, 
  같은 단어라도 위치에 따라 입력으로 들어가는 임베딩벡터의 값이 달라지게함.
- 포지셔널 인코딩 함수 : 임베딩 벡터 내 각 차원의 인덱스(문장벡터행렬의 (pos,i)에서 i)가 짝수면 sin, 홀수면 cos 함수를 사용. 
  인수는 pos/10000^(i(홀수면 i-1)/트랜스포머 출력차원) 를 사용. 

- 패딩 마스크 : <pad> 등을 마스킹(어텐션 제외 위해 값을 가림)을 함. 입력벡터중 가릴 단어의 위치에 -1e9를 넣어 곱해 그 단어가 학습에 반영되지 않게 함.
- 멀티헤드 어텐션 : d_model 차원을 num_heads로 나눠, d_model/num_heads의 차원을 가지는 Q,K,V에 대해 병렬 어텐션을 수행. 각 어텐션값 행렬을 어텐션헤드 라고 함. 
  다른시각으로 정보의 수집이 가능함. 모든 어텐션 헤드를 연결하고, 그에 또다른 가중치행렬을 곱해 최종결과물을 만들어냄.
- 포지션 와이즈 FFNN : FFNN(x(멀티헤드어텐션 결과)) = MAX(0, xW_1+b_1)W_2 + b_2. 각 벡터가 멀티헤드어텐션을 지나 FFNN을 지나가면서도 원래의 크기는 보존되고, 
  이는 한 인코더의 결과가 다음 인코더로 그대로 들어가기 떄문임.
  
- 인코더 : 레이어층의 개수(하이퍼파라미터) 만큼 인코더층을 쌓음. 인코더층은 크게 두개의 서브층(셀프어텐션, 피드포워드)으로 구성. 
  패딩마스크 > 멀티헤드어텐션/피드포워드신경망 > 드롭아웃/층정규화 같은 순서로 구성되어 있음.
- 잔차 연결 : 인코더에 추가적으로 사용되는 기법. 서브층의 입력과 출력을 더해 모델의 학습을 도움.
- 층 정규화 : 인코더에 추가적으로 사용. 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이를 이용해 값을 정규화 해 학습을 도움.

- 디코더 : 인코더 마지막층의 출력을 각 디코더층 연산에 사용. 교사강요 사용. 룩-어헤드 마스크 사용. 새개의 서브층(멀티헤드셀프 어텐션/인코더-디코더 어텐션/피드포워드)으로 구성. 
  서브층의 이후에는 드롭아웃,잔차연결,층정규화가 수행됨.
- 룩-어헤드 마스크 : RNN을 사용하지 않고 문장행렬 전체를 입력으로 받아 미래 단어까지 참고할 위험이 있어, 그를 막기 위해(가림)사용. 
  첫번쨰 서브층(멀티헤드어텐션)에서 어텐션스코어 행렬에서 마스킹을 적용해 수행됨.
- 인코더-디코더 어텐션 : 멀티헤드어텐션을 수행하기는 하나, 셀프어텐션이 아님. key와 value는 인코더의 마지막 층의 행렬로브터 얻고, query는 디코더 첫번째 서브층의 결과로 얻음. 
  다른 과정은 첫번째층과 같음. 패딩마스크를 입력으로 받음.


##### BERT
- BERT(Bidirectional Encoder Representations from Transformers) : 구글이 공개한 사전 훈련 모델. 레이블 없는 텍스트 데이터를 트랜스포머로 훈련. 
  마스크드 언어모델, 다음문장예측(NSP)을 통해 사전훈련을 함. 
- BERT 구조 : 트랜스포머의 인코더를 쌓아 올림. 문맥을 반영한 임베딩 사용. d_model 크기의 임베딩 벡터를 입력으로 받아 내부 연산을 거쳐, 동일 크기의 벡터를 출력. 
  총 워드피스 임베딩, 포지션 임베딩, 세그먼트 임베딩 3개의 임베딩층이 사용됨.
- BERT 구조 : BERT의 연산을 거친 후의 출력임베딩은 문맥을 반영한 임베딩. 셀프어텐션을 이용해 하나의 단어가 모든 단어를 참고. 이를 모든 층에서 반복.
- BERT 파인튜닝 : 사전학습된 BERT에 풀려하는 태스크의 데이터를 추가 학습시켜 테스트하는 단계. 실질적으로 태스크에 BERT를 사용하는 단계. 
- BERT 사용분야 :  텍스트 분류(CLS위치 출력층에 Dense 추가), 태깅(입략 긱 토큰 위치에 Dense사용), 텍스트 쌍 분류/회귀
  (자연어추론(두 문장간 관계)등, 텍스트사이 SEP, 두종류의 세그먼트 임베딩 사용), 질의응답(질문, 본문 두 쌍 입력, 본문의 일부를 추출해 답변) 등에 사용됨.

- 파인튜닝 : 다른 작업에 대해 파라미터 재조정을 위한 추가 훈련 과정. 레이블 없는 데이터로 사전훈련된 모델에 주로 사용. 
- 포지션 임베딩 : 단어의 위치정보를 사인,코사인이 아닌(포지셔녈인코딩)학습을 통해 얻음. 위치정보를 위한 임베딩층을 하나 더 사용, 위치에 맞는 포지션임베딩벡터를 더함. 

- MLM(Masked Language Model) : 사전훈련을 위해 입력의 15%의 단어들을 랜덤으로 마스킹. 15%중 80%은 mask로, 10%는 핸덤으로 다른 단어로, 10%는 그대로 변경된다.  
- NSP(Next Sentence Prediction) : 두 문장을 준 후 두 문장이 이어지는지 아닌지를 맞추는 방식으로 훈련. 
  각 문장의 시작과 끝([SEP\]), 실제 이어지는 문장인지 표시([CLS\])하는 특별토큰을 사용한다.
- 세그먼트 임베딩 : 문장구분을 위해 BERT가 사용하는 또 다른 임베딩층. 첫문장과 두번째 문장에 각 sent0, sent1 임베딩을 더해주며 임베딩 벡터는 두개만 사용됨.
- 어텐션마스크 : BERT실습에 필요한 추가적 시퀀스 입력. 어텐션 시 패딩토큰에도 어텐션을 하지 않도록 실제와 패딩토큰을 구분하게 알려주는 입력. 0과 1(실제단어)두가지 값을 지님.  

- 한국어용 BERT 패키지 KoBERT가 존재함.

##### GPT
- Generative Pre Training of a language model 의 약어. opne-AI 제작. google은 BERT.
- language model : 현재 알고있는 단어를 기반으로 다음 단어를 예측하는데 많이 사용되는 모델. 레이블링이 불필요하다는 장점이 있음. GPT-1의 핵심. 
  비지도, GPT-1은 다 분야의 엄청난 양의 데이터로 pre trained 됨.  
- generative model : 머신러닝 모델의 학습 방법 중 하나. 데이터가 많을수록 학습이 잘 됨.
- BPE(바이트 페어 인코딩) : 자주 함께 사용되는 char 를 하나의 묶음으로 사용(최소한의 단어). 워드 임베딩과 캐릭터 임베딩의 장점을 모두 가지고 있음. 
  (word)단어간의 유사도와 (char)처음보는 문자의 예측 모두가 가능함. 
- GPT-1 : 문장간 관계 유추, 질의 응답, 문장유사도, 분류등에 뛰어난 성능을 보임. 언어 모델로 학습, 
  파인 튜닝(linear 와 softmax, 추가적인 레이어 없이 적은 양의 레이블 만으로도 가능)두 단계를 거침. 트랜스포머의 디코더기반 모델. 
- GPT-2 : 1과 달리 파인튜닝을 없애고, 사이즈가 커짐. 입력된 값과 수행해야할 task(다음단어, 번역, 응답 등)를 함께 입력받아 다음 단어를 출력. 
- GPT-3 : 파인튜닝 제거(가능하긴 하나 필요 X)를 핵심으로 하고 있음. 제로샷이나 원샷이 아닌 퓨샷러닝(몇장의 이미지만 사용)만을 이용. 
  여러 분야에서 뛰어난 성능을 보이나 단방향으로 학습해 문맥 파악에 약하다는 단점을 가짐.     




#
******


# tensorflow | 토큰,벡터화,임베딩,RNN등
### preprocessing
##### tokenize
- tf.keras.preprocessing.text.text_to_word_sequence(sentence) : 모든 알파벳을 소문자로 변환, 구두점 제거, 죽약형은 분리하지 않는 단어 토큰화 함수. 
  정제와 단어 토큰화를 동시에 적용.
- tf.keras.preprocessing.text.Tokenizer() : 정수 인코딩을 위한 토크나이저 로드. 단어집합 생성과 토큰화를 병행. .fit_on_texts(단어집합)으로 단어 빈도수가 
  높은 순으로 낮은 정수 인덱스를 부여, texts_to_sequences로 변환. .word_index 로 단어와 인덱스를 확인할 수 있고, .word_counts 로 단어의 개수를 확인 할 수 있다. 
- Tokenizer() 매서드 : .texts_to_matrix(문장배열,mode='count')로 DTM(인덱스 0부터 시작)을 생성할 수 있다. 모드가 'binary' 면 단어의 존재여부만 보여주는 행렬을, 
  tfidf 는 tfidf 행렬을, freq 는 (단어 등장 횟수/문서 단어 총합)의 행렬을 보여준다.
- Tokenizer() 메개변수 : num_words(단어 빈도순으로 num_words개 보존), filters(걸러낼 문자모음. 디폴트 - !"#$%&()*+,-./:;<=>?@[\]^_`{|}~\t\n),
  lower(입력 문자열 소문자 변환여부. bool), split(단어분리기준. str), char_level(문자를 토큰으로 취급. bool),
  oov_token(값이 지정된 경우, text_to_sequence 호출 과정에서 word_index에 추가되어 out-of-vocabulary words를 대채) 매개변수 사용가능.
##### vectorize
- tf.keras.utils.to_categorical(벡터) : 원 핫 인코딩을 해줌. (요소 개수, 요소 종류)의 형태를 가짐.
- tokenizer.texts_to_sequences(단어집합) : 각 단어를 이미 정해진 인덱스로 변환. 만약 토크나이저 로드시 인수로 i+1을 넣었다면 i 까지의 인덱스를 가진 단어만을 사용하고 나머지는 버린다.
- tf.keras.preprocessing.sequence.pad_sequences(인코딩된 단어 집합) : 가장 긴 문장의 길이에 맞게 문장의 앞에 0을 삽이비해 ndarray 로 반환. 
  padding='post' 로 문장 뒤에 0을 삽입할 수 있고, maxlen 매개변수로 길이를 지정할 수 있다.
##### embedding
- tf.keras.layers.Embedding(총 단어 개수, 결과 벡터의 크기, 입력 시퀀스 길이) : 단어를 밀집벡터로 만듦(임베딩 층(Dense 같은)제작). 
  모델 내에서 (num of sample, input_length)형태의 정수 인코딩이 완료된 2차원 정수 배열을 입력받아 워드 임베딩 후 3차원 배열을 반환. 
  weights 매개변수에 사전 훈련된 임베딩 벡터의 값들을 넣어 이미 훈련된 임베딩 벡터 사용 가능.
##### pooling
- tf.keras.layers.GlobalMaxPooling1D() : 1차원 풀링 실행. Conv1D 뒤에 위치.
##### normalization
- tf.keras.layers.LayerNormalization/(layers) : 층정규화.
### model
##### RNN
- tf.keras.layers.SimpleRNN(hidden_size) : RNN 사용. hidden_size는 출력(은닉층, 다음 시점에 보내질 값)의 크기. 
  input_shape 매개변수에 (timesteps(입력 시퀀스, 각 문서의 단어 길이), input_dim(입력 크기, 각 단어 벡터 표현의 차원 수)) 로 넣어 입력을 정의해 줄 수 도 있음. 
  임베딩 > RNN > 출력층 만으로도 간단한 자연어 처리(메일 분류 등)가 가능.
##### CNN 
- tf.keras.layers.Conv1D(kernel, kernel_size, padding, activation) : 1차원 합성곱신경망 사용.

# sklearn | BOW, TFID, LDA 등
- sklearn.feature_extraction.text.CountVectorizer() : BOW 표현을 하게 해주는 변환기 로드. .fit(문자열이 담긴 리스트)로 사용, 
  .vocabulary_ 속성에서 반환된 {단어:등장횟수} 형태의 딕셔너리를 볼 수 있음.  tf-idf 와 함께 ngram_range=(연속 토큰 최소길이, 최대길이) 로 연속된 토큰을 고려할 수 있다. 
  보통은 하나만 하지만 많을 때 바이그램정도로 추가하면 도움이 된다.  
- Bow 표현을 만드려면 .transform(list), Scipy 희소 행렬(원소 대부분 0) 저장되어 있으며, .get_feature_names()로 각 특성에 해당하는 단어들을 볼 수 있음. 
  min_df 매개변수로 토큰이 나타날 최소 문서 개수를 지정할 수 있고, max_df 매개변수로 자주 나타나는 단어를 제거할 수 있다. 
  stop_words 매개변수에 "english" 를 넣으면 내장된 불용어를 사용한다.
- sklearn.preprocessing.LabelEncoder() : 여러개의 카테고리가 존재하는 데이터를 고유한 정수로 인코딩하는 클래스 로드.  

- sklearn.feature_extraction.text.TfidVectorizer(min_df=i) : 텍스트 데이터를 입력받아 BOW 특성 추출과 tf-idf 를 실행하고 L2정규화(스케일 조정)까지 적용하는 모델로드. 
  훈련데이터의 통걔적 속성을 사용하므로 파이프 라인을 이용한 그리드 서치를 해 주어야 한다. .idf_ 에서 훈련세트의 idf 값을 볼 수 있다. 
  idf 값이 낮으면 자주 나타나 덜 중요하다 생각되는 것이다.
- sklearn.decomposition.LatentDirichletAllocation(n_components=n, learn_method="online", random_state=k, max_iter=i) : 
  LDA 수행. .fit_transform(X), .components_ 등을 사용할 수 있다.


# gensim | word2vec, FastText
- gensim : 통계적의미론에 초점이 맞춰져, 문서의 구조를 분석한 후 유사성을 기준으로 다른 문서에 점수를 주는
  W2V, D2V, FastText, LDA등과 많이 사용하는 알고리즘에 최적화 되있는 모듈.  
- gensim.models.Word2Vec(sentences, size, window, min_count, workers, sg) : 워드 투 벡터 사용. 
  size(임베딩 벡터 차원), window(윈도우 크기), min_count(최소빈도수), workers(프로세스 수), sg(0-CBOW, 1-Skip_gram)등의 매개변수 사용가능.
- gensim.models.FastText(corpus, size, window, min_count, workers, sg) : FastText 사용. 한국어에서 이걸 사용하려면 음절단위가 아닌 자모(ㄱ,ㅏ,ㄴ,ㅓ)단위로 사용함. 

- 모델(W2V).wv.most_similar(word) : 유사한 단어들과 유사도(확률)출력.
- 모델(W2V).similarity(w1, w2) : 두 단어간 유사도 계산.
  
- 모델(W2V).wv.save_word2vec_format(name) : 모델 저장.
- gensim.models.KeyedVectors.load_word2vec_format(path) : 저장된 모델 로드.

- [google_W2V](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) : 구글 제공 3백만개의 사전훈련된 W2V 단어 벡터 로드. 
  load_word2vec_format으로 .bin 의 다운 파일 사용 가능.  
- [ko_W2V](https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view) : 박규병 님 제공 한국어 W2V 단어 벡터 로드. 

- [embedding_Projector](https://projector.tensorflow.org/) : 임베딩 벡터 시각화 사이트. 
  [!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름] 명령어를 사용해 모델명_metadata.tsv파일과 
  모델명_tensor.tsv 파일을 생성한 후 사용할 수 있음.

# glove | GloVe
- glove : pip install glove_python 으로 다운로드 가능. 워드 임베딩의 방법 중 하나인 glove를 사용할 수 있음.
- glove.Corpus() : 글로브 동시 등장 행렬 생성기 로드. .fit(corpus, window)로 사용가능. 
- glove.Glove(no_components, learning_rate) : GloVe를 수행 클래스 로드. .fit(등시등장행렬.matrix, epochs, no_threads, verbose)로 사용가능.
- glove 메서드 : .add_dictionary(동시등장행렬.dictionary) > 사전 추가 | .most_similar(word) > 비슷한 단어들과 유사도 반환. 

# SentencePiece | subword
- sentencepiece : BPE를 포함한 기타 서브워트 토크나이징(내부단어분리)알고리즘 내장 패키지. 사전 토큰화 작업 없이 단어분리 토큰화를 수행해 언어무관 사용가능.
  
- sentencepiece.SentencePieceTraner.Train() : 서브워드 토큰화. 단어 집합과 각 단어를 정수 인코딩. 
  결과로 설정파일명.model 과 파일명.vocap 의 파일이 생성, vocab 파일에서 학습된 서브워드들을 확인할 수 있음.
- 사용가능 매개변수 : input(학습시킬 파일), model_prefix(만들어질 파일(모델) 이름), 
  vocab_size(단어 집합의 크기), model_type(사용할 모델 (unigram(default), bpe, char, word)).
- 사용가능 매개변수 : pad_id, pad_piece(pad token id, 값), unk_id, 
  unk_piece(unknown token id, 값), os_id, bos_piece(begin of sentence token id, 값), eos_id, eos_piece(end of sequence token id, 값).
- 사용가능 매개변수 : max_sentence_length(문장의 최대 길이), user_defined_symbols(사용자 정의 토큰).

- sentencepiece.SentencePieceProcessor() : 센텐스피스 프로세서 로드. .load(파일명.model)로 저장한 모델 로드.
- 모델 메서드 : .encode_as_pieces(line)로 문장을 서브워드 시퀀스로, .encode_as_ids(line)로 문장을 정수 시퀀스로 변경할 수 있음.
- 모델 메서드 : .GetPieceSize()(단어집합크기), .idToPiece(int)(정수 > 서브워드), .PieceToId(subword)(서브워드 > 정수)
- 모델 메서드 : .DecodeIds(int list)(정수시퀀스>문장), .DecodePieces(서브워드 시퀀스)(서브워드시퀀스>문장), .encode(문장, output=str/int)(문장 > 서브워드/정수 시퀀스) 

# transformers | Bert
- transformers.BertTokenizer() : BERT토크나이저 로드. .from_pretrained(sent)로 사전훈련된 토크나이저(WordPiece와 비슷)사용. 
  .vocab[key\]로 단어가 있는지, 어떤 정수인지 확인 가능.

# replacers | 텍스트 대체, 반복 삭제
- replacers.RegexpReplacer() : 텍스트 대체 클래스 로드. .replace(text)로 사용, 축약을 해제하고 단어 토큰화까지 진행해 리스트로 반환.
- replacers.RepeatReplacer() : 반복 문자 삭제 클래스 로드. 위와 동일하게 사용할 수 있으며, 반복된 단어를 일반 단어로 바꿔 반환한다. 
  nltk 의 wordnet.synsets(word)에 이미 있다면 처리하지 않도록 하면 일반 단어는 반복을 삭제하지 않는다.
- replacers.WordReplacer({'바꿀단어':'바뀔단어'}) : 단어를 동의어로 변환하는 클래스 로드. 마찬가지로 사용, 목록에 있는 단어는 바꿔서, 아니면 그대로 반환한다.


# NLTK | NLP(영어토큰화, 전처리 도구들)
- nltk.download() : NLTK 세트 다운로드. 특정 세트의 이름을 넣으면 그것만 다운로드한다.
- nltk.Text(tokens) : 토큰들을 다시 문장(iter가능)화. .tokens{토큰확인}, .plot(), 
  .concordance(word){비슷한단어추출}, .vocab(){단어빈도수체크, .most_common(i)사용가능.} 

### tokenize
- nltk.tokenize.sent_tokenize(text) : 문장 토큰화함수. 문서를 문장단위로 나눠준다. 
  PunktSentenceTokenizer인스턴스(문장의 시작과 끝을 표시하는 문자나 문장 기호에 기초해 다른 유럽언어로 토큰화를 수행)를 사용해 punkt의 다운로드를 필요로 함. 
- 영어가 아닌 언어를 토큰화 하려면 'tokenizer/punkt/언어.pickle' 파일을 로드하고 사용하면 된다. 로드한 언어.pickle에
  .tokenize(text) 매서드를 사용해서도 토큰화를 사용할 수 있다.
  
- nltk.tokenize.word_tokenize(sentence) : 단어 토큰화 함수. 문장을 단어 단위로 나눠주며 축약형의 경우 단어의 의미가 유지되게(do , n't)분리한다. 
  TreebankWordTokenizer 를 사용한다.
- nltk.tokenize.sent_Tokenizer(text) : 문장 토큰화 함수. 문장 내부에 구두점이 포함되어 있어도 잘 작동한다.
- nltk.tokenize.WordPunctTokenizer() : 또 다른 단어 토크나이저. 구두점을 별도로 분리한다. 축약형의 경우 '전, ', '후 총 세가지로 나눈다.
- nltk.tokenize.TreebankWordTokenizer() : 트리뱅크워드 토큰화 함수 로드. .tokenize(Sentence) 로 토큰화를 수행할 수 있다. 분리된 축약형('Do', 'n't')으로 작동된다. 
- nltk.tokenize.WhitespaceTokenizer() : 화이트 스페이스(탭 제외?) 으로 단어 토큰화. 토크나이저.span_tokenize(sent)로 토큰의 오프셋인 튜플의 순서를 받을 수 있다.
- nltk.tokenize.util.string_span_tokenize(문자열, "separator(구분자)") : 구분자대로 분할해 전송된 토큰의 오프셋을 반환.

- nltk.tokenizer.RegexpTokenizer(정규 표현식) : 정규 표현식을 이용한 단어 토큰화 클래스 로드. .tokenize(String)으로 사용할 수 있다. 
  gaps = True 로 화이트 스페이스를 사용한 토큰화를 할 수 있다. 
- nltk.tokenizer.regexp_tokenize(sentence, patten='정규 표현식') : 정규 표현식 단어 토큰화 함수 로드. 

### stem
- nltk.stem.WordNetLemmatizer() : 표제어 추출기 생성. .lemmatize(word)로 표제어 추출 사용 가능. 정확한 추출을 위해선 (word, 품사)식으로 넣어주어야 한다.
- nltk.stem.PorterStemmer() : PorterStemmer 알고리즘의 어간 추출기 생성. .stem(토큰.norm_.lower()) 으로 어간(용언에서 뜻을 나타내는 불변인 부분)을 찾음.
- nltk.stem.LancasterStemmer() : Lancaster 어간 추출기 로드. .stem(word) 로 사용할 수 있다.

### Freq
- nltk.FreqDist(단어집합) : 단어를 키로, 빈도수를 값으로 저장해 리턴. 결과.most_common(i)로 상위 i개의 단어만 보존가능. 아래와 똑같은 효과를 지님.
- collections.Counter(단어집합) : 단어 집합에서 중복을 제거하고 단어의 모든 빈도를 쉽게 계산함. 이 메서드로 간단하게 등장단어와 빈도 파악이 가능.

- nltk.tag.pos_tag(토큰 리스트) : 품사 태깅을 수행. (단어, 품사)의 형태로 나타남. PRP(인칭대명사),VBP(동사),RB(부사),VBG(현재부사),IN(전치사),NNP(고유명사),
  NNS(복수명사),CC(접속사),DT(관사)를 의미함.
- nltk.ne_chunk(품사태깅([(토큰,품사)]형태)) : 개체명 인식을 수행. 개체명 인식이 된것(명사(NNP)만 인식)은 ()로 묶여있고, 
  최종(S (개체명, 단어) 단어 단어 단어 (개체명, 단어))식으로 배치된다.

### corpus
- nltk.corpus.stopwords : 불용어 처리 클래스 로드. .words('언어')로 불용어 리스트를 받아올 수 있으며 if w not in words 롤 거를 수 있고, 
  .fileids()로 불용어 목록이 있는 언어를 확인할 수 있다. 한국어 등은 직접 txt 파일등에 목록을 만들어 제거하는게 편함.
- nltk.corpus.wordnet : 프린스턴 대학의 동의어 집합 세트. synsets(word).definition()으로 단어의 유사어 확인 가능, .path_similarity(synsets)로 단어의 유사도 확인 가능. 
- nltk.corpus.alpino : 알피노 코퍼스(네덜란드 신문에 나오는 단어 모음) 로드. .word()로 내부의 단어들을 꺼낼 수 있다. 다른 것들도 사용가능. 
  전부 nltk.download(corpus)뒤에 사용 가능하다.

### metrics
- nltk.metrics.accuracy(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정확도(같은 정도)반환.
- nltk.metrics.precision(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정밀도(TP/(TP+FP))반환.
- nltk.metrics.recall(sentence1, sentence2) : 두 토큰화된 단어 리스트의 재현율(TP/(TP+FN))반환.
- nltk.metrics.f_measure(sentence1, sentence2) : 두 토큰화된 단어 리스트의 f1점수(정밀도와 재현율의 조화 평균(역수의 평균의 역수,곱/합))반환.

- nltk.metrics.edit_distance(word1, word2) : 두 단어간 편집거리 반환.
- nltk.metrics.jaccard_distance(set1, set2) : 두 세트간 자카드 계수 반환.
- nltk.metrics.binary_distance(set1, set2) : 두 세트간 이진거리 계수 반환.
- nltk.metrics.masi_distance(set1, set2) : 두 세트간 매시거리 계수 반환.

### ngrams
- nltk.util.ngrams(단어 리스트, n) : n개의 토큰이 연결되어 있는 n그램 생성. 
- nltk.collocations.BigramCollocationFinder : 바이그램 탐색기. .from_words(tokens)로 토큰을 저장할 수 있다.
- 토큰저장탐색기.nbest(nltk.metrics.BigramAssocMeasures.likelihood_ratio, n) : n개의 바이그램을 찾아 리스트를 받아볼 수 있다.
- 토큰저장탐색기.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq) : 바이그램을 찾는 또다른 방법.
- nltk.probability.LidstoneProbDist(fd, gamma=f, bins = n) = 최대 우도 추정 사용. fd(빈도분포)를 기반으로 f(0~1)를 사용해 n개의 샘플을 생성. 샘플들의 총 합은 1. 

### translate
- nltk.translate.bleu_score(candidate.split(), refrences.split()) : BLEU score 측정. 

# spaCy | 토큰화(영어)
- spacy.load(언어) : 해당 언어의 처리를 위한 sapcy로드. 언어는 'en'등으로 사용.
- 로드된spacy.tokenizer(sent) : 문장을 단어단위로 토큰화. 각 토큰들은 .text 로 문자열 형태로 받을 수 있음.


# KoNLpy | 한글 분석(토큰화, 태깅)
- konlpy : 한글 분석을 가능하게 함. 자바로 이뤄져 있어 JDK 1.7 이상과 JPype 가 설치되어 있어야 함. 각 분석기는 성능과 결과가 다르게 나와 용도에 따라 적절한 것을 선택해야 함.
- 형태소 분석기 종류 : 메캅(MeCab), Okt(Open Korea Text(Twitter)), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)등의 형태소 분석기 사용 가능.
- 함수 : konlpy.tag.분석기명()으로 분석기사용, .morphs(text){토큰화}, .pos(text){토큰화 후 품사 태깅(.tagset으로 종류확인)}, .nouns(text){명사만 추출}에 
  Okt.phrases(text){구문별로 나눔}, Kkma.sentences(text){문장별로 나눔}, 한나눔.analyze(text){형태소후보 모두반환}등이 사용가능.
- 매개변수 : .pos(), .morphs() 사용시 norm=bool(일정 수준의 정규화), stem=bool(표제어(원본글자)로 변형)등의 매개변수를 사용할 수 있음.
  
- MeCab : 띄어쓰기에서 속도/정확도 모두 뛰어남. 지능형 형태소 분석기(결과 수작업 수정가능), 단어 추가가능. 미등록어 처리/동음이의어 처리의 문제가 있음.  
  C/C++로 개발, CRF채용. 사용자 사전 추가시, mecab디렉토리의 user-dict에서 단어를 추가 후, add-userdic-win.ps1을 powershell에서 실행.
  [기본사전](https://github.com/Pusnow/mecab-ko-dic-msvc/releases/tag/mecab-ko-dic-2.1.1-20180720-msvc) 과
  [Mecab실행기](https://github.com/Pusnow/mecab-ko-msvc/releases/tag/release-0.9.2-msvc-3) 를 다운로드 받고,
  [Mecab python_wheel](https://github.com/Pusnow/mecab-python-msvc/releases/tag/mecab_python-0.996_ko_0.9.2_msvc-2) 에서 맞는 버전(3.7이 최신)을 설치 후
  [pip install whl파일] > [import MeCab] 혹은 [import konlpy.tag.Mecab > mecab = Mecab(mecab-ko-dic파일경로)]로 Mecab의 사용이 가능함.
- Okt : 띄어쓰기에서 가장 좋은 성능, 정제되지 않은 데이터에 대해 강점. 분석 범주가 다소 적으나 이모티콘/해쉬태그 등 인터넷텍스트에 특화된 범주가 추가.
  어근화, 정규화, 토큰화등이 가능, 미등록어 처리/등음이의어처리/분석범주적음 등의 문제가 있음. 스칼라/java로 개발.    
- Kkma : 띄어쓰기 오류에 덜 민감함. 분석시간이 꽤 길고, 정제된 언어가 사용되지 않는 문서에 대한 정확도가 낮음. 세종품사태그에 가장 가깝고, 분석범주 또한 다양.
  java로 개발, 동적프로그래밍을 이용해 모든 형태소분석 후보를 생성 후 적합한 순서대로 정렬, 기분석 사전을 이용한 인접조건검사방식(속도)과 HMM에 기반한 확률모델(품질)이용 최적화.
- Komoran : 빠른 속도와 보통의 분석품질. 여러 어절을 하나의 품사로 분석가능해, 공백이 포함된 고유명사를 더 정확히 분석가능. 개발자가 지속적으로 업데이트.
  자소분리/오탈자 문장에 대해서도 괜찮음. 로딩시간이 기나 분석속도는 빠름. 띄어쓰기 없는 문장엔 취약. java로 개발, HMM알고리즘 사용.
- Hannanum : 로딩시간이 빠른편. 전체시스템이 각 모듈들의 조합(입력필터/문장분리/형태소분석/미등록어처리/형태소분석후처리/태거)으로 구성됨. 
  띄어쓰기 없는 문장과 정제된 언어가 사용되지 않는 문서에 대한 정확도가 낮음. 상위 6개 태그에 대해 20개의 태그를 세분화해 사용. java로 개발, HMM알고리즘 사용.


### ckonlpy
- [pip install customized_konlpy] : 사용자사전 추가를 위한 패키지 설치. 
- 클래스 : konlpy와 같은 .tag.Twitter()등을 사용해 형태소분석기 등을 사용할 수 있음.
- 형태소분석기.add_dictionary(word, 품사) : 형태소분석기 사전에 단어 추가.

# soynlp | 반복 제어, 품사태깅, 단어 토큰화
- SOYNLP : 품사 태깅, 단어 토큰화 등을 지원. 비지도 학습으로 토큰화. 학습에 필요한 문서를 다운로드할 필요가 있음.
- soynlp.DoublespaceLineCorpus("txt파일.txt") : 데이터를 다수의 문서로 분리함.
- soynlp.normalizer.emoticon_normalize(sent, num_repeats=i) : ㅋㅋ,ㅎㅎ 등의 이모티콘을 i개 까지만 반복되도록 변환.
- soynlp.normalizer.repeat_normalize(sent, num_repeats=i) : 의미없이 반복되는 글자를 i개 까지만 반복되도록 변환.

# hgtk | 한국어 자소단위로 쪼개기
- hgtk.text.decompose(sent, compose_code) : 문장을 자소단위로 쪼갬. 글자간 경계 문자는 생략시 기본값인 ᴥ가 사용됨.
- hgtk.text.compose(sent, compose_code) : 자소단위의 문장을 다시 합침. compose_code에 넣은 문자는 글자간 경계 문자를 알려주는 역할.

# 한국어 전처리 | 띄어쓰기, 맞춤법, 문장 토큰화
- PyKoSpacing : 한국어 띄어쓰기 패키지. 띄어쓰기가 없는 문장을 띄어쓰기를 한 문장으로 변환해줌. pykospacing.spacing(문장)으로 사용할 수 있음. 
- Py-Hanspell : 네이버 맞춤법 검사기를 바탕으로 제작된 맞춤법(띄어쓰기 포함)보정 패키지. hanspell.spell_checker.check(문장).checked 로 개선된 문장을 볼 수 있음.
- kss.split_sentences(text) :  kss 모듈의 한국어 문장 토큰화 함수. 


# re | 정규표현식
- 정규 표현식 사용을 지원.
- 정규표현식
> [] : []안의 문자들 중 하나와 매치. [abc]면 a,b,c중 하나와 매치를 뜻함. [a-c],[a-zA-Z]식으로 범위를 지정할 수 도 있고, 
> []안, 패턴 앞에 ^ 를 붙혀 패턴 부정을 나타낼 수 도 있음. []가 없으면 그 패턴과 정확히 일치.
> 특수 문자 : .(임의 문자 1개), ?(앞 문자 0 또는 1개), *(앞 문자 0개 이상), +(앞 문자 1개 이상), ^(뒤 문자로 문자열 시작), $(앞 문자로 문자열 종료). 
> 패턴의 바로 앞/뒤에 위치해야 함.
> 메타문자 : 정규 표현식 내에서 특수한 역할을 하는 문자. 앞에 \를 붙여야 패턴으로 인식됨. |`$ *+.?([\^{ | 총 12개. []안에선 | \^-] | 만이 메타문자.
> 특수기호 : \d(숫자), \D(숫자가 아닌것), \s(whitespace, \t\n\r\f\v), \S(부정), \w(문자+숫자), \W(문자+숫자의 부정), \b(단어경계(단어-비단어)), 
> \B(부정,(단어-단어|비단어-비단어))등이 있다.
> 범위 : 문자{n,m}은 n번 부터 m이하 반복, {n}은 반드시 n번 반복으로 사용된다. a|b|c 는 여러 문자중 하나에 매칭임. 
> 캡쳐그룹 : 패턴을 ()로 감싸면 캡쳐그룹으로 만듦. \1,\2 등으로 그 순서의 그룹을 사용 가능하며, 매칭되지 않았다면 반환하지 않음. (?P<이름>표현식), (?P=이름)으로 그룹에 이름을 줘 사용.

- re.compile() : 정규 표현식 컴파일. 결과 객체 반환. re.S(.이 \n을 포함하게 함) 등을 매개변수로 줄 수도 있다.
- re.sub(r'패턴', 바꿀문자, 바뀔 문장) : 바뀔 문장에서 패턴에 일치하는 부분을 바꿀 문자로 바꿈. 패턴의 앞에는 r을 붙여줘야 함.
- re.subn() : sub와 같은 기능을 하지만, 튜플((new_string, number_of_subs_made))을 반환함.
- re.match(패턴, 문자열) : 컴파일을 거치지 않고 매치 사용. .group()으로 매치된 문자열을 볼 수 있다. 
- re.search(패턴, 문자열) : match 와 동일하지만 문장의 경우 문장 전체를 검색한다. .start()로 시작 위치, .end()로 끝 위치, .span()으로 (시작, 끝)을 받을 수 있다.
- re.split(패턴, 문자열) : 정규 표현식을 기준으로 문자열을 구분해 리스트로 리턴.
- re.findall(패턴, 문자열) : 정규 표현식에 맞는 단어만 리스트로 반환. 
- re.finditer(패턴, 문자열) : 정규 표현식에 맞는 단어만 각각을 매치 객체의 형태로 하여 반복 가능한 개채의 형태로 돌려준다. 
- re의 메서드는 전부 re.compile(표현식)후 컴파일.메서드(문자열) 형태로 사용할 수 있다.


