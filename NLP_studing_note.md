# have to study
> RNN -> Conv1d -> Attention -> transformer -> transferlearning

> -데이터 전처리
>     - bpe
>     - 형태소 분석기(konlpy, mecab)
>     .... 등

> variation of RNN - RNN, GRU, LSTM
> variation of transformer
> encoder - Bert
> decoder - gpt
> Bert + gpt -> Bart(generator)
> mnist-> 영화 감성 분류 (IMDB) -> 캐글 재난 분류

# 자연어 처리
- 자연어와 컴퓨터간 상호작용에 대함. 인공지능과 컴퓨터 언어학의 주요 분야중 하나.
- 하이퍼 파라미터 : 사용자가 직접 값을 선택해 성능에 영향을 주는 매개 변수. 
- EDA(Exploratory Data Analysis) : 탐색적 데이터 분석. 데이터 내 값의 분포, 변수간 관계, 결측값 존재 유뮤 등을 확인하는 데이터 파악 과정.

### 전처리

##### 토큰화 
- 텍스트를 토큰이라는 작은 부분으로 분할하는 과정. 문장 토큰화와 단어(단어, 단어구, 형태소) 토큰화로 나뉜다.
- 토큰화 중 토큰화의 기준(축약형 등의 경우)을 선택해야 하는 경우도 있고, 구두점이나 특수 문자가 고유한 뜻을 가지고 있거나 단어의 해석에 도움이 되는 경우도 있어 단순 제외는 피해야 한다.
- 또 단어 내에 뛰어쓰기나 구두점이 있는 경우도 있어 토큰화 함수는 그런 단어들을 하나로 인식할 수 있는 능력 또한 필요함.  
- 문장 토큰화의 경우에도 문장 내에 구두점이 있는 경우가 있기에 단순 구두점을 기준으로 구분해서는 안된다.   
- 단어 매칭, 공백 매칭 등의 정규표현식 표현 방법으로도 단어 토큰화를 수행할 수 있다.
- 마침표의 처리를 위해 입력에 따라 두 클래스로 분류하는 이진 분류기를 사용하기도 하며, 이를 위해 [약어사전]이 유용하게 쓰임.
- 한국어는 조사까지 분리해줄 필요가 있기에 단순 공백으로 구분할 수 없(형태소 토큰화 필요)고, 띄어쓰기가 비교적 잘 지켜지지 않는 등 토큰화가 어려움.
- 품사 태깅 : 품사에 따라 뜻이 달라지는 단어를 위해 토큰화 과정에서 각 단어가 어떤 단어로 쓰였는지 구분해 놓는 것.
- 사전에 추가 : 사람 이름 등 고유한 단어로 토큰화 되어야 하는 것들은 형태소 분석기에 사용자 사전을 추가해 분리되지 않게 할 수 있다.

##### 정제
- 정제 : 갖고 있는 코퍼스에서 노이즈 데이터를 제거. 토큰화보다 앞서 이뤄지기도, 이후에도 남아있는 노이즈 제거를 위해 지속적으로 이훠지기도 함.
- 노이즈 데이터 : 무의미하거나, 목적과 다른 불필요 단어이거나, 특수문자 등을 뜻함.
- 등장 빈도가 적은 단어, 길이가 짧은 단어(영어)등을 제거해 정제할 수 도 있음.

##### 정규화
- 정규화 : 표현 방법이 다른 같은 의미의 단어들을 같은 단어로 만드는 작업. 
- 문장부호(구두점)제거, 전체 대/소문자화, 숫자 단어화, 약어 전개 등의 자연어 텍스트 처리 수행을 위한 과정.
- 문장부호 제거는 반복문이나 기타 클래스/메서드로, 대/소문자화는 String.upper()/lower()로 가능하다.
- 단, 단어 내에 문장 부호가 있거나 단어가 대소문자 구분을 필요로 하기도 하기에 무턱대고 수행해서는 안된다.

###### 텍스트 대체  
- 축약이나 줄임말 등을 본래대로 풀어 처리를 효과적이게 하는 과정.
- 정규 표현식 이용 텍스트 대체:  import re >  [(r'won\'t', 'will not'), (바꿀단어, 바꿜 단어)]식으로 제작 > re.compile(바꿀 단어) > re.subn(컴파일, 바뀔 단어, text)[0] 의 과정을 거쳐 할 수 있음.

###### 반복 문자 처리
- 무의미하고 오류를 일으키는 반복문자를 일반 문자로 변환.
- 반복문자를 포함하는 단어를 역참조 방식을 사용해 제거.

###### 단어 동의어 대체
- 더 훌륭한 성능과 적은 오류를 위해 같은 의미의 단어를 하나로 변환.

##### 어간 추출 (표제어 추출)
- 표제어 추출 : 단어들을 표제어(기본 사전형 단어)로 바꿔가는(찾아가는) 과정.
- 어간 : 단어의 의미를 담고 있는 핵심 부분. 
- 형태학적 파싱 : 어간과 접사를 분리하는 작업.
- 한국어 : 한국어는 5언 9품사 구조를 가지고 있으며, 이중 용언(동사, 형용사)이 어간과 어미의 결합으로 이뤄져 있음.

##### 불용어 처리
- 불용어 처리 : 문장의 전체적 의미에 크게 기여하지 않는 불용어를 검색 공간 줄이기 등의 이유로 제거하는 과정.
- 지주 등장하나 도움이 되지 않는 조사, 접미사 등을 제거함. 미리 정의된 불용어 사전이나 직접 정의한 불용어 들을 제거할 수 있음.

##### 문장 벡터화 (정수 인코딩)
- 국소표현 : 해당 단어만 봄. 이산표현 이라고도 함.
- 분산표현 : 주변을 참고해 그 단어를 표현. 연속표현 이라고도 함.
  
- 문장 벡터화 : 자연어 처리에서 입력받는 단어들은 모델이 사용할 수 없기에 원핫 인코딩 등을 통해 숫자화시키는 과정.
  
- 카운트 기반 단어 표현 : 정수 인코딩 방법중 하나. 이름대로 등장 빈도 등 횟수에 따라 인덱스를 부여.
- 빈도수로 벡터화 : (토큰화>정제,불용어 제거>등장 단어,빈도수 기록)의 과정을 거쳐 각 단어의 빈도수가 높은 단어부터 낮은 인덱스를 부여하는 방법으로도 벡터화가 가능하다.  
- 원 핫 인코딩을 이용한 벡터화 : 단어의 개수가 늘어날 수록 벡터 저장을 위한 공간이 늘어나고, 단어의 유사도를 표현하지 못한다는 문제점을 가짐.
- OOV(Out-Of-Vocabulary) : 정제, 불용어 처리 과정을 거쳐 생긴 단어집합(모든 단어를 중복을 허용하지 않고 모아놓음)에 없는 단어. 따로 OOV 의 레이블을 만들어 OOV 를 인코딩 해줘야 한다.
 
- bag of words : 정수인코딩 방법 중 하나. 단어의 출연 빈도로 문장을 나타냄. 문장의 유사도파악 등 에도 사용됨.
- bag of words 단점 : Sparsity(단어의 개수가 많다보니 0이 많아 계산량이 많아짐), 흔한 단어의 힘이 세짐, 단어의 순서를 완전 무시, 처음 보는 단어는 처리 불가 등의 단점이 있다.
- DTM(Document Term Matrix, 문서 단어 행렬) : 서로 다른 문서의 Bow 들을 결합한 표현 방법. 각 BoW 문서를 행으로, 단어를 열으로 하는 행렬로 만듦. 희소표현(원핫벡터와 마찬가지, 0이 많아져 공간에 문제), 단순빈도수접근(불용어 처리 불가)등의 문제점을 가짐. 
- TF-IDF(term frequency inverse doc freq) : DTM, 문서에서 각 단어별 문서 연관성(문서에서 가진 정보)을 파악. 단어빈도와 역문서빈도를 사용해 각 단어들마다 중요도를 가중치로 부여. 모든 문서에서 자주 등장시 중요도를 낮게, 특정 문서에서 자주 등장시 중요도를 높게 함. TF(특정 문서 d 에서 특정 단어의 등장 횟수)*IDF(log(n/1+(t가 등장한 문서 수), 종종 자연 상수(ln)))로 이뤄짐.
- IDF : 자주 등장하는 단어(불용어)에 패널티를 주어 위의 것을 보완한 방법. log(총 문장 개수/단어 출현 문장 개수(+1, DIV_0를 피하기 위해)) 의 공식을 사용함.  TF-IDF 의 결과는 (데이터 개수, 사용 단어 수)로 나타난다.

###### word embedding (단어간 유사도 파악 + 인코딩)
- Word Embeddings : 단어의 유사도 파악. 단어를 인코딩시 단어의 유사도를 반영할 수 없어 임베딩을 사용. 다른 인코딩보다 더 작은 차원에, 유사도에 따라 단어가 가까이 위치함.
- ConceptNet : 지식 그래프 사용, 단어간의 관계를 정의해 유사도 측정.
- Word2Vec : 임베딩의 일종. 비슷한 위치의 단어들에서 유사도를 얻음(유사하게 설정함). skip gram 을 이용해 단어와 이웃으로 나누고 인코딩. 처음보는 단어도 잘 학습하게 도와준다.
- 이미 여러 유사단어들이 선행학습된 Word2Vec 을 텐서플로우허브에서 다운로드 받아 간편하게 사용할 수 있다. word2vec 과 globe 등은 이미 학습된 단어로 워드 임베딩을 실행하나 각 단어가 한개의 벡터로만 표현된다는 문제를 가짐.  
- LM : 모델 학습에 따로 레이블링 할 필요 없다는 장점. 순방향과 역방향이 있으며 둘을 모두 합친걸 biLM 이라고 한다. 
- ELMo : 문맥에 따라 단어 임베딩. 문장을 입력으로 받아 문장에 따라 다른 임베딩으로 출력가능. 
- ELMo 학습 : 순방향 LM은 현재 단어로 다음 단어를 예측하도록 학습시키고, 역방향 LM은 문장에서 뒤의 단어로 앞의 단어를 학습하도록 함. 모든 레이어의 출력값을 이용해 임베딩.  
- ELMo 순방향 LM : 먼저 단어를 char 임베딩으로 전환하고, 그것이 첫 lstm 셀로 이동된 뒤 학습을 진행한다. 첫 출력은 문자임베딩과 residual connection(상위 레이어가 하위의 특징을 잃지 않고, 역전파를 통한 그레디언트 베니싱을 극복하도록 함)을 가지고 있다.  
- ELMo 역방향 LM : 방향만 다르게 순방향과 같은 역할을 함. 끝 단어부터 첫 단어까지 예측.

##### 토픽 모델링 (주제 찾기)
- 토픽 모델링 : 문서의 주제를 발견하기 위한 텍스트마이닝 기법.
- 행렬 용어 : 전치행렬 - 원래 행렬에서 행과 열을 바꿈 | 직교행렬 - 원래 행렬\*전치행렬,n\*n 행렬에서 원 행렬*전치행렬 = 단위행렬을 만족해야 함. | 단위행렬 - 대각행렬(정사각 아니여도 됨, 아니여도 (i,i) 가 1), 대각선은 1, 나머지는 0인 정사각 행렬. | 역행렬 - A\*어떤 행렬 이 단위 행렬일 때 어떤 행렬. 
- SVD(특이값 분해) : m\*n 차원의 행렬 A 를 UΣV^T 로 분해하는 행렬분해 방법. U(m\*m 직교행렬), V(n\*n 직교행렬), Σ(m*n 직사각 대각 행렬) 로 분해한다.
- Truncated SVD(절단된 SVD) : 대각 행렬 Σ의 원소값 중 상위 t 개만 남음. t 는 클수록 많은 정보를 가져 갈 수 있지만 작을수록 노이즈 제거 가능. 원 행렬 복구 불가.
- LSA(latent Semantic Analysis, 잠재 의미 분석) : 단어의 의미를 고려. DTM 을 축소해 축소 차원에서 근접 단어를 토픽으로 묶음. 기존 TF-IDF에 절단된 SVD 사용. 단어를 행, 문장을 열로 나타낸 뒤, SVD 를 사용해 세개의 매트릭스(토픽을 위한 단어, 토픽 강도, 문장)로 나타냄. 뒤의 두 매트릭스를 곱해 단어간 유사도를 파악. 쉽고 빠른 구현과 단어의 잠재 의미를 이끌어 내지만 새 정보의 업데이트가 어렵단 문제점을 가짐. 
- LDA(Latent Dirichlet Allocation, 잠재 디레클레 할당) : 토픽 모델링 대표 알고리즘. DTM 이나 TF-IDF 행렬을 입력으로 해 역공학(문사 작성 과정 역추적)을 해 토픽 추출. 단어가 특정 토픽에 존재할 확률과 특정 토픽이 존재할 확률을 결합확룰로 추정해 토픽을 추출한다.    

###### S2S
- 단어별로 번역하는 방법은 문장의 순서나 원문과 번역문의 글자수 차이가 날 때 번역이 어려움.
- Sequence2Sequence(encoder-decoder architecture) : 각 단어의 모든 정보를 순차적으로 받아 함축된 문맥 벡터를 만드는 인코더와 그걸로 번역을 하는 디코더로 이뤄짐. 문장이 길어지면 문제발생.
- 어텐션 매커니즘 : S2S 를 발전시켜 각각의 스테이트를 활용해 디코더에서 문맥 벡터(컨텍스트 벡터)를 만듦. 벡터의 크기가 동적이라 문장 길이의 제약이 없고, 중요한 단어에만 집중하도록 할 수 있음. 
- 각 입력에서 나온 스테이트와 최종 출력을 tanh 에 넣고, 그것을 softmax에 넣어 단어의 중요도를 판단할 수 있으며, 그걸 이용해 컨텍스트 벡터를 만든다. 최초 번역은 스타트를 넣고 결과를 인코더에 다시 넣은 뒤, 그 뒤로 tanh 부터 다시 거쳐 뒤의 단어를 번역해나감. 
- teach forcing : 어텐션 메커니즘에서 예측결과가 잘못되면 그걸 더한 프리 커넥트도 문제가 생기기에 예측이 아닌 정답을 넣어주는 방법.

##### 패딩
- 패딩 : 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업. 이를 통해 문장들을 하나의 행렬로 보고 병렬처리를 가능하게 함.
- 방법 : 정수 인코딩시 문장별로 단어를 모아 두어 문장의 길이를 확인할 수 있는데, 이때 가장 긴 문장의 길이에 맞게 다른 문장에 0을 삽입한다.

### 모델, 평가

##### 언어 모델
- 언어 모델(LM) : 문장에 확률을 할당하는 모델. 문장에 대해 그 문장이 적절한지, 말이 되는지(문법,문맥 등) 등을 판단. 보편적으로 이전 단어가 주어지면 다음 단어를 예측하도록 함. 혹은 빈칸 추론을 시킴. 통계를 이용하거나 ANN 을 이용해 만들 수 있음.
- 언어 모델링 : 주어진 단어들로 아직 모르는 단어를 예측하는 것.  
- 언어모델의 기능 : 기계번역에서 문맥을 파악하거나, 오타를 교정하거나, 음성인식을 더 잘 되게 도와주는 등 보다 적절한 문장을 판단함.  
- 조건부 확률의 연쇄 법칙 : 조건부 확률(사건 B가 일어난 경우 사건 A가 일어날 확률)이 가지는 특징. P(x1,x2...xn)=P(x1)P(x2|x1)...P(xn|x1...xn−1). 이를 문장에 적용해 다음 단어에 대한 예측 확률을 모두 곱하여 문장의 확률을 구할 수 있음. 
  
- SLM(Statistical Language Model) : 통계적 언어모델. 카운트에 기반해 확률을 구함(count(현 문장+예측할 단어)/count(현 문장)). 데이터 부족시 희소문제(데이터 부족으로 정확한 모델링이 불가능한 문제)가 발생하며 이를 완화하기 위해 n-gram,스무딩,백오프등 일반화 기법이 있으나 결국 NN 언어 모델로 넘어가게 됨.  
- N-그램 : n개의 토큰이 연속적으로 모인것. 한개면 유니그램, 두개면 바이그램등으로 지칭. 문맥의 파악이 가능. 문장 벡터화의 방법 중 하나인 bag of words 의 단점을 해소 할 수 있음.
- N-gram 언어 모델 : SLM 의 일종. 일부 단어(N-1 개)만 고려. 몇개의 단어만 고려하기에 의도와 다른 단어를 고를 수 있다는 것과 여전히 남아있는 희소 문제, n을 몇으로 선택 할지(trade-off) 등의 문제를 가짐. 훈련에 사용되는 코퍼스에 따라 성능이 비약적으로 달라진다는 SLM 의 특징또한 가지고 있음.

- 한국어 언어모델 : 한국어는 다음 단어의 예측이 훨싼 까다로운데, 어순이 크게 중요하지 않고, 교착어(형태소가 붙어 단어 형성)이며, 띄어쓰기가 잘 지켜지지 않는 한국어의 특성 때문이다.

##### 유사척도 (평가)
- 펄플렉서티(Perplexity, PPL) : 언어 모델 평가를 위한 내부 평가 지표. 낮을수록 성능이 좋음. 테스트 데이터에 의존. 분기계수(선택할 수 있는 가능한 경우의 수) 임.

- 벡터 유사도 : 각 문서의 단어들을 어떤 방법으로 수치화해 표현 했는지, 문서간 단어 차이를 어떤 방법으로 계산했는지에 따라 달라짐. 이런 유사도르 이용해 영화추천(본 영화와 비슷한 영화를 추천)등을 구현할 수 있음.
- 코사인 유사도 : 두 벡터간 코사인 각도를 이용해 구할 수 있는 두 벡터의 우사도. 문자열을 축으로 해 축과 문장간의 각도로 유사도를 측정. 둘의 곱/둘의 거리로 구할 수 있음. 
- 자카드 유사도(자카드 계수, 타니모토 계수) : 두 세트의 곱집합 / 합집합. 동일하면 1, 완전 다르면 0. 두 문서의 총 단어 중 두 문서 전부 등장한 단어 비율.
- 유클리디안(유클리드) 거리 : 단어간의 거리가 짧으면 더욱 유사. 벡터 공간에서 유사도 측정. 자카드나 코사인보다는 효과가 떨어짐.

- 이진 거리 : 문자열 유사도 메트릭. 두 라벨이 동일하면 0, 다르면 1을 반환.
- 매시 거리 : 부분 일치에 기초. 1 - (곱집합 길이/합집합 길이)*(두 세트의 길이차에 따른 점수(1, 0.67, 0.33, 0))

- 지프의 법칙 : 토큰이 언어로 배포되는 방법을 설명. 토큰 빈도가 정렬된 목록의 순위와 정비례하게 함. 
- 편집 거리 : 두 문자열을 동일하게 하려면 삽입,대체,삭제해야 하는 문자의 수를 계산. 
- 스미스 워터맨 거리 : 편집거리와 유사. 관련된 단백질서열 및 광학정렬을 검출하기 위해 개발됨. 
  
- 문서의 유사도 구하기 : bag of ward 에 코사인 유사도를 적용하거나(bow 각 요소들의 곱의 합/bow 합의 제곱근)  TF-IDF 를 적용하는 방법으로도 구할 수 있음. 구현이 쉽고 불용어를 잘 거른다는 장점이 있지만, 단어단위로 보기에 동음이의어를 잡지 못하고 토픽은 알 수 없다는 단점이 있음.
- WMD(word mover's distance) : 문서의 유사도를 구하는 방법. word2vec 근간으로 하여 단어들간의 유클리디안 거리를 사용. w2v 로 임베딩된 단어에 대해 문서 단어들의 유사도를 판단한다. 거리가 다양하다면 단어를 여러 벡터에 대입하여 판단. 이것을 보완한 RWMD(속도 빠름, 결과 비슷)도 존재한다. 

- 혼동행렬 : 머신러닝에서 정확도 측정시 자세한 내용을 알기 위해 사용. TP/FN/FP/TN 으로 이뤄진 행렬. 정밀도, 재현율 등이 등장.

#### 머신러닝

##### 텐서
- 텐서의 종류 : 기본적으로 행렬. scalar - 요소 하나만 가지고 있는 0차원(랭크 0)의 텐서. Vector - 1차원(벡터도 요소 수에 따라 차원이 있음, 이건 텐서의 차원), matrix - 2차원, 그 이후부터는 N-tensor 이다.
- 이미지는 (이미지 수, 행, 렬, rgb)의 4차원 텐서로(흑백은 3차원), 영상은(프레임,이미지,행,렬,rgb)의 5차원 텐서로 이뤄진다. 

##### RNN
- 순환 신경망. 이전의 결과가 다음의 결과에서도 영향을 끼침. 자연어 처리에서 단어의 품사 추축등 여러 분야에서 이용됨.
- 그라디엔트 베네싱 문제 : 미분값이 0에 가까워져 가중치가 한없이 0에 가까워지는 문제.
- 그라디엔트 익스플로싱 문제 : 미분값이 커 가중치가 커지는 바람에 왔다갔다하고 한곳으로 모여들지를 못하는 문제.
- LSTM : 순환신경망의 응용. 긴 문장의 경우 간단한 바닐라 RNN 으로는 해결이 어려워 사용한다. gradient descent(기울기의 좋은 정도, W - (러닝레이트*가중치의 미분값의 합) 으로 이용해 구할 수 있음)를 구하고 조정하기 위해 사용한다. 
- LSTM 읽는 방법 : 본래 기록되어있던 정보에 새로 들어온 값이 히든 스테이트와 시그모이드를 거쳐 본래 알고있던 값을 얼마만큼 가져갈지 기록한다.  
- LSTM 기억하는 방법 : 새로운 정보가 히든과 sigmoid, tanh 를 거쳐 둘이 곱해진 뒤 본래의 기억에 더해진다.  
- LSTM 출력(예측)하는 방법 : 메모리 셀의 정보가 tanh 를, 히든 스테이트와 현재의 정보가 시그모이드를 거쳐 서로 곱해져 출력과 다음 히든 스테이트가 된다.

###### 트렌스포머
- 인코더/디코더를 발전시킨 모델로 RNN을 사용하지 않음. 병렬화를 통해 일을 한번에 처리함. 오직 어텐션만을 사용. 
- 인코더 : 단순한 행렬 곱을 통해 RNN 의 역할을 해냄. 한번의 연산으로 모든 중요정보를 각 단어에 인코딩 시킴. 포지셔널 인코딩(인코더,디코더마다 상대적인 위치정보 입력, -1~1)을 통헤 위치를 파악한다. 
- 셀프 어텐션 : 인코더에서 이뤄지는 어텐션. 벡터형태의 query, key, value 를 이용. softmax(쿼리*키(어텐션점수)/차원수의 제곱근)로 나온 스코어를 확률로 변화시킬 수 있다.
- 멀티 헤드 어텐션 : 병렬처리된 어텐션 레이어. 8개의 어텐션 레이어를 병렬처리해 사용한다. 연관된 정보를 다른 관점에서 분석해 모호한 부분을 해결할 수 있다.  
- 워드 임베딩에 포지셔널 인코딩을 더해주는데, 이것의 손상을 막기 위해 레지듀얼 커넥션을 사용하고, 능력향상을 위해 레이어 노말리제이션을 하며, 이런 인코더 레이어를 6개 연속해서 사용.
- 디코더 : 최초 단어부터 순차적으로 출력하는 건 똑같으나 현재까지 출력과 이번 출력의 어텐션을 사용함.
- 첫 MHA 레이어는 masked Multi head attention 이며, 현재 디코더의 입력을 쿼리로, 인코더 출력 정보를 키와 벨류로 사용한다.
- 벡터를 실제 단어로 출력하기 위해 끝에 리니어와 softmax 가 존재한다. 레이블 스무딩을 이용해 마지막까지 성능 향상을 한다.

##### GPT
- Generative Pre Training of a language model 의 약어.
- language model : 현재 알고있는 단어를 기반으로 다음 단어를 예측하는데 많이 사용되는 모델. 레이블링이 불필요하다는 장점이 있음. GPT-1의 핵심. 비지도, GPT-1은 다 분야의 엄청난 양의 데이터로 pre trained 됨.  
- generative model : 머신러닝 모델의 학습 방법 중 하나. 데이터가 많을수록 학습이 잘 됨.
- BPE(바이트 페어 인코딩) : 자주 함께 사용되는 char 를 하나의 묶음으로 사용(최소한의 단어). 워드 임베딩과 캐릭터 임베딩의 장점을 모두 가지고 있음. (word)단어간의 유사도와 (char)처음보는 문자의 예측 모두가 가능함. 
- GPT-1 : 문장간 관계 유추, 질의 응답, 문장유사도, 분류등에 뛰어난 성능을 보임. 언어 모델로 학습, 파인 튜닝(linear 와 softmax, 추가적인 레이어 없이 적은 양의 레이블 만으로도 가능) 두 단계를 거침. 트랜스포머의 디코더기반 모델. 
- GPT-2 : 1과 달리 파인튜닝을 없애고, 사이즈가 커짐. 입력된 값과 수행해야할 task(다음단어, 번역, 응답 등)를 함께 입력받아 다음 단어를 출력. 
- GPT-3 : 파인튜닝 제거(가능하긴 하나 필요 X)를 핵심으로 하고 있음. 제로샷이나 원샷이 아닌 퓨샷러닝(몇장의 이미지만 사용)만을 이용. 여러 분야에서 뛰어난 성능을 보이나 단방향으로 학습해 문맥 파악에 약하다는 단점을 가짐.     





******


# NLTK
- nltk.download() : NLTK 세트 다운로드. 특정 세트의 이름을 넣으면 그것만 다운로드한다.

### tokenize
- nltk.tokenize.sent_tokenize(text) : 문장 토큰화함수. 문서를 문장단위로 나눠준다. PunktSentenceTokenizer 인스턴스(문장의 시작과 끝을 표시하는 문자나 문장 기호에 기초해 다른 유럽언어로 토큰화를 수행)를 사용한다. 
- 영어가 아닌 언어를 토큰화 하려면 'tokenizer/punkt/언어.pickle' 파일을 로드하고 사용하면 된다.  로드한 언어.pickle 에  .tokenize(text) 매서드를 사용해서도 토큰화를 사용할 수 있다.
  
- nltk.tokenize.word_tokenize(sentence) : 단어 토큰화 함수. 문장을 단어 단위로 나눠주며 축약형의 경우 단어의 의미가 유지되게(do , n't)분리한다. TreebankWordTokenizer 를 사용한다.
- nltk.tokenize.sent_Tokenizer(text) : 문장 토큰화 함수. 문장 내부에 구두점이 포함되어 있어도 잘 작동한다.
- nltk.tokenize.WordPunctTokenizer() : 또 다른 단어 토크나이저. 구두점을 별도로 분리한다. 축약형의 경우 '전, ', '후 총 세가지로 나눈다.
- nltk.tokenize.TreebankWordTokenizer() : 트리뱅크워드 토큰화 함수 로드. .tokenize(Sentence) 로 토큰화를 수행할 수 있다. 분리된 축약형('Do', 'n't')으로 작동된다. 
- nltk.tokenize.WhitespaceTokenizer() : 화이트 스페이스(탭 제외?) 으로 단어 토큰화. 토크나이저.span_tokenize(sent)로 토큰의 오프셋인 튜플의 순서를 받을 수 있다.
- nltk.tokenize.util.string_span_tokenize(문자열, "separator(구분자)") : 구분자대로 분할해 전송된 토큰의 오프셋을 반환.

- nltk.tokenizer.RegexpTokenizer(정규 표현식) : 정규 표현식을 이용한 단어 토큰화 클래스 로드. .tokenize(String)으로 사용할 수 있다. gaps = True 로 화이트 스페이스를 사용한 토큰화를 할 수 있다. 
- nltk.tokenizer.regexp_tokenize(sentence, patten='정규 표현식') : 정규 표현식 단어 토큰화 함수 로드. 

- nltk.tag.pos_tag(토큰 리스트) : 품사 태깅을 수행. (단어, 품사)의 형태로 나타남. PRP(인칭대명사),VBP(동사),RB(부사),VBG(현재부사),IN(전치사),NNP(고유명사),NNS(복수명사),CC(접속사),DT(관사)를 의미함.

- nltk.stem.WordNetLemmatizer() : 표제어 추출기 생성. .lemmatize(word)로 표제어 추출 사용 가능. 정확한 추출을 위해선 (word, 품사)식으로 넣어주어야 한다.
- nltk.stem.PorterStemmer() : PorterStemmer 알고리즘의 어간 추출기 생성. .stem(토큰.norm_.lower()) 으로 어간(용언에서 뜻을 나타내는 불변인 부분)을 찾음.
- nltk.stem.LancasterStemmer() : Lancaster 어간 추출기 로드. .stem(word) 로 사용할 수 있다.

### Freq
- nltk.FreqDist(단어집합) : 단어를 키로, 빈도수를 값으로 저장해 리턴한다. 아래와 똑같은 효과를 지닌다.
- collections.Counter(단어집합) : 단어 집합에서 중복을 제거하고 단어의 모든 빈도를 쉽게 계산함.  이 메서드로 간단하게 등장단어와 빈도 파악이 가능.


### corpus
- nltk.corpus.stopwords() : 불용어 처리 클래스 로드. .words('언어')로 불용어 리스트를 받아올 수 있으며 if w not in words 롤 거를 수 있고, .fileids()로 불용어 목록이 있는 언어를 확인할 수 있다. 한국어 등은 직접 txt 파일등에 목록을 만들어 제거하는게 편함.
- nltk.corpus.wordnet() : 유의어들의 목록 로드. 유사한 단어와 각 단어의 유사도를 파악 가능. synsets(word).definition()으로 단어의 유사어 확인 가능, .path_similarity(synsets)로 단어의 유사도 확인 가능. 
- nltk.corpus.alpino() : 알피노 코퍼스(네덜란드 신문에 나오는 단어 모음) 로드. .word()로 내부의 단어들을 꺼낼 수 있다. 다른 것들도 사용가능.

### metrics
- nltk.metrics.accuracy(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정확도(같은 정도)반환.
- nltk.metrics.precision(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정밀도(TP/(TP+FP))반환.
- nltk.metrics.recall(sentence1, sentence2) : 두 토큰화된 단어 리스트의 재현율(TP/(TP+FN))반환.
- nltk.metrics.f_measure(sentence1, sentence2) : 두 토큰화된 단어 리스트의 f1점수(정밀도와 재현율의 조화 평균(역수의 평균의 역수,곱/합))반환.

- nltk.metrics.edit_distance(word1, word2) : 두 단어간 편집거리 반환.
- nltk.metrics.jaccard_distance(set1, set2) : 두 세트간 자카드 계수 반환.
- nltk.metrics.binary_distance(set1, set2) : 두 세트간 이진거리 계수 반환.
- nltk.metrics.masi_distance(set1, set2) : 두 세트간 매시거리 계수 반환.

### ngrams
- nltk.util.ngrams(단어 리스트, n) : n개의 토큰이 연결되어 있는 n그램 생성. 
- nltk.collocations.BigramCollocationFinder : 바이그램 탐색기. .from_words(tokens)로 토큰을 저장할 수 있다.
- 토큰저장탐색기.nbest(nltk.metrics.BigramAssocMeasures.likelihood_ratio, n) : n개의 바이그램을 찾아 리스트를 받아볼 수 있다.
- 토큰저장탐색기.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq) : 바이그램을 찾는 또다른 방법.
- nltk.probability.LidstoneProbDist(fd, gamma=f, bins = n) = 최대 우도 추정 사용. fd(빈도분포)를 기반으로 f(0~1)를 사용해 n개의 샘플을 생성. 샘플들의 총 합은 1. 

# re
- 정규 표현식 사용을 지원.
- 정규표현식
> - [] : []안의 문자들과 매치. [abc]면 a,b,c중 하나와 매치를 뜻함. [a-c],[a-zA-Z]식으로 범위를 지정할 수 도 있고, 패턴 앞에 ^ 를 붙혀 패턴 부정을 나타낼 수 도 있다.
> - 특수 문자 : .(임의 문자 1개), ?(앞 문자 0 또는 1개), *(앞 문자 0개 이상), +(앞 문자 1개 이상), ^(뒤 문자로 문자열 시작), $(앞 문자로 문자열 종료)
> - 특수기호 : \d(숫자), \D(숫자가 아닌것), \s(whitespace, \t\n\r\f\v), \w(문자+숫자), \W(문자+숫자의 부정)등이 있다.
> - 범위 : 문자{n,m}은 n번 부터 m이하 반복, {n}은 반드시 n번 반복으로 사용된다.

- re.compile() : 정규 표현식 컴파일. 결과 객체 반환. re.S(.이 \n을 포함하게 함) 등을 매개변수로 줄 수도 있다.
- re.sub(패턴, 바꿀문자, 바뀔 문장) : 바뀔 문장에서 패턴에 일치하는 부분을 바꿀 문자로 바꾼다.
- re.match(패턴, 문자열) : 컴파일을 거치지 않고 매치 사용. .group()으로 매치된 문자열을 볼 수 있다 
- re.search(re, sent) : match 와 동일하지만 문장의 경우 문장 전체를 검색한다. .start()로 시작 위치, .end()로 끝 위치, .span()으로 (시작, 끝)을 받을 수 있다.
- re.split(패턴, 문자열) : 정규 표현식을 기준으로 문자열을 구분해 리스트로 리턴.
- 컴파일.match(word) : 단어가 표현식에 맞는지 반환. 없으면 None, 있으면 매치 객체를 반환. 문장의 경우 첫 단어만 판단.
- 컴파일.search(sent) : match 와 동일하지만 문장의 경우 문장 전체를 검색. .start()로 시작 위치, .end()로 끝 위치, .span()으로 (시작, 끝)을 받을 수 있다.
- 컴파일.findall(sent) : 정규 표현식에 맞는 단어만 리스트로 반환. 
- 컴파일.finditer(sent) : 정규 표현식에 맞는 단어만 각각을 매치 객체의 형태로 하여 반복 가능한 개채의 형태로 돌려준다. 
- 컴파일의 메서드는 전부 re.메서드(표현식, 문자열) 형태로 사용할 수 있다.

# replacers
- replacers.RegexpReplacer() : 텍스트 대체 클래스 로드. .replace(text)로 사용, 축약을 해제하고 단어 토큰화까지 진행해 리스트로 반환.
- replacers.RepeatReplacer() : 반복 문자 삭제 클래스 로드. 위와 동일하게 사용할 수 있으며, 반복된 단어를 일반 단어로 바꿔 반환한다. nltk 의 wordnet.synsets(word)에 이미 있다면 처리하지 않도록 하면 일반 단어는 반복을 삭제하지 않는다.
- replacers.WordReplacer({'바꿀단어':'바뀔단어'}) : 단어를 동의어로 변환하는 클래스 로드. 마찬가지로 사용, 목록에 있는 단어는 바꿔서, 아니면 그대로 반환한다.

# tensorflow
- tf.keras.preprocessing.text.text_to_word_sequence(sentence) : 모든 알파벳을 소문자로 변환, 구두점 제거, 죽약형은 분리하지 않는 단어 토큰화 함수.  정제와 단어 토큰화를 동시에 적용.
- tf.keras.utils.to_categorical(벡터) : 원 핫 인코딩을 해줌. (요소 개수, 요소 종류)의 형태를 가짐.
- tf.keras.preprocessing.sequence.pad_sequences(인코딩된 단어 집합) : 가장 긴 문장의 길이에 맞게 문장의 앞에 0을 삽이비해 ndarray 로 반환. padding='post' 로 문장 뒤에 0을 삽입할 수 있고, maxlen 매개변수로 길이를 지정할 수 있다.
- tf.keras.preprocessing.text.Tokenizer() : 정수 인코딩을 위한 토크나이저 로드. .fit_on_texts(단어집합) 으로 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여할 수 있다. 단어의 개수는 .word_counts 로 확인 할 수 있다.
- tokenizer.texts_to_sequences(단어집합) : 각 단어를 이미 정해진 인덱스로 변환. 만약 토크나이저 로드시 인수로 i+1을 넣었다면 i 까지의 인덱스를 가진 단어만을 사용하고 나머지는 버린다.

# sklearn
- sklearn.feature_extraction.text.CountVectorizer() : BOW 표현을 하게 해주는 변환기 로드. .fit(문자열이 담긴 리스트)로 사용, .vocabulary_ 속성에서 반환된 {단어:등장횟수} 형태의 딕셔너리를 볼 수 있음.  tf-idf 와 함께 ngram_range=(연속 토큰 최소길이, 최대길이) 로 연속된 토큰을 고려할 수 있다. 보통은 하나만 하지만 많을 때 바이그램정도로 추가하면 도움이 된다.  
- Bow 표현을 만드려면 .transform(list), Scipy 희소 행렬로 저장되어 있으며, .get_feature_names()로 각 특성에 해당하는 단어들을 볼 수 있음. min_df 매개변수로 토큰이 나타날 최소 문서 개수를 지정할 수 있고, max_df 매개변수로 자주 나타나는 단어를 제거할 수 있다. stop_words 매개변수에 "english" 를 넣으면 내장된 불용어를 사용한다.
  
- sklearn.feature_extraction.text.TfidVectorizer(min_df=i) : 텍스트 데이터를 입력받아 BOW 특성 추출과 tf-idf 를 실행하고 L2정규화(스케일 조정)까지 적용하는 모델로드. 훈련데이터의 통걔적 속성을 사용하므로 파이프 라인을 이용한 그리드 서치를 해 주어야 한다. .idf_ 에서 훈련세트의 idf 값을 볼 수 있다. idf 값이 낮으면 자주 나타나 덜 중요하다 생각되는 것이다.

- sklearn.decomposition.LatentDirichletAllocation(n_components=n, learn_method="online", random_state=k, max_iter=i) : LDA 수행. .fit_transform(X), .components_ 등을 사용할 수 있다.

# KoNLpy
- 한글 분석을 가능하게 함. 자바로 이뤄져 있어 JDK 1.7 이상과 JPype 가 설치되어 있어야 함. 각 분석기는 성능과 결과가 다르게 나와 용도에 따라 적절한 것을 선택해야 함.
- Okt(Open Korea Text, Twitter), 꼬꼬마(Kkma), 메캅(Mecab, 속도 중시), 코모란(Komoran), 한나눔(Hannanum) 등의 형태소 분석기 사용 가능.
- kss.split_sentences(text) :  kss 모듈의 한국어 문장 토큰화 함수. 
- konlpy.tag.Okt() >  Okt 클래스 객체 생성. .morphs(text)로 형태소 분석(토큰화)을, .pos(text)로 형태소 토큰화 후 품사 태깅을, .nouns(text)로 명사만 받아볼 수 있다.
- konlpy.tag.Kkma() > 꼬꼬마 토크나이저 로드. Okt 보다 조금 더 세분화(한 > 하,ㄴ)해 분리한다.

# 한국어 전처리 패키지
- PyKoSpacing : 한국어 띄어쓰기 패키지. 띄어쓰기가 없는 문장을 띄어쓰기를 한 문장으로 변환해줌. pykospacing.spacing(문장)으로 사용할 수 있음. 
- Py-Hanspell : 네이버 맞춤법 검사기를 바탕으로 제작된 맞춤법(띄어쓰기 포함)보정 패키지. hanspell.spell_checker.check(문장).checked 로 개선된 문장을 볼 수 있음.

- SOYNLP : 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저. 비지도 학습으로 토큰화. 학습에 필요한 문서를 다운로드할 필요가 있다.
- soynlp.DoublespaceLineCorpus("txt 파일.txt") : 데이터를 다수의 문서로 분리함.
- soynlp.word.WordExtractor() : 형태소를 학습할 단어 토크나이저 로드. .train(corpus)로 훈련을, .extract()로 단어 점수표를 확인할 수 있고, 이를 이용해 soynlp.tokenizer 의 LTokenizer, MaxScoreTokenizer 등을 사용한다.   
- soynlp.normalizer.emoticon_normalize(sent, num_repeats=i) : ㅋㅋ,ㅎㅎ 등의 이모티콘을 i개 까지만 반복되도록 변환.
- soynlp.normalizer.repeat_normalize(sent, num_repeats=i) : 의미없이 반복되는 글자를 i개 까지만 반복되도록 변환.

