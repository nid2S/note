# ChatBot
- 챗봇 : 음성이나 문자를 통한 인간과의 대화를 통해 특정 작업을 수행하도록 제작된 컴퓨터 프로그램. 이를 구현하기 위한 다양한 API가 존재함. Dialogue task. 
- 동작방식 : 자연어처리를 심각하게 적용하는 언어 이해 방식, 입력에서 특정 단어/어구를 검출해 준비된 응담을 출력하는 검색방식, 각본을 미리 만든 뒤 그에 따르는 각본방식이 있음.
- 페르소나 : (다른 사람들 눈에 비치는, 특히 그의 실제 성격과는 다른, 한 개인의) 모습. 챗봇의 성격. 성격을 뜻하는 personality를 써도 되나 persona의 쪽이 더 자주 사용되고 있음.
  `안나는 매사 행복하고 기운이 넘친다. 공감능력이 뛰어나고 긍정적이며 리액션이 좋은 편이다` 등의, 성격과 말투/텐션 등 어떤 식으로 대화하는지 알려줄 수 있는 페르소나를 설정해야 한다.

## 대화 시스템
- 대화시스템(Dialogue Systems) : 사람과 기계가 대화를 나눌 수 있게 하는 AI시스템. 인공지능 어플리케이션에서 필수적인 기능. 목적지향 대화시스템과 대화지향 대화시스템으로 분류됨.
- 대화처리기술 : 사용자의 말을, 명령어 중에서 정확히 이해하는 수준에서 이해하고 공감하며 전문분야수준의 지식으로 사용자 요구를 만족시키는 기술수준까지 포함. 기계가 인간의 대화 능력과 지식을 가져, 
  인간의 역할이 가능하도록 하는 기술을 이로 보기도 함. 세분화된 특정 기술 및 능력에 집중되어 발전해 왔으나, 여러 시스템들이 결합해 인간대화를 처리하도록 하고 있음. 

### 목적지향 대화시스템
- 목적지향 대화시스템(goal-oriented dialogue system) : 특정 목적을 위해 구축된, 사용자가 자연어 형태의 음성이나 텍스트 입력을 통해 원하는 작업을 수행할 수 있도록 도와주는 대화시스템. 
  사용자가 시스템과의 대화를 통해 달성하고자 하는 목표가 있고, 시스템과 연결되어있는 대용량 DB가 있어, 거기서 원하는 정보를 찾아 전달해 주는 것을 목적으로 함. 문제해결용 대화시스템과 사실상 동일.

- 문제해결용 대화시스템(Task-Oriented Dialog Systems, 기능지향 대화시스템) : 문제의 해결을 위해 설계된 시스템. 보통 NLU, DM, NLG 3개의 모듈 아키텍쳐로 구성됨. 파이프라인 목적지향.
  최대한 적은 대화로 유저가 원하는 것을 이해하고 해결하는데 초점. 문장의 의도를 알아내는 Intent Classification과 구체적인 요청을 알아내는 Slot Filling으로 구성.  
  고객서비스/추천/질의응답 등(Assistive)과 Co-operative(에이전트 둘이 함께 대화로 task해결), Adversarial(에이전트 둘이 대화를 통해 task에서 경쟁)등이 있음.
- NLU(자연어 이해) : 사용자의 발화를 이해하기 위한 모듈. intent(Search-Weather)와 Slot(location(Seoul), date(2020-05-17)등)을 분석함.
- Dialogue Manager(DM, 대화관리) : 주고받는 대화흐름을 관리. 크게 DialogStateTracker(대화추적관리/대화진행중 데이터유지)와 PolicyManager(시스템 액션결정. NLG/DB로 연결)로 구성.
- NLG(자연어 생성) : DM으로부터 데이터를 입력받아 사람이 이해할 수 있는 시스템의 응답을 생성. 각 시스템의 페르소나를 결정할 수 있기에 매우 주의깊게 다뤄져야 함.

- DM분류 : 유한상태 기계기반 방법 - 시스템 주도로 대화를 진행, 정의한 순서대로 발화하며 사용자의 응답이 적합해야 다음으로 넘어감. 쉬운구현/안정적성능/단순목적적합 장점과 일방적/제한된 대화 단점. |
  양식기반 방법 - 양식채우기 방식이라고도 함, 필수 정보를 슬롯으로 정의하고 여러 슬롯의 양식이 채워질 때 까지 대화로 정보를 요구함. 대화순서에서 자유도를 누림. 대화이해정확도 필요. | 두가지로 나뉨. 

- 대화지향 For 목적지향 : 챗봇을 특정 목적을 위해 타 프로그램과 같이 사용하는 경우, 챗봇또한 목적지향의 범주에 들어갈 수 있음(영어 교육을 위한 챗봇+문법교정 시스템 등).

### 대화지향 대화시스템
- 대화지향 대화시스템(Chat-oriented Dialog Systme) : 대화를 위해 제작된 시스템. 유저가 어떠한 주제로 말을 걸어도 알맞은 답변을 해 대화를 이어나감. 자유주제(OpenDomain)대화시스템이라고도 함.
  대화 에이전트라고도 함. 대화시스템에서는 이 Task를 챗봇이라고 칭함. 대화task의 SocialDialogue(흥미를 유발해 긴 대화를 목적으로 하는 ChitChat, 그 외에 테라피/멘탈웰빙등)에 속함.
- 방법 종류 : 규칙/패턴기반 방법(패턴지식+제약규칙+유의어/관련어 규칙사용. 거의 자연어분석/사용자의도 이해 과정이 불필요), 검색기반 방법, 생성기반 방법(통계기반 자동번역/신경망기반 자동번역)이 있음. 
  대용량 데이터셋의 사용이 가능해짐에 따라 최근에는 규칙/패턴기반 방법과 검색기반 방법을 함께 사용하고 있음. 복잡한 구조의 표현을 위해 스크립트 언어를 사용하는 챗봇이 많음.
- 학습 : 과거엔 학습을 위해 방대한 데이터에 직접 annotation해 hand-crafted feature를 선별했지만, 도메인 영역이 작을 수 있고 모델 개발까지 오랜 시간을 걸릴 수 있다는 단점이 있었음.
  이의 해결을 위해 End-to-end 방식(hand-crafted feature에 의존적이지 않으며, 긴 대화 유지에 유용함)을 사용함. 데이터는 대부분 대용량의 대화예문(발화-응답, 텍스트/패턴형식 가능)을 사용.

- 생성 대화 모델(Generative Conversational model) : seq2seq모델을 이용해 유저의 말을 읽고 답변을 한단어씩 생성하는 모델. 많은 연산량이 필요하고, 고품질을 위해 큰 모델과 데이터셋이 필요함.
- 한계 : 표준적 S2S+어텐션 방법의 고질적 문제는 일반적/지루한 응답, 주제/문맥과 무관한 응답, 같은 단어/문장의 반복, 문맥의 결핍(페르소나의 일관성 부족), 긴대화/질문/추가학습 불가등이 있었고, 
  이의 해결법은 목적함수를 MMI(Maximum Mutual Information, argmax{로그(가능도)-(혼자서 출현할 확률)})로 쓰고, 샘플링기반 디코딩 알고리즘(빔 서치등, 희소단어의 가중치를 높여줌)을 쓰고,
  빔 서치등의 경우 반복이 발생하면 이후의 동일단어를 블록/coverage 매커니즘/반복에 패널티를 주는 새로운 목적함수 등울 쓰고, persona를 임베딩해 학습시키거나 페르소나정보까지 레이블링된 데이터를 씀.

 
- 검색 기반 모델(Retrieval-based model) : DB에서 답변을 고르는 모델방식. 생성 대화 모델의 보완을 위해 탄생. DB에서 그럴듯한 N개의 답변 후보를 뽑은 뒤 한개의 답변을 선택함. 이루다의 방식.
  NLU(텍스트로 구성된 메세지를 벡터로 변환, BERT구조)모듈과 답변후보들이 저장된 DB, Re-ranker(최종답변선택을 위한 모델)로 구성됨. DB의 답변들은 실제사람이 쓴것이기에 자연/개인정보 유출 위험.
  사용자 발화는 질의로, 대용량 대화예문의 발화는 가상의 문서로 생각해 유사한 발화가 검색되면 그 발화의 응답을 시스템 응답으로 출력함. 혹시 유사예문이 없다면 임의발화를 쓰거나, 생성모델적용이 가능.

- 모델구조 : 검색, 생성, 검색-정제(retrieve-and-refine)의 모델로 구성, 모든 모델은 트랜스포어 베이스.
- 검색 : 대화기록(컨텍스트)이 입력되면 검색시스템은 다음 대화를 거대한 답변후보(모든 가능한 트레인 셋 답변)에서 점수를 매겨 선정하고, 가장 높은 점수의 하나를 반환함. 논문에선 폴리-인코더 구조를 사용.
  폴리인코더는 각각 가능한 후보 답변에 참조되고 있는 다중 표현을 사용하는 컨텍스트(글로벌 속성)를 인코드 해 싱글 글로벌 벨터표현에 향상된 효과를 보여주는 인코더(어텐션)들(bi-encoders)과,
  계산하기 쉬우나 간단히 입력과 출력을 연결한 것으로 비교되는 트랜스포머들(cross-encoders)로 이뤄지 있고, 다른 검색기반 모델과 비교했을때 최고이며, 우승한 생성모델과 유사한 효과를 보여줌. 두개 사용.
- 생성 : ParlAI version에 기반한 스탠다드 S2S 트랜스포머 아키텍처와 Byte-Level BPE 토큰화를 사용, 세가지 사이즈의 모델을 제시하였음(파라미터 개수 - 90M, 2.7B, 9.4B).
  각각 9.4B 파라미터 - 4(인코더)/32(디코더)/4096(임베딩차원)/32(어텐션 헤드), 2.7B - 2(인코더)/24(디코더)/2560(임베딩차원)/32(어텐션 헤드)를 가지고 있음.
- 검색-정제 : 생성모델에 단조롭고 반복적인 응답만 생성하고, 응답을 환각시키며, 일반적으로 외부 추가지식을 일고 엑세스 할 수 없으며, 임베딩이 완벽하지 않을 수 있단 이슈가 있었는데,
  이를 완화하려는 노력 중 하나가 생성 전에 검색 단계를 나누는 것 이였음. dialogue retrieval과 knowledge retrieval 두 종류의 검색 단계를 사용함.
- Dialogue Retrieval : 대화 기록이 주어지면 검색모델이 먼저 응답을 생성함. 이 응답을 바로 보여주기보다 입력시퀀스에 구분 토큰과 함께 더해짐. 사람의 작성 답변 중 가장 높은 가능도를 가진 답변을 선택함.
- Knowledge Retrieval : 검색된 지식으로 조건부생성을 함. 최초 지식 후보를 생성하기 위해 같은 검색시스템을 사용함. 트랜스포머 검색모델은 후보에 랭킹을 매기고 조건부 생성에 사용될 단일문장을 선택함. 
  추가로 지식이 필요하지 않은 컨텍스트를 위해 검색수행여부를 정하는 트랜스포머기반 분류기를 훈련함. (파인튜닝 task안에서) 지식이 필요한지 아닌지를 판별하는 이진분류로 훈련됨.

- 객체 훈련 : 검색을 위한 랭킹을 매김 -> 생성을 위한 가능도 훈련을 함 -> 검색과 다듬기(Refine)를 위한 α-blending -> 생성을 위한 불가능도(Unlikelihood)훈련 순으로 이뤄짐.
- 검색을 위한 랭킹 : 검색모델 훈련을 위해 cross-entropy는 응답의 점수로 구성된 logits로 최소화, 학습중 배치의 다른 응답을 부정으로 사용(샘플링)함(네거티브샘플링, 빠른학습/임베딩재사용/큰배치 가능).
- 생성을 위한 가능성 훈련 : 생성모델 훈련을 위해 스탠다드 MLE(Maximum Likelihood Estimation, 데이터셋이 log(pθ(y의 t번째 토큰 | x, y의 t이전토큰))의 합*(-1)로 최소화됨)사용. 
- 검색과 다듬기를 위한 α-blending : 검색과 리파인을 위해 응답을 생성모델의 컨텍스트에 덧붙여 MLE로 훈련하는건 모델이 종종 검색된 말을 무시하는걸 선택해 만족스러운 결과를 제공하지 않아,  
  이 값이 사용되는지 확인하기 위해서 검색된 응답을 골드응답 α(하이퍼파라미터)%로 대체할 수 있었음. 검색과 생성only시스템간 부드러운 변환을 줌. 
- 생성을 위한 불가능성 훈련 : 모델 생성에서 실패를 방지하는 대안방법으로 손실함수를 바꿈. unlikelihood loss(사람과 모델간 불일치를 고치는데(반복/과다표현된 어휘토큰 줄임 등)도움)를 사용.
  각 스텝에서 토큰세트에 페널티를 줌. likelihood는 전체 시퀀스 확률 분포를 모델링 하려하나, 알려진 편향을 수정할 가능성은 거의 없음. 각 단계에서 계산된 부정후보로 후보생성메소드를 미리지정(반복된 등).
  likelihood는 골드토큰의 확률을 높이고, unlikelihood는 음수후보의 확룰을 낮춤. 훈련중 모델생성시 나타나는 n그램분포의 실행카운트를 유지하고, 골드응답에서 측정된것보다 높으면 토큰을 부정적후보로 함. 

- 디코딩 : 추론시 입력으로 주어진 대화 컨텍스트에 대한 응답을 생성할 디코딩 함수를 골라야 함. 몇가지 잘 알려진 접근에는 BeanSearch, Sampling, ResponseLength, subsequence blocking이 있음. 
- BeamSearch : greedy search(최대확률)와 함께 널리 사용되는 결정적 디코딩접근중 하나. greedy search는 이것의 특별한 경우라고 볼 수 있음. 빔 사이즈를 바꿔가며 실험해봐야 할 필요성이 있음.
- Sampling : 하나의 대안은 각 단계에서 모델종속분포에서 샘플링 하는것임. 낮은 가능성의 토큰을 샘플링하는걸 막기 위한 전형적 접근법은 각 단계에서 vocab의 하위집합으로 샘플링을 제한하고,
  (재정규화된)가능성들에 따라 샘플링 하는것이 있음. 메서드를 샘플링하기 위해, top-k샘플링과 sample-and-rank(S번 샘플링해 가장 높은 가능성으로 생성된 샘플 선택)를 비교함. 
- ResponseLength : 빔으로 생성시 훈련된 사람말의 평균길이와 일치하지 않는 짧은 생성을 생성하는 경향이 있으나, 높은 품질의 더 긴 응답은 매우 짧은 것보다 더 매력적일 수 있음. 
  인간 분포를 따르는 게 최적성능의 제공은 아니나, 실패 가능성이 적어 평가개선을 위해 간결하게 사용할 수 있음. 인간의 응답시간이 길어지면 더 많은 정보를 제공하고 덜 둔해질 수 있음.
- MinimumLength : 모델응답의 길이를 제어하는 두개의 간단한 방법 중 하나. 최소생성길이에 대한 엄격한 제약. 최소 시퀀스 길이에 도달할 때 까지 엔드토큰이 생성되지 않도록 함.
- PredictiveLength : 인간간의 대화데이터에 기반한 길이를 예측하는 것. 이를 위해 대음대화의 길이를 저장해 4진분류 분류기를 학습시킴(길이 범위별로). 이는 검색모델과 동일한 구조를 가짐. 
  그 후 분류기는 먼저 다음 응답의 길이를 예측하고, 해당하는 예측에 최소생성길이제약을 설정함. 자연스러워 보일때 긴 반응을 보장하며 더 자연스러운 가변 길이로 만듦. 시스템을 더 복잡하게 만든다는 단점존재.
- SubsequenceBlocking : 문장생성모델(특히 빔서치등 확률적 메서드/샘플링 메서드)은 서브시퀀스를 반복한다고 알려짐. 따라서 생성된 말의 n그램 반복/입력된 말의 반복을 모두 블로킹(n그램 표준 빔 블로킹).  

- 훈련 : (?)

#### Breakdown Detection
- breakdown : 시스템이 대화의 흐름에 맞지 않는 발화를 생성했을 때 발생. 크게 멀티 턴으로 인해 시스템이 의도를 잘 못 이해하거나, 대화에서 많은 주제를 다뤄 모든 흐름파악에 어려움을 겪어 발생함.
- Breakdown Detection : 사람과 시스템 사이의 대화에서 문맥의 흐름이 끊겨 사람이 더이상 대화를 이어나갈 수 없을 때(breakdown), 시스템의 발화가 breakdown을 유발하는지 탐색하는 것. 
  자연스러운 대화 유지를 위한 필수 task이며, 시스템이 잘못 발화한 경우에 대한 오류 발견에도 활용가능. End-to-end방식에 대한 연구에도 불구하고, 일일히 문맥흐름을 파악해 감지하기에는 어려움이 있음.
- 초기 연구 : 각 발화에 대한 키워드 추출을 통해 breakdown되기까지 휴리스틱규칙을 이용한 rule-based방법(전통)과 LSTM을 이용해 빈번히 나오는 단어(벡터)를 feature로 구성하는 방법이 있었음(딥러닝). 
  규칙기반(전통적인)방법은 휴리스틱하게 일일히 비교해보며 찾아야 해 시간이 많이 소비되며, 딥러닝기반의 모델도 멀티 턴을 모두 반영하기에는 부족함이 있음.
- Memory attention기반 Temporal Utterance Encoding모델 : 사용자 및 시스템 발화에 대한 문장표현을 수행한 뒤 둘을 연결, LSTM > 지금까지의 메모리를 이용힌 어텐션 가중치를 얻어 예측.

#### 캐릭터 대화
- (챗봇)캐릭터 대화의 조건 : 고유의 대화체 + 그것을 유지, 캐릭터 세계관 유지(기본 프로필, 배경, 철학, 특정 질문에 일관된 답변), 유창하게 말하기.
- 퓨샷프롬프트 : 챗봇의 답변생성에 사용될(모델의 입력이 될)몇가지 대화내용이 들어있는 프롬프트. 특정 주제와 관련된 전체 대화문이 몇개(두개정도) 들어감.  
- 기본적인 대화모델의 구성 : 사람의 발화(쿼리텍스트 입력/음성인식) -> | 퓨샷프롬프트 -> 초대형 언어모델(답변 텍스트 출력) | -> 퓨샷 프롬프트/사람에 전달 의 구조를 가짐.  
- 일관적 캐릭터 대화체 : 유저 발화와 생성되는 답변에 의해 캐릭터 대화체가 변할 수 있음 -> 
  답변의 페르소나를 탐지하는 모델을 별도로 훈련시켜 언어모델을 보조하고, 해당 모델의 피드백을 받아 캐릭터 대화체 변환기가 대화체를 변환하게 함.
- 캐릭터 세계관 유지 : 특정한 질문에 대해 일관성있게 답변해야 함 -> 일관성을 유지해야 하는 문답 예제가 포함된 대화장면 여러개를 검색해 입력 프롬프트에 포함해야 함 ->
  프롬프트 길이의 한계로 모든 세계관을 담기 힘듦 -> 별도로 훈련시킨 주제탐지기를 이용해 유저발화에 따라 주제 안에서 일관성을 유지해야 하는 예제를 검색해 프롬프트에 포함시켜 줄 수 있음.
- 프롬프트 인코딩 : 더 효과적인 퓨샷러닝을 위함. 프롬프트를 지시문, 대화주제, 대화장면 파트로 분리한 뒤 각각의 인코더 모델을 초대형 언어모델 앞에 두어 언어모델어 더 효과적으로 프롬프트 패턴을
  인식할 수 있게 함. 이를 위해 각 인코더 모델은 P-Tuning, Prompt-Tuning과 같은 기법으로 별도로 훈련이 되어야 함.
- PCU(Prompt Control Unit) : 캐릭터 대화체 변환기, 대화 씬 검색 모델, 대화주제 탐지기, 프롬프트 인코더가 합쳐진 캐릭터 대화를 위한 제어 장치.
- ![참고이미지 from NaverCloba](img/char_chatbot.png)













# REFERENCE
- [1](https://ettrends.etri.re.kr/ettrends/178/0905178006/34-4_55-64.pdf)
- [2](https://jiho-ml.com/weekly-nlp-31/)
- [3](https://www.koreascience.or.kr/article/CFKO201832073078524.pdf)
- [4](https://www.koreascience.or.kr/article/JAKO201734963782120.pdf)
- [5](https://arxiv.org/pdf/2004.13637v2.pdf)
- [6](https://velog.io/@changdaeoh/cs224n-Lec15.-Natural-Language-Generation)
- [7](https://www.youtube.com/watch?v=tf46i2hZ_1w&list=PLq8dHmDf5DDX7HSXPh5pAxcDrAl7PJIoN&index=12)