# have to study
> RNN -> Conv1d -> Attention -> transformer -> transferlearning

> -데이터 전처리
>     - bpe
>     - 형태소 분석기(konlpy, mecab)
>     .... 등

> variation of RNN - RNN, GRU, LSTM
> variation of transformer
> encoder - Bert
> decoder - gpt
> Bert + gpt -> Bart(generator)
> mnist-> 영화 감성 분류 (IMDB) -> 캐글 재난 분류
> 
> 음성인식(ASR), 음성이해(NLU), 음성합성(TTS)
> 챗봇 : 의도 분류(Intent Classification)는 개체명 인식(Named Entity Recognition)과 더불어 챗봇의 중요 모듈로서 사용
> RNN, DL chapter review

# 자연어 처리
- [참고자료](https://wikidocs.net/21667)

- 자연어와 컴퓨터간 상호작용에 대함. 인공지능과 컴퓨터 언어학의 주요 분야중 하나.
- 하이퍼 파라미터 : 사용자가 직접 값을 선택해 성능에 영향을 주는 매개 변수. 
- EDA(Exploratory Data Analysis) : 탐색적 데이터 분석. 데이터 내 값의 분포, 변수간 관계, 결측값 존재 유뮤 등을 확인하는 데이터 파악 과정.

- 자연어 처리 언어모델의 발전 과정 : 
  사전 훈련된 워드 임베딩 -> W2V, GloVe등 워드 임베딩 방법 등장, 임베딩 사용 방법 중 임베딩층을 랜덤 초기화 해 처음부터 학습하는 방법보다 이미 학습된 벡터를 가져오는 방법, 
  문맥 고려가 불가하단 문제점이 있었음.
  사전 훈련된 언어 모델 -> 우선 LSTM을 학습하고, 학습한 LSTM을 다른 태스크에 추가 학습. 레이블이 없는 데이터 사용. ELMo(양방향 언어모델 학습, 거기서 임베딩 값을 얻음)등도 존재.
  트랜스포머 -> 언어모델 학습시 LSTM 대신 트랜스포머 사용. 디코더를 총 12층 쌓은 후 방대한 텍스트 데이터를 이용해 GPT-1이 탄생.
  언어모델 구조 변경 -> 마스크드 언어 모델(입력 단어 집합의 15%를 랜덤으로 마스킹, 이 단어들을 예측하게 함)을 이용해 단방향 언어모델을 기존 예측할 단어를 이미 관측해버려 
  잘 쓰이지 않았던 양방향 언어모델로 바꿈. 
- MIT의 렉스 프리드만 교수는 2020년 기준 인간 뇌 수준의 인공지능을 훈련시키려면 26억 달러(2조원)이상의 돈이 들지만 2040년이 되면 8만달러(8000만원)정도면 가능해질것이라는 이야기를 한 적 있음.

- TPU 사용 : TPU초기화(tf.config... | tf.tpu...) > Strategy셋팅(tf.distribute) > 모델정의시 strategy.scope내에서 이뤄져야 함(함수를 만들고 내부에서 호출, 컴파일 하는 방식).

## 전처리
- 순서 : 데이터 생성(로드) > 데이터 확인(길이/개수/null) > 토큰화 > 단어집합생성 > 정수인코딩 > 패딩 > 벡터화

### 데이터 처리
#### 정제 (불용어)
- 정제 : 갖고 있는 코퍼스에서 노이즈 데이터를 제거. 토큰화보다 앞서 이뤄지기도, 이후에도 남아있는 노이즈 제거를 위해 지속적으로 이훠지기도 함.
- 종류 : 등장 빈도가 낮은 단어 제외, 문자가 아닌것을 제외, 소문자화, 불용어 처리, 본문(글자형식)이 아닌것을 제외, 구두점 제거, null 행 제거 등 
- 노이즈 데이터 : 무의미하거나, 목적과 다른 불필요 단어이거나, 특수문자 등을 뜻함.
- 등장 빈도가 적은 단어, 길이가 짧은 단어(영어), 빈 값(data.isnull().values.any()으로 확인가능)등도 제거해야함.
##### 불용어 처리
- 불용어 처리 : 문장의 전체적 의미에 크게 기여하지 않는 불용어를 검색 공간 줄이기 등의 이유로 제거하는 과정.
- 지주 등장하나 도움이 되지 않는 조사, 접미사 등을 제거함. 미리 정의된 불용어 사전이나 직접 정의한 불용어 들을 제거할 수 있음.

#### 정규화
- 정규화 : 표현 방법이 다른 같은 의미의 단어들을 같은 단어로 만드는 작업. Normalize.
- 문장부호(구두점)제거, 전체 대/소문자화, 숫자 단어화, 약어 전개 등의 자연어 텍스트 처리 수행을 위한 과정.
- 문장부호 제거는 반복문이나 기타 클래스/메서드, strip(string.punctuation(구두점모음))으로, 대/소문자화는 str.upper()/lower()로 가능하다.
- 단, 단어 내에 문장 부호가 있거나 단어가 대소문자 구분을 필요로 하기도 하기에 무턱대고 수행해서는 안된다.
##### 텍스트 대체  
- 축약이나 줄임말 등을 본래대로 풀어 처리를 효과적이게 하는 과정.
- 정규 표현식 이용 텍스트 대체:  import re >  [(r'won\'t', 'will not'), (바꿀단어, 바꿜 단어)\]식으로 제작 > re.compile(바꿀 단어) > 
  re.subn(컴파일, 바뀔 단어, text)[0\] 의 과정을 거쳐 할 수 있음.
##### 어간 추출 (표제어 추출)
- 표제어 추출 : 단어들을 표제어(기본 사전형 단어)로 바꿔가는(찾아가는) 과정.
- 어간 : 단어의 의미를 담고 있는 핵심 부분. 
- 형태학적 파싱 : 어간과 접사를 분리하는 작업.
- 한국어 : 한국어는 5언 9품사 구조를 가지고 있으며, 이중 용언(동사, 형용사)이 어간과 어미의 결합으로 이뤄져 있음.
##### 반복 문자 처리
- 무의미하고 오류를 일으키는 반복문자를 일반 문자로 변환.
- 반복문자를 포함하는 단어를 역참조 방식을 사용해 제거.
##### 단어 동의어 대체
- 더 훌륭한 성능과 적은 오류를 위해 같은 의미의 단어를 하나로 변환.

### 태깅
- 시퀀스 레이블링 : 입력시퀀스에 대해 레이블 시퀀스를 각각 부여하는 작업([X1,X2],[Y1,Y2]). 태깅이 대표적.

- 태깅 작업 : 각 단어가 어떤 유형에 속해 있는지를 알아냄. RNN의 다대다 작업이며 양방향 RNN(모든은닉상태 출력)을 사용함. 챗봇, 기계번역 등의 전처리 작업으로써 필요할 때가 많음.
- 개체명 인식 : 각 단어의 유형이 사람, 장소, 단체등의 유형인지 알아냄. 주어진 유형중 단어가 어떤 유형인지 예측하며, 모델헤 따하선 품사 정보를 입력으로 요구하기도 함. 
  도메인이나 목적에 특화시키려면 직접 만들어야 함.
- 품사 태깅 : 각 단어의 품사가 명사, 동사, 형용사등 인지 알아냄.  품사에 따라 뜻이 달라지는 단어를 위해 토큰화 과정에서 각 단어가 어떤 단어로 쓰였는지 구분해 놓는 것.
- 사전에 추가 : 사람 이름 등 고유한 단어로 토큰화 되어야 하는 것들은 형태소 분석기에 사용자 사전을 추가해 분리되지 않게 할 수 있음.

- BIO(IOB, begin inside outside)표현 : 코퍼스로부터 개체명을 인식하기 위한 보편적 방법. 개체명의 시작은 B, 개채명의 내부는 I, 개체명의 외부는 O로 표기. 
  B/I-개체명 식으로 어떤 객체인지도 함께 태깅.

### 모델링
#### 토픽 모델링 (주제 찾기)
- 토픽 모델링 : 문서의 주제를 발견하기 위한 텍스트마이닝 기법.
- 행렬 용어 : 전치행렬 - 원래 행렬에서 행과 열을 바꿈 | 직교행렬 - 원래 행렬\*전치행렬,n\*n 행렬에서 원 행렬*전치행렬 = 단위행렬을 만족해야 함. | 
  단위행렬 - 대각행렬(정사각 아니여도 됨, 아니여도 (i,i) 가 1), 대각선은 1, 나머지는 0인 정사각 행렬. | 역행렬 - A\*어떤 행렬 이 단위 행렬일 때 어떤 행렬. 
- SVD(특이값 분해) : m\*n 차원의 행렬 A 를 UΣV^T 로 분해하는 행렬분해 방법. U(m\*m 직교행렬), V(n\*n 직교행렬), Σ(m*n 직사각 대각 행렬) 로 분해한다.
- Truncated SVD(절단된 SVD) : 대각 행렬 Σ의 원소값 중 상위 t 개만 남음. t 는 클수록 많은 정보를 가져 갈 수 있지만 작을수록 노이즈 제거 가능. 원 행렬 복구 불가.
- LSA(latent Semantic Analysis, 잠재 의미 분석) : 단어의 의미를 고려. DTM이나 TF-IDF와 같은 각 문서에서 각 단어의 빈도수를 카운트한 행렬을 입력으로 받아 차원을 축소해 의미를 추출함. 
  쉽고 빠른 구현과 단어의 잠재 의미를 이끌어 낼 수 있단 장점이 있음.
- LSA 자세히 : 단어를 행, 문장을 열로 나타낸 뒤, SVD 를 사용해 세개의 매트릭스(토픽을 위한 단어, 토픽 강도, 문장)로 나타냄. 뒤의 두 매트릭스를 곱해 단어간 유사도를 파악. 
  새 정보의 업데이트가 어렵고, 단어 의미 유추 작업에서 성능이 떨어진단 문제점을 가짐. 
- LDA(Latent Dirichlet Allocation, 잠재 디레클레 할당) : 토픽 모델링 대표 알고리즘. DTM 이나 TF-IDF 행렬을 입력으로 해 역공학(문사 작성 과정 역추적)을 해 토픽 추출. 
  단어가 특정 토픽에 존재할 확률과 특정 토픽이 존재할 확률을 결합확룰로 추정해 토픽을 추출한다.

#### 통계적 모델링
- 통계적 모델링 : 확률/수학적 모형을 가지고 현실의 데이터형성과정을 모방한 것. 현실을 그대로 반영하지는 못하지만, 적절한 가정들 하에 유용하게 쓰임.
- 통계적 모델링의 목적 : 불확실성의 측정, 통계쩍 추론, 가설 검증을 위한 도구 제공, 머신러닝에서의 예측.
- 과정 : 문제 이해 > 계획수립/데이터수집 > 자료탐색 > 모델상정 > 모델적용 > 모델확인 > 더 나은 모델을 위해 상정~확인 반복 > 모델사용

- 마르코프 확률 과정 : 현재에 대한 조건부로 과거와 미래가 서로 독립인 확률과정(미래를 유추하려 한다면 과거는 아무 정보 제공을 못하고, 오직 현재의 값만 쓸모가 있음). 
- HMM(hidden Markov model, 은닉 마르코프 모델) : 통계적 마르코프 모형의 하나, 시스템이 은닉상태와 관찰가능한 결과 두가지 요소로 이뤄져있다 보는 모델. 마르코프 과정을 통해 도출된 결과만 관찰가능.
- CRF(conditional random field, 조건부 무작위장) : 통계적 모델링 방법중 하나. 구조적 예측에 사용. 이웃하는 표본을 고려. 입력시퀀스에 대한 출력시퀀스의 조건부확률.

### 토큰화 
- 토큰화 : 텍스트를 토큰이라는 작은 부분으로 분할하는 과정. 문장 토큰화와 단어(단어, 단어구, 형태소)토큰화로 나뉨.

- 인코딩 : 문서의 인코딩이 utf-8 등 처리가 불가한 것으로 되어있을 경우, s.encode("utf8").decode("ascii",'ignore')식으로 바뀌줘야 한다
- 토큰화의 기준 : 토큰화 중 토큰화의 기준(축약형 등)을 선택해야 하거나, 구두점/특수문자등이 고유한 뜻을 가지거나 단어의 해석에 도움이 되는 경우도 있어 단순 제외는 피해야 함.
- 토큰화 함수(단어) : 단어 내에 뛰어쓰기나 구두점이 있는 경우도 있어 토큰화 함수는 그런 단어들을 하나로 인식할 수 있는 능력 또한 필요함.  
- 토큰화 함수(문장) : 문장 토큰화의 경우에도 문장 내에 구두점이 있는 경우가 있기에 단순 구두점을 기준으로 구분해서는 안 됨.
- 마침표의 처리를 위해 입력에 따라 두 클래스로 분류하는 이진(문장의 끝, 이외)분류기를 사용하기도 하며, 이를 위해 약어사전이 유용하게 쓰임.
- 한국어는 조사까지 분리해줄 필요가 있어 단순 공백으로는 구분할 수 없(형태소 토큰화 필요)고, 띄어쓰기가 비교적 잘 지켜지지 않는 등 토큰화가 어려움.
#### 서브워드 토큰화
- 서브워드 분리(Subword segmentation) : 한 단어를 여러 서브워드(더 작은 단위의 의미있는 단어)로 분리해 인코딩 및 임베딩. OOV나 희귀, 신조어 등의 문제를 완화가능.
  
- BPE(Byte Pair Encoding) : 대표적인 서브워드 분리 알고리즘. 원래는 데이터 압축 알고리즘. 연속적으로 가장 많이 등장한 글자의 쌍을 찾아 하나의 글자로 병합. 
  더이상 병합할 바이트의 쌍이 없거나 토큰의 수/길이가 원하는 정도가 될 때 까지 병합.   
- BPE in NLP : 글자단위에서 점차적으로 단어집합을 만드는 bottom up 방식의 접근을 사용. 훈련데이터의 단어들을 글자/유니코드 단위로 단어 집함을 만들어, 
  가장 많이 등장하는 유니그램을 한 유니그램으로 통합. 
- BPE 알고리즘 : 딕셔너리(등장하는 단어의 집합)를 글자단위로 분리(초기단어집합), 알고리즘의 동작(가장 빈도수가 높은 유니그램(바이트)의 쌍을 한 유니그램으로 통합)을 몇 번 반복할지 결정.

- WordPiece : BPE의 변형 알고리즘. 가장 빈번한 기호 쌍이 아닌 코퍼스의 가능도(우도, likelihood)를 가장 높이는 쌍을 병합. BERT/DistillBERT 및 Electra에서 사용.
  학습데이터의 모든 문자를 포함하도록 어휘를 초기화하고 주어진 수의 병합규칙을 점진적으로 학습. 가치의 확인을 위해 두개의 기호를 병합해 손실을 평가한다는 차이가 있음.

- Unigram : 기본 어휘를 많은 수의 토큰(모든 단어와 일반적인 하위 문자열)으로 초기화한 뒤 각 토큰을 점진적으로 줄임. 주로 SentencePiece와 함께 사용됨. 
  각 단계에서 현재 어휘와 훈련데이터의 손실을 정의 후, 특정 토큰이 제거될 시 전체 손실이 얼마나 증가할 지 계산, 가장 적은 영향을 미치는 토큰을 제거함. 이를 반복.
  학습 후 각각의 확률을 계산할 수 있도록 어휘저장 외에 각 토큰의 확률을 저장함. 손실은 L = -(N)∑(i=1)log(∑(x∈S(xi))P(X))의 식으로 전개됨. S(xi)는 단어에 가능한 모든 토큰화.

- SentencePiece : 공백을 사용해 단어를 구분하는 다른 알고리즘과 달리, 입력을 원시스트림으로 처리 후 BPE또는 유니그램으로 적절한 어휘를 구성하는 방법.

### 문장 벡터화 (정수 인코딩)
- 국소표현 : 해당 단어만 봄. 이산표현 이라고도 함.
- 분산표현 : 주변을 참고해 그 단어를 표현. 연속표현 이라고도 함.
  
- 단어집합(사전) : 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓은 것. 전체 단어 뿐 아니라 특별 토큰(pad, unk 등)도 등록해두어야 함. 
- 정수인코딩 : 단어집합에 고유한 숫자를 부여(각 단어에 빈도수 등을 기준으로, 순서대로 고유한 정수를 부여)하는 작업.
- 문장 벡터화 : 정수 인코딩된 단어집합의 단어들을 모델이 처리하기 쉽도록 벡터화 하는 것. 원-핫 인코딩(희소행렬), 워드 임베딩(밀집벡터)등을 이용함.  

- 카운트 기반 단어 표현 : 정수 인코딩 방법중 하나. 이름대로 등장 빈도 등 횟수에 따라 인덱스를 부여.
- 빈도수로 벡터화 : (토큰화>정제,불용어 제거>등장 단어,빈도수 기록)의 과정을 거쳐 각 단어의 빈도수가 높은 단어부터 낮은 인덱스를 부여하는 방법으로도 벡터화가 가능하다.  
- 원 핫 인코딩을 이용한 벡터화 : 단어집합의 크기를 차원으로, 표현하고 싶은 단어의 인덱스엔 1, 나머지는 0을 부여하는 표현방식. 결과 벡터를 원-핫벡터(희소행렬임)라고 함.
- 원-핫 인코딩 단점 : 단어의 개수가 늘어날 수록 벡터 저장을 위한 공간이 늘어나고, 단어의 유사도를 표현하지 못한다는 문제점을 가짐.
- OOV(Out-Of-Vocabulary) : 정제, 불용어 처리 과정을 거쳐 생긴 단어집합(모든 단어를 중복을 허용하지 않고 모아놓음)에 없는 단어. 따로 OOV 의 레이블을 만들어 OOV 를 인코딩 해줘야 한다.
 
- bag of words : 정수인코딩 방법 중 하나. 단어의 출연 빈도로 문장을 나타냄. 문장의 유사도파악 등 에도 사용됨.
- bag of words 단점 : Sparsity(단어의 개수가 많다보니 0이 많아 계산량이 많아짐), 흔한 단어의 힘이 세짐, 단어의 순서를 완전 무시, 처음 보는 단어는 처리 불가 등의 단점이 있다.
- DTM(Document Term Matrix, 문서 단어 행렬) : 서로 다른 문서의 Bow 들을 결합한 표현 방법. 각 BoW 문서를 행으로, 단어를 열으로 하는 행렬로 만듦. 
  희소표현(원핫벡터와 마찬가지, 0이 많아져 공간에 문제), 단순빈도수접근(불용어 처리 불가)등의 문제점을 가짐. 
- TF-IDF(term frequency inverse doc freq) : DTM, 문서에서 각 단어별 문서 연관성(문서에서 가진 정보)을 파악. 단어빈도와 역문서빈도를 사용해 각 단어들마다 중요도를 가중치로 부여. 
  모든 문서에서 자주 등장시 중요도를 낮게, 특정 문서에서 자주 등장시 중요도를 높게 함. 
  TF(특정 문서 d 에서 특정 단어의 등장 횟수)*IDF(log(n/1+(t가 등장한 문서 수), 종종 자연 상수(ln)))로 이뤄짐.
- IDF : 자주 등장하는 단어(불용어)에 패널티를 주어 위의 것을 보완한 방법. log(총 문장 개수/단어 출현 문장 개수(+1, DIV_0를 피하기 위해)) 의 공식을 사용함.
  TF-IDF 의 결과는 (데이터 개수, 사용 단어 수)로 나타난다.

- Recommendation System using Document Embedding : 문서간 비교를 위해 각 문서를 고정된 길이의 벡터로 변환. 
- 문서 벡터 변환 : Doc2Vec, Sent2Vec 등을 사용할 수 도 있으나, 가장 간단한 방법은 문서에 존재하는 단어 벡터들의 평균을 구하는 것임.
#### 패딩
- 패딩 : 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업. 이를 통해 문장들을 하나의 행렬로 보고 병렬처리를 가능하게 함.
- 방법 : 정수 인코딩시 문장별로 단어를 모아 두어 문장의 길이를 확인할 수 있는데, 이때 가장 긴 문장의 길이에 맞게 다른 문장에 0을 삽입한다.

### Embedding
- Word Embeddings : 텍스트 내 단어를 밀집 벡터로 만드는 과정. 단어의 유사도(의미)정보도 벡터화 가능. 유사도에 따라 단어가 유사한 값을 띄게 됨. 
  정수 인코딩(원핫인코딩)된 벡터를 입력으로 받음.
- 임베딩 벡터 : 사용자가 설정한 임베딩벡터 사이즈(차원)에 따라 임베딩 벡터가 생성됨. 임베딩 벡터 안에는 각 단어의 정보를 가지고 있는 (임베딩 사이즈)개의 실수가 들어 있음.  
- 밀집벡터 : 희소벡터(대부분의 값이 0인 벡터)와 달리 대부분의 값이 실수이고 상대적으로 저차원인 벡터. 사용자가 설정한 값으로 벡터의 차원을 맞춤. 
  특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로브터 임베딩 벡터값을 가져오는 룩업테이블.
- 분산표현 : 분포 가설(비슷한 위치에서 등장하는 단어는 비슷한 의미를 가짐)에 기초해 만들어진 표현 방법. 단어의 의미를 여러 차원에 분산하여 표현. 단어간 유사도 계산 가능.

- 임베딩 층 : 정수 인코딩된 단어들을 입력으로 받아 밀집벡터(임베딩벡터)로 변환. 입력 정수에 대해 밀집벡터로 맵핑하고, 이 벡터는 가중치가 학습되는 것과 같은 방식으로 훈련됨. 
  원-핫 벡터를 입력으로 받으면 룩업테이블과 내적곱을 해 임베딩 벡터를 가져오고, 정수인코딩된 단어를 입력으로 받으면 인덱스로 가져옴.

- 워드 임베딩 평균 : 임베딩층 뒤에 GlobalAveragePooling1D()를 사용해 할 수 있음. 기타 은닉층의 사용 없어도 높은 정확도의 분류가 가능함.
#### char embedding (글자 임베딩)
- 글자 임베딩 : 워드 임베딩과는 다르게 단어의 벡터 표현을 얻어 OOV문제를 해결함. 모르는 단어도 뜻을 추측(이해)해서 예측함. 워드 임베딩의 대체로도 사용할 수 있지만, 
  워드 임베딩과 연결해 신경망의 입력으로 사용하기도 함. 글자단위의 정수 인코딩이 필요.
  
- 1D CNN 이용 : FastText가 글자의 N-gram조함을 이용하듯이, 전체입력 내부의 더 작은 시퀀스로부터(N-gram) 정보를 얻음.
- 1D CNN 방법 : 단어를 글자단위로 분리, 글자에 대해 임베딩(이후 패딩 가능), 그 후 1D CNN을 적용. 나온 벡터를 풀링층을 거쳐 스칼라로 만들고, 이를 연결해 하나의 벡터로 만듦. 
  이 벡터가 단어의 벡터.
  
- BiLSTM 이용 : 단어를 글자로 쪼갠 뒤 임베딩, 정방향/역방향 LSTM을 사용, 둘의 마지막 은닉 상태를 연결해 벡터화, 이를 단어의 벡터로 사용.
#### sentence Embedding(문장임베딩) 
- 문장임베딩 : 다수의 문장간 비교를 위해 각 문장을 고정된 길이의 벡터로 변환하는 것. 여러 방법이 있으나, 간단하게는 문장의 단어벡터평균을 구하는 방법이 있음.
- 과정 : 모든 문장에 대해 문장벡터를 만든 뒤, 문장벡터간 코사인 유사도 행렬을 만들고, 이를 페이지랭크 알고리즘의 입력으로 사용해 각 문장의 점수를 구한 뒤, 
  점수가 가장 높은 문장을 상위n개 선택해 문서의 요약문으로 함. 

#### Word2Vec
- word to vector : 워드 임베딩을 하는 대표적 방법. 은닉층에 활성화 함수가 존재하지 않으며, 룩업테이블(투사층)이 그 역할을 대신함. 
  일반적으로 SGNS(Skip-Gram with Negative Sampling)을 사용. 동음이의어를 잘 반영하지 못한다는 문제점이 있음. 추천시스템에도 사용.
- W2V 자세히 : 투사층(임베딩 벡터의 차원(M)), 입력층과 투사층 사이(단어집합크기(V)*M), 투사층과 출력층 사이(M\*V) 의 크기를 가짐. 
  윈도우 크기 내에서만 주변 단어를 고려해 전체적 통계 정보를 반영하지 못한다는 문제점을 가짐. 
- VS NNLM : NNLM의 느린 속도와 정확도를 개선해 탄생. 다음단어가 아닌 중심 단어를 예측, 이전단어만 참고하는게 아닌 전후 단어 모두 참고, 활성화 함수와 은닉층 제거, 
  계층적 softmax와 네거티브 샘플링 등을 이용해 은닉층 제거 이외에도 속도를 개선.   

- 네거티브 샘플링(Negative Sampling) : W2V가 학습과정에서 전체 단어가 아닌 일부 단어에만 집중할 수 있도록 하는 방법. 
  무작위로 주변단어가 아닌 단어를 선택하고, 그것들을 기준으로 단어 집합을 만들어 긍정과 부정의 이진 분류 문제로 만듦.
- 자세히 : 일반 W2V(Skip-Gram)에는 속도(마지막 단계에서 softmax를 적용하고 오차를 구하며 임베딩 벡터를 조정하는 작업을 관계없는 단어에도 적용)문제를 가짐.
  해결(일부집합에 대해서만 고려)을 위해, 주변 단어를 가져와 집합을 만들고, 임의로 주변단어가 아닌 단어를 가져와 집합을 만든 뒤 마지막 단계를 이진분류 문제로 바꿈.
  이는 다중클래스 분류 문제를 이진분류문제로 바꾸면서도 훨씬 효율적인 연산량을 가짐.

- CBOW(Continuous Bag of Words) : word2vec의 두가지 방법중 하나. 주변 단어를 이용해 중간의 단어를 예측. 
  윈도우(앞뒤로 몇개의 단어를 볼 지)를 계속 움직여 주변과 중심 단어 선택을 바꿔가며 데이터 셋을 만드는(슬라이딩 윈도우)것이 가능. 
- CBOW 자세히 : 두개의 가중치 행렬(입력-투사, 투사-출력)을 학습해 나감. 입력된 원 핫 벡터(윈도우 안의 단어들)들을 가중치 행렬과 곱(i번째 행을 가져옴)해 그 결과 벡터들의 평균을 구함.
  그 평균 벡터는 두번째 가중치 행렬과 곱해져 원-핫 벡터와 차원이 동일한 벡터를 생성하고, 여기에 softmax와 손실함수로 cross-entropy를 사용해 
  각 단어가 중간 단어일 확률을 가진 스코어 벡터를 생성.
- Skip-gram : word2vec의 또다른 방법. 중심 단어에서 주변 단어(윈도우 안의 단어들)를 예측. CBOW와 유사한 과정을 통해 구함. CBOW보다 성능이 좋다고 알려져 있음.
- SGNG(Skip-Gram with Negative Sampling) : 네거티브 샘플링을 사용하는 Skip-Gram. 중심단어와 주변단어 둘 다 입력이 되고, 두 단어가 실제 윈도우 내에 존재하는 이웃일 확률을 계산. 

- 사전훈련된 W2V(GloVe)코드 : [urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", 
  filename="GoogleNews-vectors-negative300.bin.gz")
  word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True))]

- LSA(잠재 의미 분석) : 각 단어의 빈도수를 카운트한 행렬(DTM)을 입력으로 받아 차원을 축소(Truncated SVD)해 잠재된 의미를끌어내는 방법론. 토픽모델링 기법이자 워드 임베딩 방법.
- 단점 : 카운트 기반으로 코퍼스의 전체적인 통계정보를 고려하기는 하지만, 단어의 의미 유치 작업에서 성능이 떨어짐.  

#### GloVe
- GloVe : 또 다른 워드 임베딩 방법. 카운트 기반과 예측 기반 모두 사용하는 방법론. 카운트 기반의 LSA(잠재 의미 분석)과 W2V의 단점을 지적, 
  보완한다는 목적으로 나와 W2V와 비슷한 성능을 보여줌.  
- GloVe 아이디어 : 임베딩된 중심 단어와 주변 단어 벡터의 내적곱이 전체 데이터에서의 동시 등장 확률(log(P(i,j)))이 되도록 만드는 것. 
- 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix) : 행과 열을 전체 단어의 집합으로 구성한 뒤, i단어의 윈도우 안에서 j단어가 등장한 횟수를 (i,j)에 나타낸 행렬. 
  전치해도 동일한 행렬.
- 동시 등장 확률(Co-occurrence Probability) : 동시 등장 행렬에서 특정 단어(i)의 전체 등장 횟수와 특정 단어(i)가 등장했을때 특정 단어(j)가 
  등장한 횟수를 카운트해 계산((i,j)/sum(i))한 조건부 확률. 

#### FastText
- FastText : 페이스북 개발 워드 임베딩 방법. 메커니즘은 W2V와 비슷하지만, 하나의 단어 안에도 여러 단어들이 존재한다고 생각(내부단어 고려)함. 
  W2V, LSA와 달리 OOV와 RaerWord에 대한 대응이 가능함.
- 내부단어 고려 : 각 단어를 글자단위 n-gram의 구성으로 취급. n을 몇으로 하냐에 따라 단어들이 얼마나 분리될지 결정. 
- 토큰 분리 : 시작과 끝을 의미하는 <, > 에 기존 단어에 <>를 추가한 토큰까지 총 (글자수 - n + 4)개의 토큰이 나옴. 
  이때 n의 범위를 지정할 수 있는데, 이 경우 범위 안의 정수가 모두, 차례대로 n으로 사용됨.

- OOV(Out Of Vocabulary)대응 : 데이터 셋 내 모든 단어의 n-gram에 대해 워드 임베딩이 되어, 데이터셋만 충분하면 모르는 단어(OOV)에 대해서도 다른 단어와 유사도 계산이 가능. 
- RareWord(빈도수가 적은 단어)대응 : 단어가 희귀단어라도 그 단어의 n-gram이 다른 단어와 겹친다면 비교적 높은 임베딩 벡터값(유사도)을 얻음. 
  노이즈가 많은 코퍼스에서도 같은 이유로 강점을 가짐. 

#### ELMo
- ELMo(Embeddings from Language Model) : 새로운 워드 임베딩 방법. 사전 훈련된 언어 모델 사용. 같은 표기의 단어라도 문맥을 반영해 임베딩 할 수 있음.
- biLM(Bidirectional Language Model) : 양쪽 방향 RNN을 모두 사용하는 언어 모델. 단어단위로 입력을 받음. 다층구조. 다음 층으로 보내기 전이 아니라 훈련 후 표현을 위해 은닉상태를 연결. 
- biLM 이용 임베딩 : 각 층을 연결 > 출력별로 가중치 > 출력값을 전부 더함 > 벡터의 크기(스칼라)를 곱함. 이렇게 완성된 벡터를 ELMo표현이라고 하고, 
  이를 기존 임베딩 벡터와 함께(기존 벡터와 연결, 사전 흔련된 언어 모델 가중치는 고정)사용할 수 있음.

## 디코딩 알고리즘
- 디코딩 알고리즘 : 확률분포로부터 최종출력단어를 결정하는데 사용하는 알고리즘. 
- GreedyDecoding : 각 스텝의 출력확률분포에서 최대확률인 단어 선택. 현재만 기준으로 최선을 선택해 backtracking이 안되기에 출력에 오류를 포함하기 쉽고, 부자연/어법 등의 문제가 존재. 
- Beam Search : 높은 확률값을 갖는 단어들의 후보군을 만들어, 상위 k개의 후보군을 동시에 트래킹함. k가 작으면 greedyDecoding같은/k가 크면 계산비용 증가와 포괄적이 된단 문제.
  각 스텝마다 vocabulary의 각 토큰을 기존 가설에 추가하고, 결과를 채점한 뒤 가장 높은 점수의 것을 선택해 새로운 가설을 형성함. 빔사이즈를 바꿔 실험해볼 필요가 있음.
- sampling-based Decoding : LM 디코더의 예측확률분포에서 확률샘플링으로 매 step출력단어를 결정. PureSampling(t 확률분포에서 이후 단어를 랜덤(확률)추출(하나 추출, argmax는 아님))
  과 Top-n sampling(t의 확률분포에서 이후단어를 랜덤(확률)추출, 여기서 top-n개 단어만 고려. 축약된 확률분포 이용. n=1(그리드)/V(pure). n 증가-다양/위험, 감소-일반/안전 출력)이 있음.
- SoftmaxTemperature : Scaled-dot product 어텐션처럼 softmax전에 입력되는 단어점수를 스케일링함(softmax는 0중심함수라, 입력값이 0에서 멀어지면 포화(saturated)되는 경향이 있음).
  원래 `np.exp(x) / np.sum(np.exp(x))`였던 softmax식을 `np.exp(x/t) / np.sum(np.exp(x/t))`로 바꿈. t는 하이퍼 파라미터로, 커지면 확률분포가 고르게, 적이지면 첨예하게 됨. 

## Metrics

### 유사척도(평가)
- 벡터 유사도 : 각 문서의 단어들을 어떤 방법으로 수치화해 표현 했는지, 문서간 단어 차이를 어떤 방법으로 계산했는지에 따라 달라짐. 
  이런 유사도르 이용해 영화추천(본 영화와 비슷한 영화를 추천)등을 구현할 수 있음.
- 코사인 유사도 : 두 벡터간 코사인 각도를 이용해 구할 수 있는 두 벡터의 우사도. 문자열을 축으로 해 축과 문장간의 각도로 유사도를 측정. 둘의 곱/둘의 거리로 구할 수 있음. 
- 자카드 유사도(자카드 계수, 타니모토 계수) : 두 세트의 곱집합 / 합집합. 동일하면 1, 완전 다르면 0. 두 문서의 총 단어 중 두 문서 전부 등장한 단어 비율.
- 유클리디안(유클리드) 거리 : 단어간의 거리가 짧으면 더욱 유사. 벡터 공간에서 유사도 측정. 자카드나 코사인보다는 효과가 떨어짐.

- 이진 거리 : 문자열 유사도 메트릭. 두 라벨이 동일하면 0, 다르면 1을 반환.
- 매시 거리 : 부분 일치에 기초. 1 - (곱집합 길이/합집합 길이)*(두 세트의 길이차에 따른 점수(1, 0.67, 0.33, 0))

- 지프의 법칙 : 토큰이 언어로 배포되는 방법을 설명. 토큰 빈도가 정렬된 목록의 순위와 정비례하게 함. 
- 편집 거리 : 두 문자열을 동일하게 하려면 삽입,대체,삭제해야 하는 문자의 수를 계산. 
- 스미스 워터맨 거리 : 편집거리와 유사. 관련된 단백질서열 및 광학정렬을 검출하기 위해 개발됨. 
  
- 문서의 유사도 구하기 : bag of ward 에 코사인 유사도를 적용하거나(bow 각 요소들의 곱의 합/bow 합의 제곱근)TF-IDF 를 적용하는 방법으로도 구할 수 있음. 
  구현이 쉽고 불용어를 잘 거른다는 장점이 있지만, 단어단위로 보기에 동음이의어를 잡지 못하고 토픽은 알 수 없다는 단점이 있음.
- WMD(word mover's distance) : 문서의 유사도를 구하는 방법. word2vec 근간으로 하여 단어들간의 유클리디안 거리를 사용. 
  w2v 로 임베딩된 단어에 대해 문서 단어들의 유사도를 판단한다. 거리가 다양하다면 단어를 여러 벡터에 대입하여 판단. 이것을 보완한 RWMD(속도 빠름, 결과 비슷)도 존재한다. 

- 혼동행렬 : 머신러닝에서 정확도 측정시 자세한 내용을 알기 위해 사용. TP/FN/FP/TN 으로 이뤄진 행렬. 정밀도, 재현율 등이 등장.

- 펄플렉서티(Perplexity, PPL) : 언어 모델 평가를 위한 내부 평가 지표. 낮을수록 성능이 좋음. 테스트 데이터에 의존. 분기계수(선택할 수 있는 가능한 경우의 수)임.

- unigram precision : 기계번역, 단어계수 카운트로 측정. 후보 문장 중 reference에서 등장한 단어의 개수를 전체 단어의 개수로 나눔(count/cand_ngram_count). 
  같은 단어가 여러번 등장하면 정확도가 높아져 중복을 고려한 측정이 필요.
- 보정된 유니그램 정밀도(Modified Unigram Precision) : 단어의 중복을 고려하기 위해, 유니그램이 한 ref에서 최대 몇번 등장했는지를 센 후 
  기존 count보다 작다면(min(count,max_ref_count))그것을 사용해 정밀도를 계산한다. 문맥의 고려가 불가.

- BLEU(Bilingual Evaluation Understudy)Score : 기계번역의 성능측정을 위해 사용. 높을수록 성능이 좋음. 언어에 구애받지 않고, 빠름. 
  보정된 유니그램 정밀도에서 문맥을 위해 유니그램을 n-gram으로 교체하고,
  후보 문장의 길이가 점수에 주는 영향을 상쇄하기 위해 (길이가 가장 비슷한)참조 문장의 길이가 후보보다 크면 패널티를 줌.
- BLEU 식 : BLEU=BP×exp(^N∑_(n=1)w_n log p_n). BP = (c>r)? 1 : e^(1−r/c).   C = Candidate 길이, r : Candidate와 가장 적은 길이 차이의 Reference 길이.

### BenchMarks
- GLUE : 일반언어이해평가(General Language Understanding Evaluation)벤치마크. NLU기술이 실질적으로 최대한 유용하려면 일반적이여야 하므로 등장한, 기존의 다양한 작업에서 모델의 성능을 평가하고 분석하기 위한도구.
- GELU Tasks : TextClassification, SentimentAnalysis, SemanticTentualSimilarity, Paraphrasing, 
  NaturalLanguageInference, LinguisticAcceptabiliy, StochasticOptimization, QA등.

## NN
- MLP(다층 퍼셉트론) : 단층 퍼셉트론에서 은닉층이 1개 이상 추가된 신경망. FF 신경망의 가장 기본적 형태.
- NNLM(신경망 언어모델) : n-gram 언어 모델의 희소문제 해결을 위해 등장. 기계가 단어간 유사도를 파악. n 개의 단어 벡터를 이용해 목표 단어를 예측. 
- NNLM 특징 : 밀집 벡터로 인해 희소문제를 해결했고, 모든 n-gram 을 저장하지 않아도 된다는 이점이 있지만, 마찬가지로 정해진 n개의 단어만 확인하고 
  입력의 길이가 고정되어 있다는 문제점이 있다.

### RNN
- 순환 신경망 : 자연어 처리에서 단어의 품사 추축등 여러 분야에서 대표적으로 이용됨. 입력과 출력의 길이를 다르게 셜계할 수 있음. 가장 기본적인 시퀀스 모델.
  뉴런단위 보단 입력, 출력벡터와 은닉상태(층)라는 표현을 주로 씀. 모든 은닉층의 상태를 출력 > 각 스텝마다 cost를 계산해 하위 스텝으로 전파 > 
  각 가중치를 업데이트하게 하면 many-to-many 문제를 해결할 수 있음.

- Ht=tanh(WxXt+WhHt−1+b) 의 식을 사용함. 두 입력이 각각의 가중치와 곱해져 메모리 셀의 입력이 되고, 이를 활성화 함수(tanh)의 입력으로 사용, 그 값이 은닉층의 출력(은닉상태)이 됨.
- Wx는 (은닉 상태의 크기 × 입력의 차원), Wh는 (은닉 상태의 크기 × 은닉 상태의 크기), b는 (은닉 상태의 크기)의 크기를 가짐.
- 장기 의존성 문제 : Vanilla RNN(Simple RNN)의 단점. 시점이 길어질수록 앞의 정보고 뒤로 전달되질 못해 비교적 짧은 시퀀스에서만 효과를 보이는 것.
  
- 메모리셀 : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드. 이전의 값을 기억하려는 일종의 메모리 역할을 수행. RNN 셀 또는 그냥 셀 이라고 함. 
  이전 은닉층의 셀에서 나온 값은 자신의 입력으로 사용하는 재귀적 활동도 함.
- 은닉상태 : 메모리셀이 츨력층이나 다음 시점의 자신에게 보내는 값. 현재 시점의 메모리 셀은 이전 시점의 메모리셀이 보낸 은닉값을 현시점의 은닉 상태 계산을 위한 입력값으로 사욯함.
- RNN 층 : (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받아 설정에 따라 메모리 셀 최종지점의 은닉상태((batch_size, output_dim), many_to_one 문제)나 
  각 시점의 상태값을 모아 전체 시퀀스를((batch_size, timesteps, output_dim), many-to-many 문제)출력한다.

- DRNN(Deep Recurrent Neural Network) : 깊은 순환 신경망. 다수의 은닉층을 가지는 RNN.
- BRNN(Bidirectional Recurrent Neural Network) : 양방향 순환 신경망. 어느 시점의 값 예측을 위해 이전 데이터 뿐 아닌 이후 데이터도 사용. 두개의 메모리셀 사용.
- CRNN(Char Recurrent Neural Network) : 글자단위 RNN. 입출력의 단위가 단어가 아닌 글자.

- RNN 이용 텍스트 분류 : 텍스트를 입력으로 박아 텍스트가 어떤 종류의 범주에 속하는지 구분. 감성분석, 의도분석 등으로 나뉨. 
  다대일(Many-to-one, 모든 시점에 대해 입력을 받아 최종 지점의 셀만 출력)문제에 속함.

- BPTT(BackPropagation through time) : 모델이 기억할 시퀀스 길이. 숫자가 높을수록 모델의 복잡성과 메모리 사용량이 증가. pytorch에선 t.detach()로 추적을 멈추는 식으로 구현할 수 있음.

###### RNNLM
- RNNLM(RNN 언어 모델) : RNN을 이용한 언어 모델. 입력의 길이를 고정하지 않을 수 있음.  
- 교사강요 : RNN, RNNLM의 훈련방법. 훈련 시 t시점에서 예측한 값을 다음 시점의 입력으로 사용하는게 아닌 t시점의 레이블(정답)을 다음 시점의 입력으로 사용한다.

- Char RNNLM : 글자단위 RNN언어모델. 글자단위를 입출력으로 하여 임베딩 층을 사용하지 않음. 

###### LSTM
- LSTM : 장단기 메모리(Long Short-Term Memory). 장기 의존성 문제를 해결하기 위해, 메모리 셀에 입력게이트, 망각 게이트, 출력게이트를 추가해 불필요한 기억을 지움. 
  셀 상태 라는 값이 추가됨. 긴 시퀀스의 입력을 처리하는데 탁월.
- LSTM 처리 : 각 단어가 벡터로 면환된 문장 행렬로 입력을 받아(DTM등)시점마다 한 행식 입력으로 받아 처리.
- BiLSTM : 양방향 LSTM. 두개의 독립적 LSTM 아키텍쳐를 함께 사용. 

- 셀 상태(장기상태) : LSTM 에서 추가된 값. 이전시점의 셀 상태가 다음의 셀 상태를 구하기 위한 입력으로 사용됨. 입력게이트에서 나온 두 값을 원소곱(같은 위치끼리 곱)을 하고, 
  이걸 삭제 게이트 결과값에 더헤 현재 셀 상태를 구함.
- 게이트 : 은닉 상태(단기상태)값과 셀 상태 값을 구하기 위해 3개의 게이트(삭제,입력,출력. 모두 시그모이드 함수 존재)가 사용됨. 
- 입력 게이트 : 현재 정보 기억을 위한 게이트. (현재시점 x값 * 가중치)+(이전시점 은닉상태 * 가중치)가 시그모이드 함수를 지나고, 같은 값이 tanh 을 지남. 
  결과로 나온 두 값을 가지고 선택된 기억할 정보의 양을 정함. 시그모이드 함수의 값이 0이라면 현재의 입력값이 출력값에 영향을 줄 수 없음. 현시점 입력값의 반영도를 뜻함. 
- 삭제 게이트 : 기억 삭제를 위한 게이트. 현재시점 x값, 이전시점 은닉상태가 시그모이드 함수를 지남. 나온 값이 삭제 과정을 거친 정보의 양. 
  나온 값이 0이라면 이전의 상태값이 현재의 결과에 영향을 줄 수 없음. 이전시점 입력값의 반영도를 뜻함.
- 출력 게이트 : 현 시점의 은닉상태 결정을 위한 게이트. 현 시점 x값, 이전 은닉상태가 시그모이드를 지난 값.
``` 식
i_t(input_gate)  = sigmoid(W_ii * x_t + b_ii + W_hi * h_(t-1) + b_hi)
f_t(forget_gate) = sigmoid(W_if * x_t + b_if + W_hf * h_(h-1) + b_hf)
g_t(cell)        = tanh   (W_ig * x_t + b_ig + W_hg * h_(t-1) + b_hg)
o_t(output_gate) = sigmoid(W_io * x_t + b_io + W_ho * h_(t-1) + b_ho)
c_t(cell_state)  = f_t ⨀ c_(t-1) + i_t ⨀ g_t
h_t(hidden_state)= o_t ⨀ tanh(c_t)
```

- 양방향 LSTM + CRF : CRF(Conditional Random Field)와 양방향 LSTM을 함께 사용. 레이블 사이의 의존성(예측 개체명)을 고려할 수 있음. 
  활성화 함수의 결과를 CRF층으로 전달, 레이블의 시퀀스에 대해 가장 높은 시퀀스를 가지는 시퀀스를 예측. 출력 레이블에 대한 양방향 문맥을 반영. 
  keras_contrib의 설치가 필요 ([!pip install git+https://www.github.com/keras-team/keras-contrib.git]).

###### GRU
- GRU(Gated Recurrent Unit) : 게이트 순환 유닛. LSTM과 성능은 유사하며 복잡했던 구조를 간단화 시킴. 업데이트 게이트와 리셋 단 두개의 게이트 만을 사용함. 
- 성능 : 데이터 양이 적을때는 GRU 가, 많을때는 LSTM이 낫다고 알려져 있음. 

### CNN
- CNN(Convolution Neural Network) : 합성곱 신경망. 원래는 비전 분야에서 주로 사용되지만 NLP에 사용하기 위한 다양한 시도가 있었음.

- 1D CNN : 자연어 처리에 활용되는 합성곱. 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음(LSTM과 동일). 
- 1D CNN 커널 : 커널은 너비를 임베딩 벡터 차원과 동일하게 설정하고, 높이만 따로 설정해 커널의 사이즈라고 칭함. 너비가 임베딩 벡터와 동일하니 높이 방향으로만 움직임.
- 학습 방법 : 사이즈가 같거나 다른 커널 여러개를 사용해 벡터를 여러개 얻고, 거기에 풀링을 거쳐 스칼라화 한 다음, 그것들을 모두 연결해 하나의 벡터로 만듦. 
  이를 출력층에 연결해 텍스트 분류를 실행.

### seq2seq
- Sequence-to-Sequence : 번역기등 입력 시퀀스에서 다른 시퀀스를 출력하는 기능에서 대표적으로 사용되는 모델. 크게 인코더와 디코더 두개의 아키텍쳐로 구성. 
  고정크기의 벡터에 모든 정보를 압축하려 하니 생기는 정보의 손실과, RNN의 기울기 소실 문제가 있음. 
  컨택스트 벡터를 디코더의 초기 은닉상태로 사용하거나(기본), 매시점마다 하나의 입력으로 사용하거나, 
  어텐션매커니즘을 통해 더욱 문맥을 반영하는 컨택스트 벡터를 생성해 매시점 입력으로 사용하는 등의 변형이 있음.
- S2S 훈련 : 두개의 쌍의 길이가 같지 않아도 됨. 훈련시에는 디코더에 원래 문장을 입력(<sos> + sent)받으면 정답(sent + <eos>)이 나와야 된다 알려주며(+ 교사강요로 디코더 입력도 사용)
  훈련하나, 테스트(실제)과정에선 컨텍스트 벡터와 <sos>만을 입력받아 단어를 예측하게(교사강요 사용 X)함. 따라서 decoder_input(<sos>+sent)/decoder_tar(sent+<eos>)따로 인코딩 필요.
- RNN : 인코더와 디코더의 내부는 전부 RNN(LSTM/GRU)으로 구성되어 있음. 입력 문장을 받는 RNN셀이 인코더, 문장을 출력하는 RNN셀이 디코더. 
  
- S2S 인코더 : 입력 문장의 모든 단어(임베딩 벡터)를 순차적으로 입력받아 마지막에 모든 단어 정보를 압축, 하나의 벡터(컨텍스트 벡터)로 만듦. 이를 디코더로 전송.
- 인코더 자세히 : 입력 문장이 단어토큰화를 거쳐, 각 토큰 각각이 RNN셀 각 시점의 입력이 됨. 마지막 시점의 은닉상태를 컨텍스트 벡터로 함.
- S2S 디코더 : 인코더로부터 전달받은 컨텍스트 벡터에서 번역된 단어를 한개씩 순차적으로 출력. 기본적으로 RNNLM, 기본적으로 이전시점의 출력만을 입력값으로 함. 
  초기 입력으로 문장의 시작을 의미하는 <sos>가 들어오고, 문장의 끝을 의미하는 <eos>가 나오면 반복종료.
- 디코더 자세히 : <sos>가 들어온 후 다음 올 단어를 예측하고, 그 단어를 다믐 시점의 입력으로 넣는 것을 <eos>가 다음 단어로 예측될 때 까지 반복(테스트 과정). 
  <sos>와 <eos>는 문장의 시작과 끝을 의미하는 어떤 단어(\t, <sos>등)도 될 수 있음. 실제값 시퀀스에서는 시작토큰을 제거해야 함.
- 컨텍스트 벡터(context vector) : 인코더에서 생성된 입력 단어들의 정보가 압축된 벡터. 인코더 마지막 시점의 은닉상태이며 디코더의 첫번째 은닉상태로 사용됨. 

- 인코더 구조 모델 : 단어 이해에 강점을 보여, 분류, 예측, 대화, QA등의 Task에 사용됨.
- 디코더 구조 모델 : 문장 생성에 강점으로 보여, 생성 등의 Task에 사용됨.
- 인코더+디코더 구조 모델 : 인코더와 디코더가 모두 있다는 특징을 살려 Many-to-Many Task, 생성계(번역, 요약 등)Task에 사용됨.

- 교사강요 : 디코더 셀, 훈련과정에서 이전의 예측이 틀렸음에도 그걸 현재의 입력으로 사용하면 현재의 예측도 잘못될 가능성이 높기에, 
  이전 디코더셀의 출력 대신 실제값을 현 디코더셀의 입력으로 사용하는 방법. 

###### 텍스트 요약
- 텍스트 요약 : 큰 원문에서 핵심내용만 간추려 요약문으로 변환. Seq2Seq에 어텐션을 적용해 구현할 수 있음.

- 추출적 요약 : 원문에서 핵심 문장 또는 단어구를 몇 개 뽑아 이로 구성된 요약문을 만드는 방법. 모델의 언어표현능력이 제한된다는 단점이 있음.
- 텍스트랭크 : 추출적 요약의 대표적 알고리즘. 페이지랭크(검색엔진에서 웹페이지의 순위를 정하기 위해 사용되던 알고리즘)알고리즘이 기반이 됨. 
  노드는 문장, 간선의 가중치는 문장간 유사도를 의미함.

- 추상적 요약 : 원문에 없던 문장이라도 핵심문맥을 반영한 새로운 문장을 생성해 원문을 요약하는 방법. 사람이 요약하는것 같은 방법, 대표적으로 S2S. 
  지도학습이라 실제 요약문도 필요로 한다는 단점이 있음.
- 한국어 추상적 요약 : [링크](https://www.slideshare.net/BOAZbigdata/deeptitle?fbclid=IwAR2GHvEwLC-cvnlbjhvbKc0dxT7Xyof0CjooXvrcejqJoPN_cFLk6ewbMjQ)

### 트렌스포머
- 어텐션 메커니즘 : 신경망(S2S)의 성능증가를 위한 매커니즘이었고, 트랜스포머의 기반이 됨. 입력시퀀스의 길이가 길어지면 출력의 정확도가 떨어지는 현상을 보정하기 위해 등장. 
  이 외에도 텍스트분류 등에서 손실된 정보의 복구를 위해(양방향LSTM등과함께)사용되기도 함. 은닉 상태의 벡터를 모든 단어로부터(각 시점으로부터)만들어, 하나의 고정길이벡터 라는 제약을 없앰.
- 어텐션 아이디어 : 디코더에서 출력을 예측하는 매 시점마다 인코더의 전체 문장을 다시 참고, 이때 해당 시점에서 예측해야할 단어와 연관있는 부분을 더 집중해서 봄. 
  쿼리에서 모든 키 벡터에 대해 유사도를 구함. 이때 인코더의 은닉상태는 인코더의 각 시점마다 생성된 은닉상태들을 연결한 행렬이며, 여기에 가중치를 부여(어텐션 가중치)해서 전부 합해 현 시점의 context벡터를 생성함. 이를 이전시점 은닉상태와 사용.
- 어텐션 함수 : 디코더에서 동작. 주어진 쿼리(현 디코더 은닉상태)에 대해 모든 키(인코더 모든시점 은닉상태)와의 유사도를 각각 구해 키와 맵핑된 각 값(인코더 모든시점 은닉상태)에 반영하고, 
  유사도가 반영된 값을 모두 더해(어텐션값)반환함. 이때 유사도는 내적(두 벡터간 방향성에 따라 내적값이 커지기 때문)을 통해 구하며, 내적값이 음수일 수 있으니 이를 softmax를 통해 정규화함.
- 어텐션 함수 자세히 : 이전시점의 은닉상태를 softmax 함수에 통과시켜 각 입력 단어가 출력단어 예측에 얼마나 도움되는지를 구함. 이를 하나의 정보로 담아 디코더에 전송. 
  어텐션값과 현 시점의 은닉상태를 결합해 하나의 벡터로 만들고, 이를(tanh을 거치기도)예측연산의 입력으로 사용함.
- 어텐션 함수 종류 : dot(루옹, 은닉상태 전치 후 내적곱)외에도 scale dot, general, concat(바다나우, 바로이전의 은닉상태를 이용해 어텐션값을 구함), 
  location-base 등의 어텐션 스코어 함수가 있음. 
- 어텐션 스코어 : 현재 디코더의 시점에서 단어 예측을 위해, 인코더의 모든 은닉상태가 현시점의 은닉상태와 얼마나 유사한지 판단하는 스코어값. 
  각 인코더의 은닉상태와 어텐션 가중치값을 곱해 모두 더함. 컨텍스트 벡터(S2S에선 인코더의 마지막 은닉상테)라고도 불림.
- 어텐션 분포 : 모든 은닉상태의 어텐션스코어 모음값에, softmax를 적용한 스키마. 각 값을 어텐션가중치라고 함.

- 셀프 어텐션 : 쿼리, 키, 벨류가 입력문장의 모든 단어벡터들로 동일. 인코더(Encoder) 혹은 디코더에서(Masked decoder) 이뤄짐. 
  입력문장내 단어들끼리 유사도를 구해 단어의 의미(it 등이 무엇을 뜻하는지)를 찾아냄.
- 셀프 어텐션 실행 : 각 단어벡터들에 가중치행렬(단어벡터차원*(단어벡터차원/num_heads)의 크기를 지님)을 곱해 일정크기(단어벡터차원/num_heads)의 쿼리, 키, 벨류 벡터를 얻음.
- 셀프 어텐션 실행 : 이를 이용해 스코어(q*k/√(k벡터 차원), 트랜스포머는 Scaled dot-product Attention을 사용)를 구한 뒤 softmax를 지나 어텐션 분포를 구하고, 
  이를 가중합해 어텐션값을 구함.
- 셀프 어텐션 행렬연산 : 위 과정은 벡터 연산이 아닌 행렬연산을 사용하면 일괄계산이 가능해 행렬연산으로 구현됨. 헤드의 수만큼 병렬을 수행함.
- 셀프 어텐션 행렬연산 과정 : 문장행렬에 가중치행렬을 곱해 Q,K,V 행렬을 구하고, Q와 K를 내적곱(Q*K^t)하고 (q\*k/√(k벡터 차원))로 나눠 어텐션스코어를 얻으며, 
  여기에 softmax를 지나게 하고(어텐션 분포) V행렬을 곱해 어텐션값 행렬을 만들 수 있음.
 
- 트랜스포머 : S2S의 단점을 개선하면서도 인코더-디코더의 구조는 유지. RNN을 사용하지 않고(단어입력 순차적X) 셀프 어텐션을 이용해 문장을 이해. 
  인코더와 디코더가 여러개 존재할 수 있음(병렬화). 각 단어의 임베딩 벡터에서 조정된 값을 얻음. 디코더, 인코더 둘 다 임베딩,포지셔널인코딩을 거친 행렬을 입력으로 받음. 
- 트랜스포머 하이퍼 파라미터 : 입력과 출력의 크기, 인코더와 디코더의 층 수, 어텐션 사용시 사용될 분할 병렬 어텐션의 수, 트랜스포머 내부 신경망의 은닉층 크기 등의 
  하이퍼 파라미터를 사용할 수 있음. 학습률을 경과에 따라 변하도록, 인코더-디코더 뒤에 출력용 전밀집층을 추가해 설정.
- 포지셔널 인코딩 : 각 단어의 임베딩 벡터에 위치 정보를 더해 입력으로 사용, 임베딩벡터가 모인 문장벡터행렬과 포지셔널인코딩 벡터행렬 간의 덧셈을 통해, 
  같은 단어라도 위치에 따라 입력으로 들어가는 임베딩벡터의 값이 달라지게함.
- 포지셔널 인코딩 함수 : 임베딩 벡터 내 각 차원의 인덱스(문장벡터행렬의 (pos,i)에서 i)가 짝수면 sin, 홀수면 cos 함수를 사용. 
  인수는 pos/10000^(i(홀수면 i-1)/트랜스포머 출력차원) 를 사용. 

- 패딩 마스크 : <pad> 등을 마스킹(어텐션 제외 위해 값을 가림)을 함. 입력벡터중 가릴 단어의 위치에 -1e9를 넣어 곱해 그 단어가 학습에 반영되지 않게 함.
- 멀티헤드 어텐션 : d_model 차원을 num_heads로 나눠, d_model/num_heads의 차원을 가지는 Q,K,V에 대해 병렬 어텐션을 수행. 각 어텐션값 행렬을 어텐션헤드 라고 함. 
  다른시각으로 정보의 수집이 가능함. 모든 어텐션 헤드를 연결하고, 그에 또다른 가중치행렬을 곱해 최종결과물을 만들어냄.
- 포지션 와이즈 FFNN : FFNN(x(멀티헤드어텐션 결과)) = MAX(0, xW_1+b_1)W_2 + b_2. 각 벡터가 멀티헤드어텐션을 지나 FFNN을 지나가면서도 원래의 크기는 보존되고, 
  이는 한 인코더의 결과가 다음 인코더로 그대로 들어가기 떄문임.
  
- 인코더 : 레이어층의 개수(하이퍼파라미터) 만큼 인코더층을 쌓음. 인코더층은 크게 두개의 서브층(셀프어텐션, 피드포워드)으로 구성. 
  패딩마스크 > 멀티헤드어텐션/피드포워드신경망 > 드롭아웃/층정규화 같은 순서로 구성되어 있음.
- 잔차 연결 : 인코더에 추가적으로 사용되는 기법. 서브층의 입력과 출력을 더해 모델의 학습을 도움.
- 층 정규화 : 인코더에 추가적으로 사용. 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이를 이용해 값을 정규화 해 학습을 도움.

- 디코더 : 인코더 마지막층의 출력을 각 디코더층 연산에 사용. 교사강요 사용. 룩-어헤드 마스크 사용. 새개의 서브층(멀티헤드셀프 어텐션/인코더-디코더 어텐션/피드포워드)으로 구성. 
  서브층의 이후에는 드롭아웃,잔차연결,층정규화가 수행됨.
- 룩-어헤드 마스크 : RNN을 사용하지 않고 문장행렬 전체를 입력으로 받아 미래 단어까지 참고할 위험이 있어, 그를 막기 위해(가림)사용. 
  첫번쨰 서브층(멀티헤드어텐션)에서 어텐션스코어 행렬에서 마스킹을 적용해 수행됨.
- 인코더-디코더 어텐션 : 멀티헤드어텐션을 수행하기는 하나, 셀프어텐션이 아님. key와 value는 인코더의 마지막 층의 행렬로브터 얻고, query는 디코더 첫번째 서브층의 결과로 얻음. 
  다른 과정은 첫번째층과 같음. 패딩마스크를 입력으로 받음.

### FoundationModel
- Foundation model(기초 모델) : 대규모 데이터셋에서 레이블 없는 데이터로 사전훈련된 모델. GPT, Bert, Bart등이 속함. 이후 task에 맞게 파인튜닝되어 사용됨. 대부분의 언어 모델에서 SOTA를 달성하고 있음.

- feature-based approach : 특정 작업을 수행하는 네트워크에 사전훈련된 언어표현을 추가적 feature로 제공(두개의 네트워크를 붙여 사용)하는 방식. ELMo등이 대표. 또다른 사전훈련 언어표현의 적용방식.
- 전이학습(Transfer Learning) : Imagenet(큰데이터)으로 pre-train된 backbone을 이용해 featureMap을 뽑아낸 뒤 자신의 데이터셋에 맞게 fc layer만 다시 설계. 파인튜닝과 비슷함. 
  backbone - base model, Bottleneck feature - Conv layer를 거쳐서 나온 특성. Conv layer - 모델에서 데이터의 특성을 추출하는 복잡한 층, fc layer - 출력(전결합)층.
- 파인튜닝(Fine Tuning) : 기존에 학습된 모델을 기반으로 아키텍쳐를 새 목적에 맞게 변형. 파라미터를 미세하게 조정하는 행위. 기존에 학습된 레이어에 내 데이터를 추가로 학습해 파라미터를 업데이트.
- 주의사항 : lr을 크게잡으면 사전훈련된 모델의 가중치를 훼손시킬 수 있어, 보통 원래/10정도로 세팅. optimizer도 이전가중치보존/안정적학습속도의 SGD등 안정성 있는 것을 쓰는게 바람직.
  만약 새 레이어를 붙인다면, 모든 레이어는 한번 이상 학습이 완료되어야 함. 무작위 가중치가 부여된 새 레이어는 큰 가중치가 학습되어, 핵심학습내용을 잊어버릴 수 있단 위험이 있음.
  이를 위해 bottleneckFeature들로 미리 학습을 진행해 가중치를 저장한 뒤, 사전훈련된 모델에 붙여 학습을 진행하는 등의 방법이 있음.
- 방법1 : 모델의 모든 부분을 재학습. 모델의 구조만 사용. 내 데이테셋이 충분하고 선행학습된 데이터셋과 많이 다를경우 사용. 
- 방법2 : 모델의 일부 부분을 재학습. 초반레이어는 일반적인 특징을, 후반레이어는 특별한(task에 맞는)특징을 추출하는 것을 이용. 
  ConvLayer초기계층은 lr을 0으로 해 학습을 진행하지 않고, 현 task에 조금 더 맞는 특징을 유도하기 위해 Conv layer후반 계층 일부와 fc layer만 파인튜닝. 
  데이터셋이 충분하나 선행학습된 것과 유사할때, 데이터셋이 적고 선행학습된 데이터와 많이 다를 때(오버피팅의 위험이 있어 전부 학습하는것이 불가능)사용. 
- 방법3 : fc레이어만 재학습. 데이터셋이 적고, 선행학습된 것과 유사할떄 사용. 오버피팅의 가능성이 있어 Conv층을 동결시키고 bottleneckFeature만 뽑아 FC층을 학습.
  데이터 증강이 필요 없을 정도로 학습데이터셋의 크기가 클 때도 사용가능.

- 메타학습 : 메타인지(자신이 아는것과 모르는것을 즉각적으로 구별할 줄 아는것)으로부터 시작된 개념. 학습하는 방법을 학습. 적은 양의 데이터와 주어진 환경만으로도 스스로 학습하고, 학습한 정보와 알고리즘을 새로운 문제에 적용해 해결하는 학습 방식.
  일반적으로 제로샷 러닝, 원샷 러닝, 퓨샷 러닝 세가지의 접근 방식으로 나뉨. 

#### GPT
- GPT : Generative Pre-training Transfomer. OpenAI가 제작한 초거대규모(foundation)모델. 다양한 종류의 GPT가 있으며 현재는 GPT3까지 존재하고, 2022년 7~8월 경에 GPT4가 출시될 것 이라고 함.
  하나의 언어를 입력하면 이를 사용자에게 가장 유용한 다음언어로 변환하도록 설계된 언어 예측 모델. 방대한 양의 텍스트를 비지도학습(의미분석)을 사용해 사전학습함.
- 구조 : 트랜스포머 기반. 트랜스포머의 디코더 구조를 사용하며, 조건부 언어모델을 핵심으로 하고있음. BPE를 도입함. 인코더 부분이 **없으며** Text+Position 임베딩만이 있을 뿐임.
- BPE(바이트 페어 인코딩) : 자주 함께 사용되는 char 를 하나의 묶음으로 사용(최소한의 단어). 워드 임베딩과 캐릭터 임베딩의 장점을 모두 가지고 있음. (word)단어간의 유사도와 (char)처음보는 문자의 예측 모두가 가능함.
- 사용 Task : 문장 생성능력이 중요한 Task에 사용됨. GPT3의 경우는 각종 언어관련 문제풀이, 랜덤 글짓기, 간단한 사칙연산, 번역, 주어진 문장에 따른 간단 웹코딩 등이 있음.
##### GPT 시리즈
- GPT-1 : 문장간 관계 유추, 질의 응답, 문장유사도, 분류등에 뛰어난 성능을 보임. 
  언어 모델로 학습, 파인 튜닝(linear와 softmax, 추가적인 레이어 없이 적은 양의 레이블 만으로도 가능)두 단계를 거침. 트랜스포머의 디코더기반 모델. 
- GPT-2 : 1과 달리 파인튜닝을 없애고, 사이즈가 커짐. 입력된 값과 수행해야할 task(다음단어, 번역, 응답 등)를 함께 입력받아 다음 단어를 출력.
- GPT-3 : 파인튜닝 제거(가능하긴 하나 필요 X)를 핵심으로 하고 있음. 제로샷(한번도 볼 필요 없음)이나 원샷(한장만 보면 됨)이 아닌 퓨샷러닝(몇장의 이미지만 사용)만을 이용. 
  여러 분야에서 뛰어난 성능을 보이나 단방향으로 학습해 문맥 파악에 약하다는 단점을 가짐. 약 175B의 파라미터를 가지고 있음. 이 이전까지는 마이크로소프트의 튜링NLG가 가장 큰 모델(17B)였으며, 이 이후 MS는 경쟁을 포기하고 독점적사용권을 얻었음.
##### GPT 변형
- GPT(original) : Book Corpus데이터셋에서 사전훈련된 트랜스포머 아키텍쳐를 기반으로 하는 자기회귀모델.
- CTRL : GPT와 동일하나 제어코드의 개념을 추가함. 텍스트는 프롬프트와 텍스트생성에 영향을 미치는데 사용되는 제어코드중 하나(이상)에서 생성.
- Transformer-XL : 일반 GPT와 동일하나 두개의 연속 세그먼트에 대한 반복 매커니즘을 도입함. 이전 세그먼트의 은닉상태를 현 입력에 연결해 어텐션 점수를 계산한 뒤,
  위치임베딩을 위치상대임베딩으로 변경하고 어텐션스코어의 계산방식을 약간 바꿈.
- REformer : 메모리 풋프린트와 계산시간을 줄이기 위한 많은 트릭이 있는 자동회귀 트랜스포머 모델.
  축 위치 인코딩, LSH어텐션, backward pass동안 리버시블 트랜스포머층을 사용해 각 레이어의 중간결과를 저장/재계산함, 전체배치 > 청크 의 트릭을 사용함.
- XLNet : 자기회귀모델 기반 훈련. 모델이 마지막 n개의 토큰을 사용해 n+1의 토큰을 예측하게 하며, 이 과정을 모두 마스크에서 진행함.  
#### BERT
- BERT : Bidirectional Encoder Representations from Transformer. 2018년 구글이 공개한 언어표현모델. 사전학습을 위해 제작. 한국어용 BERT 패키지 KoBERT가 존재함.
- 구조 : 트랜스포머의 인코더 부분만을 사용. 포지셔널 인코딩 대신 포지션 임베딩과 segment임베딩을 추가해, 총 세가지 임베딩의 합산 결과를 취함. 이후 N개의 인코더블록(멀티헤드어텐션-PositionWiseFFLayer)을 RNN처럼 재귀적으로 반복처리함.
- 특징 : wiki나 book data등의 대용량 unlabeled data로 모델을 학습시킨 뒤, 특정 task의 labeled data로 transfer Learning해 하위 NLP테스크에 적용하는 Semi-Supervised Learning 모델. 
  BERT이전의 비슷한 모델들(ELMo, OpenAI GPT등)과 달리 bidirectional함. 특정 task의 처리를 위해 새 네트워크를 붙일 필요 없이, 모델자체의 fine-tuning을 통해 state-of-the-art달성.
- 학습 : 이후 단어의 예측을 위해 Bidirectional하게 학습할 수 없었던 이전 모델의 단점을 해결하기 위해 다른 형태(MLM, NSP)의 문제로 전환해 학습.
- 사용 Task : Conversation, Span(Slot)Prediction, Zero-shot Learning, QA, MT, NER, 분류, 형태소 분석, 의미적 유사도, 그 외 언어 이해능력이 중요한 Task에서 사용됨.
#### BART
- BART : Bidirectional and Auto-Regressive Transformers. 2020년 페이스북이 발표. 넓은 분야에 적용할 수 있도록 Seq2Seq구조로 만들어진 denoising auto-encoder.
- 구조 : Seq2Seq 트랜스포머 구조를 사용, GeLU 활성화 함수 사용. 
- 학습 : 손상된 text를 복구하도록 학습하며, 디코더의 출력과 원본텍스트의 loss를 줄이도록 함. 다른 오토인코더모델과는 다르게 모든 종류의 노이즈를 적용할 수 있음.
  BERT와 GPT의 특성을 모두 적용해, 손상된 text를 bidirectional모델(BERT)로 인코딩하고, 정답에 대한 likehood를 autoregressive(GPT)디코더로 계산함.
- 사용 Task : generation task(생성, 번역 등. 특히 요약)에 주로 사용. 분류 Task에서도 BERT와 비슷한 성능을 낼 수 있음.

## Tasks
### Embedding
- Word Embeddings : 텍스트 내 단어를 밀집 벡터로 만드는 과정. 단어의 유사도(의미)정보도 벡터화 가능. 유사도에 따라 단어가 유사한 값을 띄게 됨. 
  정수 인코딩(원핫인코딩)된 벡터를 입력으로 받음.
- 임베딩 벡터 : 사용자가 설정한 임베딩벡터 사이즈(차원)에 따라 임베딩 벡터가 생성됨. 임베딩 벡터 안에는 각 단어의 정보를 가지고 있는 (임베딩 사이즈)개의 실수가 들어 있음.  
- 밀집벡터 : 희소벡터(대부분의 값이 0인 벡터)와 달리 대부분의 값이 실수이고 상대적으로 저차원인 벡터. 사용자가 설정한 값으로 벡터의 차원을 맞춤. 
  특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로브터 임베딩 벡터값을 가져오는 룩업테이블.
- 분산표현 : 분포 가설(비슷한 위치에서 등장하는 단어는 비슷한 의미를 가짐)에 기초해 만들어진 표현 방법. 단어의 의미를 여러 차원에 분산하여 표현. 단어간 유사도 계산 가능.

- 임베딩 층 : 정수 인코딩된 단어들을 입력으로 받아 밀집벡터(임베딩벡터)로 변환. 입력 정수에 대해 밀집벡터로 맵핑하고, 이 벡터는 가중치가 학습되는 것과 같은 방식으로 훈련됨. 
  원-핫 벡터를 입력으로 받으면 룩업테이블과 내적곱을 해 임베딩 벡터를 가져오고, 정수인코딩된 단어를 입력으로 받으면 인덱스로 가져옴.

- 워드 임베딩 평균 : 임베딩층 뒤에 GlobalAveragePooling1D()를 사용해 할 수 있음. 기타 은닉층의 사용 없어도 높은 정확도의 분류가 가능함.
#### char embedding (글자 임베딩)
- 글자 임베딩 : 워드 임베딩과는 다르게 단어의 벡터 표현을 얻어 OOV문제를 해결함. 모르는 단어도 뜻을 추측(이해)해서 예측함. 워드 임베딩의 대체로도 사용할 수 있지만, 
  워드 임베딩과 연결해 신경망의 입력으로 사용하기도 함. 글자단위의 정수 인코딩이 필요.
  
- 1D CNN 이용 : FastText가 글자의 N-gram조함을 이용하듯이, 전체입력 내부의 더 작은 시퀀스로부터(N-gram) 정보를 얻음.
- 1D CNN 방법 : 단어를 글자단위로 분리, 글자에 대해 임베딩(이후 패딩 가능), 그 후 1D CNN을 적용. 나온 벡터를 풀링층을 거쳐 스칼라로 만들고, 이를 연결해 하나의 벡터로 만듦. 
  이 벡터가 단어의 벡터.
  
- BiLSTM 이용 : 단어를 글자로 쪼갠 뒤 임베딩, 정방향/역방향 LSTM을 사용, 둘의 마지막 은닉 상태를 연결해 벡터화, 이를 단어의 벡터로 사용.
#### sentence Embedding(문장임베딩) 
- 문장임베딩 : 다수의 문장간 비교를 위해 각 문장을 고정된 길이의 벡터로 변환하는 것. 여러 방법이 있으나, 간단하게는 문장의 단어벡터평균을 구하는 방법이 있음.
- 과정 : 모든 문장에 대해 문장벡터를 만든 뒤, 문장벡터간 코사인 유사도 행렬을 만들고, 이를 페이지랭크 알고리즘의 입력으로 사용해 각 문장의 점수를 구한 뒤, 
  점수가 가장 높은 문장을 상위n개 선택해 문서의 요약문으로 함. 

#### Word2Vec
- word to vector : 워드 임베딩을 하는 대표적 방법. 은닉층에 활성화 함수가 존재하지 않으며, 룩업테이블(투사층)이 그 역할을 대신함. 
  일반적으로 SGNS(Skip-Gram with Negative Sampling)을 사용. 동음이의어를 잘 반영하지 못한다는 문제점이 있음. 추천시스템에도 사용.
- W2V 자세히 : 투사층(임베딩 벡터의 차원(M)), 입력층과 투사층 사이(단어집합크기(V)*M), 투사층과 출력층 사이(M\*V) 의 크기를 가짐. 
  윈도우 크기 내에서만 주변 단어를 고려해 전체적 통계 정보를 반영하지 못한다는 문제점을 가짐. 
- VS NNLM : NNLM의 느린 속도와 정확도를 개선해 탄생. 다음단어가 아닌 중심 단어를 예측, 이전단어만 참고하는게 아닌 전후 단어 모두 참고, 활성화 함수와 은닉층 제거, 
  계층적 softmax와 네거티브 샘플링 등을 이용해 은닉층 제거 이외에도 속도를 개선.   

- 네거티브 샘플링(Negative Sampling) : W2V가 학습과정에서 전체 단어가 아닌 일부 단어에만 집중할 수 있도록 하는 방법. 
  무작위로 주변단어가 아닌 단어를 선택하고, 그것들을 기준으로 단어 집합을 만들어 긍정과 부정의 이진 분류 문제로 만듦.
- 자세히 : 일반 W2V(Skip-Gram)에는 속도(마지막 단계에서 softmax를 적용하고 오차를 구하며 임베딩 벡터를 조정하는 작업을 관계없는 단어에도 적용)문제를 가짐.
  해결(일부집합에 대해서만 고려)을 위해, 주변 단어를 가져와 집합을 만들고, 임의로 주변단어가 아닌 단어를 가져와 집합을 만든 뒤 마지막 단계를 이진분류 문제로 바꿈.
  이는 다중클래스 분류 문제를 이진분류문제로 바꾸면서도 훨씬 효율적인 연산량을 가짐.

- CBOW(Continuous Bag of Words) : word2vec의 두가지 방법중 하나. 주변 단어를 이용해 중간의 단어를 예측. 
  윈도우(앞뒤로 몇개의 단어를 볼 지)를 계속 움직여 주변과 중심 단어 선택을 바꿔가며 데이터 셋을 만드는(슬라이딩 윈도우)것이 가능. 
- CBOW 자세히 : 두개의 가중치 행렬(입력-투사, 투사-출력)을 학습해 나감. 입력된 원 핫 벡터(윈도우 안의 단어들)들을 가중치 행렬과 곱(i번째 행을 가져옴)해 그 결과 벡터들의 평균을 구함.
  그 평균 벡터는 두번째 가중치 행렬과 곱해져 원-핫 벡터와 차원이 동일한 벡터를 생성하고, 여기에 softmax와 손실함수로 cross-entropy를 사용해 
  각 단어가 중간 단어일 확률을 가진 스코어 벡터를 생성.
- Skip-gram : word2vec의 또다른 방법. 중심 단어에서 주변 단어(윈도우 안의 단어들)를 예측. CBOW와 유사한 과정을 통해 구함. CBOW보다 성능이 좋다고 알려져 있음.
- SGNG(Skip-Gram with Negative Sampling) : 네거티브 샘플링을 사용하는 Skip-Gram. 중심단어와 주변단어 둘 다 입력이 되고, 두 단어가 실제 윈도우 내에 존재하는 이웃일 확률을 계산. 

- 사전훈련된 W2V(GloVe)코드 : [urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", 
  filename="GoogleNews-vectors-negative300.bin.gz")
  word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True))]

- LSA(잠재 의미 분석) : 각 단어의 빈도수를 카운트한 행렬(DTM)을 입력으로 받아 차원을 축소(Truncated SVD)해 잠재된 의미를끌어내는 방법론. 토픽모델링 기법이자 워드 임베딩 방법.
- 단점 : 카운트 기반으로 코퍼스의 전체적인 통계정보를 고려하기는 하지만, 단어의 의미 유치 작업에서 성능이 떨어짐.  

#### GloVe
- GloVe : 또 다른 워드 임베딩 방법. 카운트 기반과 예측 기반 모두 사용하는 방법론. 카운트 기반의 LSA(잠재 의미 분석)과 W2V의 단점을 지적, 
  보완한다는 목적으로 나와 W2V와 비슷한 성능을 보여줌.  
- GloVe 아이디어 : 임베딩된 중심 단어와 주변 단어 벡터의 내적곱이 전체 데이터에서의 동시 등장 확률(log(P(i,j)))이 되도록 만드는 것. 
- 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix) : 행과 열을 전체 단어의 집합으로 구성한 뒤, i단어의 윈도우 안에서 j단어가 등장한 횟수를 (i,j)에 나타낸 행렬. 
  전치해도 동일한 행렬.
- 동시 등장 확률(Co-occurrence Probability) : 동시 등장 행렬에서 특정 단어(i)의 전체 등장 횟수와 특정 단어(i)가 등장했을때 특정 단어(j)가 
  등장한 횟수를 카운트해 계산((i,j)/sum(i))한 조건부 확률. 

#### FastText
- FastText : 페이스북 개발 워드 임베딩 방법. 메커니즘은 W2V와 비슷하지만, 하나의 단어 안에도 여러 단어들이 존재한다고 생각(내부단어 고려)함. 
  W2V, LSA와 달리 OOV와 RaerWord에 대한 대응이 가능함.
- 내부단어 고려 : 각 단어를 글자단위 n-gram의 구성으로 취급. n을 몇으로 하냐에 따라 단어들이 얼마나 분리될지 결정. 
- 토큰 분리 : 시작과 끝을 의미하는 <, > 에 기존 단어에 <>를 추가한 토큰까지 총 (글자수 - n + 4)개의 토큰이 나옴. 
  이때 n의 범위를 지정할 수 있는데, 이 경우 범위 안의 정수가 모두, 차례대로 n으로 사용됨.

- OOV(Out Of Vocabulary)대응 : 데이터 셋 내 모든 단어의 n-gram에 대해 워드 임베딩이 되어, 데이터셋만 충분하면 모르는 단어(OOV)에 대해서도 다른 단어와 유사도 계산이 가능. 
- RareWord(빈도수가 적은 단어)대응 : 단어가 희귀단어라도 그 단어의 n-gram이 다른 단어와 겹친다면 비교적 높은 임베딩 벡터값(유사도)을 얻음. 
  노이즈가 많은 코퍼스에서도 같은 이유로 강점을 가짐. 

#### ELMo
- ELMo(Embeddings from Language Model) : 새로운 워드 임베딩 방법. 사전 훈련된 언어 모델 사용. 같은 표기의 단어라도 문맥을 반영해 임베딩 할 수 있음.
- biLM(Bidirectional Language Model) : 양쪽 방향 RNN을 모두 사용하는 언어 모델. 단어단위로 입력을 받음. 다층구조. 다음 층으로 보내기 전이 아니라 훈련 후 표현을 위해 은닉상태를 연결. 
- biLM 이용 임베딩 : 각 층을 연결 > 출력별로 가중치 > 출력값을 전부 더함 > 벡터의 크기(스칼라)를 곱함. 이렇게 완성된 벡터를 ELMo표현이라고 하고, 
  이를 기존 임베딩 벡터와 함께(기존 벡터와 연결, 사전 흔련된 언어 모델 가중치는 고정)사용할 수 있음.

### Translate
- 기계 번역 : 기본적으로 S2S 네트워크를 이용. 인코더는 입력 시퀀스를 벡터로 압출하고, 디코더는 해당 벡터를 새로운 시퀀스로 펼치게 됨. 모델의 개선을 위해 어텐션 매커니즘을 사용함.
- 전처리 : 각 문자열들을 정제/정규화 한 뒤(필요에 따라 각 텍스트를 문장으로 분할하거나, 길이 및 내용으로 필터링), vocab을 생성해 정수인코딩을 진행함(임베딩의 입력이 됨).
- 인코더 : 모든 입력 단어에 대해 벡터와 은닉상태를 출력, 다음 입력단어에 대해 은닉상태를 사용함. 입력단어는 임베딩 벡터를 지나고, 이전은닉상태와 함께 RNN(GRU등)을 지나 output과 hidden을 반환함.
- 단순 디코더 : 인코더의 마지막 출력(context vector)만을 사용. 이것을 디코더의 초기 은닉상태로 사용하며, 주어진 입력토큰(지난 output)과 은닉상태(지난 hidden)을 사용해 RNN을 지나 output과 hidden을 반환함.
- 어텐션 디코더 : 컨텍스트벡터만 전달할 경우, 전체 문장을 인코딩하는 부담을 가지므로 어텐션을 사용, 모든 단계에 대해 인코더출력의 다른부분에 집중. 입력토큰과 지난 은닉상태, 컨텍스트 벡터를 어텐션 한 벡터와 지난 은닉상태를 사용해 output과 hidden을 반환함.
- 훈련 : TeacherForcing은 디코더의 추측을 다음 입력으로 사용하는 대신 실제 목표 출력을 사용. 네트워크가 더 빨리 수렴되지만, 정확한 번역에서 멀어지거나 처음 번역에서 문장을 만드는 방법을 제대로 배우지 못할 수 있어, 랜덤으로 교사강요를 사용하거나 하지 않도록 할 수 있음.
  전체 교육 과정은 타이머 시작 -> 옵티마이저/metrics초기화 -> 훈련 데이터 생성 -> 훈련 스텝. 중간중간 progress bar를 출력하고, plot을 위한 데이터를 저장해 줌. 어텐션은 입력 시퀀스의 특정 인코더 출력에 가중치를 부여하는데 사용되므로, 각 스텝에서 가장 집중하는 위치를 볼 수 있어 학습 진행 확인에 유용함.



#
******


# sklearn | BOW, TFID, LDA 등
- sklearn.feature_extraction.text.CountVectorizer() : BOW 표현을 하게 해주는 변환기 로드. .fit(문자열이 담긴 리스트)로 사용, 
  .vocabulary_ 속성에서 반환된 {단어:등장횟수} 형태의 딕셔너리를 볼 수 있음.  tf-idf 와 함께 ngram_range=(연속 토큰 최소길이, 최대길이) 로 연속된 토큰을 고려할 수 있다. 
  보통은 하나만 하지만 많을 때 바이그램정도로 추가하면 도움이 된다.  
- Bow 표현을 만드려면 .transform(list), Scipy 희소 행렬(원소 대부분 0) 저장되어 있으며, .get_feature_names()로 각 특성에 해당하는 단어들을 볼 수 있음. 
  min_df 매개변수로 토큰이 나타날 최소 문서 개수를 지정할 수 있고, max_df 매개변수로 자주 나타나는 단어를 제거할 수 있다. 
  stop_words 매개변수에 "english" 를 넣으면 내장된 불용어를 사용한다.
- sklearn.preprocessing.LabelEncoder() : 여러개의 카테고리가 존재하는 데이터를 고유한 정수로 인코딩하는 클래스 로드.  

- sklearn.feature_extraction.text.TfidVectorizer(min_df=i) : 텍스트 데이터를 입력받아 BOW 특성 추출과 tf-idf 를 실행하고 L2정규화(스케일 조정)까지 적용하는 모델로드. 
  훈련데이터의 통걔적 속성을 사용하므로 파이프 라인을 이용한 그리드 서치를 해 주어야 한다. .idf_ 에서 훈련세트의 idf 값을 볼 수 있다. 
  idf 값이 낮으면 자주 나타나 덜 중요하다 생각되는 것이다.
- sklearn.decomposition.LatentDirichletAllocation(n_components=n, learn_method="online", random_state=k, max_iter=i) : 
  LDA 수행. .fit_transform(X), .components_ 등을 사용할 수 있다.

# transformers | 트랜스포머기반 모델
- transformers : 허깅페이스(HuggingFace)가 제작한, 트랜스포머 기반의 다양한 모델(transformer.models)과 학습 스크립트(transformer.Trainer)를 구현해 놓은 모듈.
  다양한 트랜스포머 기반 모델구현체를 손쉽게 쓸 수 있으나 high-level로 구현되어있어 커스터마이징이 비교적 어려움(소스코드를 참고해 원하는 클래스를 상속받아 오버라이딩 해야함).
  각 모델마다(GPT, Bert, Bart) task별, 모델의 변형별 모델이 있어, 로드한 모델과 사용 task에 맞게 잘 선택해야 한다. torch모델의 경우, 기존의 torch모델과 달리 x, y 둘다 LongTensor여야 함.
- 기본 제공 task : 감정분석(긍정/부정), 텍스트 생성(영어, 다음문장생성), NER(각 단어를 나타내는 엔티티로 레이블지정), 
  QA(컨텍스트+질문 -> 답변), 마스킹된 텍스트 채우기(공백([MASK\])이 있는 텍스트의 공백을 채움), 요약, 번역, 특징추출(텍스트의 텐서 표현 반환).
- from_pretrained() : 토크나이저/모델/설정.from_pretrained(모델명)으로 사전 훈련된 모델을 사용할 수 있음. Auto계열로도 사용가능.
- config : 모델config 에 저장되어있는 모델설정 라이브러리로 모델의 설정을 가져올 수 있음. .id2lavel로 모델의 레이블 리스트를 가져오는 등 사용가능. 

### pipeline
- pipeline : 주어진 task에서 사전훈련 모델을 사용하는 가장 쉬운 방법. 기본제공 task에 속한다면 사용가능. pipline()이외에 작업별 파이프라인이 task당 하나씩/task에 없는것도 존재함.
- transformers.pipeline(task명) : 사전훈련된 모델과 해당하는 토크나이저를 다운로드. 명령결과(sent)로 간단하게 사용가능.
  model인자와 tokenizer인자를 사용해 직접 사용할 모델과 토크나이저를 전달해 줄 수 도 있음.
- pipline task종류 : sentiment-analysis, question-answering, fill-mask, text-generation, text2text-generation, ner, summarization, translation(_xx_to_yy or 모델 필요),
  conversational, zero-shot-classification, audio-classification, image-classification, image-segmentation, object-detection, table-question-answering(torch).

- 파이프라인 커스텀 : Pipeline을 상속하는 클래스를 제작 후 preprocess, _forward, postprocess, _sanitize_parameters를 정의함.
- preprocess : inputs를 입력으로 받아 전처리(입력을 모델에 제공할 수 있는걸로 변환) 후 {"model_input" : model_input}형태로 반환함.
- _forward : 구현 세부정보. model_inputs(preprocess의 반환값)를 입력으로 받아  `oututs = self.model(**model_inputs)`형태로 출력을 받아 반환함.
- postprocess : _forward의 출력을 받아 후처리(최종출력으로 변경)함. 분류모델기준 `best_class = model_outputs["logits"].softmax(-1)`의 형태.
- _sanitize_parameters : 원하는 시간(생성, 초기화, 호출등)에 매개변수를 전달. **kwargs를 입력으로 받아, 각 메서드들에 들어갈 매개변수를 dict형식으로 반환. 따로 매개변수 설정이 없다면 {}반환.
  `if "args" in kwargs: preprocess_kwargs["args"] = kwargs["args"] > return preprocess_kwargs, {}, {}`의 형식. 만약 postprecess등에도 매개변수가 있다면 따로 dict를 반환. 

### tokenizer
- TokenizerFast : Rust라이브러리를 기반으로 함. 일괄 토큰화시 상당한 속도 향상이 있고, vocab과 토큰간 매핑방법이 다름(일반 dict > 인덱스를 얻음). 일부 토크나이저는 미지원.
  T5, ALBERT, CamemBERT, XLMRoBERTa, XLNet을 제외한 토크나이저는 Fast를 사용할 수 있음.
- transfomers.AutoTokenizer.from_pretrained(모델명) : 사전훈련된(미리 다운로드 받은)모델과 관련된 토크나이저를 자동선택해 다운로드. 모델은 허깅페이스 홈페이지에서 확인가능.

##### 토크나이저 사용
- tokenizer(sequence) : 문장을 기준에 맞춰 토큰화. input_ids키에 정수인코딩까지 완료된 문장(텐서)이 들어있고, token_type_ids(segment), attention_mask등을 같이 반환하기도 함.
  [sent1, sent2\]처럼 넣어 여러 문장을 토큰화할 수 도 있고, 두개의 시퀀스를 두개의 인수로 넣으면 두 시퀀스를 인코딩과 동시에 합침(BERT에 맞게, 리스트로 넣었으면 같은 인덱스의 문장들끼리 이어짐). 
- tokenizer 인자 : return_tensors=str(반환될 텐서 종류. pt/tf), padding=bool/str(True-최대길이로 패딩, 'max_length'-max_length인자의 길이/모델이 허용하는 최대길이로 패딩, 
  False-패딩X(default)), truncation=bool/str(True-모델이 허용하는 최대길이로 자름, 'only_second/first'-앞과 동일하나 한쌍의 시퀀스(배치)가 주어지면 두번째/첫번째만 자름, False-자름X),
  max_length=int(패딩/잘림 길이 제어. truncation의 경우 False가 아니라면 모두 여기서 지정한 길이로 자름), is_split_into_words=bool(단어가 이미 나뉘어있는지 여부, 리스트속 문자열==한 문장),
  return_tensors를 생략하면 리스트로 반환됨.

- tokenizer.encode(sent) : 주어진 시퀀스를 인코딩. 원시 텍스트 시퀀스도, 이미 토큰화된 시퀀스도 처리가능(is_pretokenized=True). __call\_\_()의 ["input_ids"\]와 동일함.  
- tokenizer.encode_batch(sent) : 주어진 시퀀스배치를 인코딩. 각 배치(문장)는 리스트 형태롤 존재하며, 문장내에 리스트, 튜플 형태로 존재할 수 있음.
- tokenizer.decode(encoded_sequence) : 디코딩. 인코딩된 시퀀스를 원래의 문장으로 되돌림. skip_special_tokens=False면 특별토큰은 그대로 유지됨.
- tokenizer.decode_batch(encoded_sequence) : ID배치를 디코딩. 디코딩하려는 시퀀스배치를 입력으로 넣어줌. 
- encode 인자 : add_special_tokens = bool(문장의 시작과 끝에 [cls\], [sep\]토큰 추가), max_length = i(문장의 최대 길이), 
  pad_to_max_length = bool(패딩), return_attention_mask = bool(어텐션마스크 반환), return_tensors = str(반환될 텐서 종류. pt/tf).

- tokenizer.prepare_for_tokenization(text, is_split_into_words=bool, **kwargs) : 토큰화 전에 필요한 변환을 수행함.
- tokenizer.tokenize(text) : 토크나이저를 사용해 토큰시퀀스의 문자열을 반환함.

- tokenizer.add_tokens(토큰) : 토큰 추가. 이미 존재하지 않는 경우에만 추가됨.
- tokenizer.get_[added_\]vocab() : 인덱싱할 토큰사전(vocab)반환.

- tokenizer.convert_ids_to_tokens(ids, skip_sepcial_tokens = bool) : 정수/정수배열을 토큰으로 변환함. 특별토큰을 생략할 수 있음. 
- tokenizer.convert_tokens_to_ids(token) : 토큰(들)을 정수로 변환함. 

- tokenizerFast.backend_tokenizer : 백엔드로 사용되는 Rust 토크나이저 반환.
- tokenzierFast.set_truncation_and_padding(padding_strategy, truncation_strategy, max_length, stride) : Fast토크나이저에서 사용할 자르기/패딩을 정의. 

- tokenizer.save_pretrained(path) : 토크나이저 저장. 저장한 토크나이저는 from_pretrained로 사용가능.

### model
- transfomers.models : 트랜스포머 기반의 다양한 모델을 파이토치/텐서플로우로 각각 구현해놓은 모듈. 각 모델에 맞는 토크나이저도 구현되어있음.

- transformers.(TF)AutoModel.from_pretrained(모델명) : 사전훈련된 모델과 관련된 모델 사용. .vocab[key\]로 단어가 있는지, 어떤 정수인지 확인 가능.
- transformers.(TF)AutoModelForSequenceClassification.from_pretrained(모델명, from_pt=bool) : 사전훈련된 모델을 기반으로 시퀀스 분류기 로드.
- transformers.(TF)DistilBertForSequenceClassification.from_pretrained(모델명, from_pt=bool) : 더 작고/빠르고/저렴하고/가벼운 BERT의 증류된버전 DistilBert 시퀀스분류기 사용.
- from_pretrained로 모델을 가져올때, num_labels등의 인자를 이용해 간단한 모델변경이 가능하나, 이 경우 사전훈련헤드를 버리고 랜덤초기화인 분류헤드로 바꿔, 이전의 지식을 모델에 전달해야 함(전이학습).

- transformers.DistilBertConfig(설정들) : 모델의 설정정의. 모델(config)로 구조는 동일한 채 파라미터만 약간 다른 사용자정의 모델을 사용할 수 있음.


##### collator
- collator : 허깅페이스의 토크나이저. 트레이너의 인자로 들어가 데이터셋을 가져오는 동시에 처리함. 처리 전 토크나이즈를 해줘도 됨. 배치 내 가장 긴 길이로 패딩(동적패딩)할 수 있음.
- DataCollatorWithPadding(tokenizer=tokenizer) : 동적 패딩을 하는 데이터 collator를 생성.

##### 모델 사용
- model(encoded_input(인코딩된 문장들)) : 모델 사용. pytorch의 경우 사전압축을 풀어야(**)함. labels인자로 label을 전달해 줄 수 있으며, 이 경우 반환에 loss가 생성됨.
  .logit으로 반환값을 볼 수 있으며, 예측을 위한 softmax는 물론, 다른 trainloop에서도 사용가능함.
- model.generate(input_ids) : 모델사용. 파인튜닝 이전의 모델만 사용하려 하거나 테스트에 유용함. 
- model 인자 : output_hidden_states=bool(모든 은닉상태 반환), output_attentions=bool(모든 어텐션가중치 반환)

- model.save_pretrained(경로) : 훈련된(미세조정된)모델(파라미터)저장. 저장한 모델은 from_pretrained로 사용가능히며, from_pt/tf로 어디서 생성된 모델인지 알려줘야 함.

### Trainer
- transformers.Trainer : 딥러닝 학습/평가에 필요한 optimizer, lr schedul, tensorboard, 평가등을 수행하는 모듈. 그냥 torch나 tf의 훈련방식대로 훈련할 수 도 있으나, 조금 더 편함. 

- transformers.TrainingArgments(output_dir) : Trainer의 정의를 위한 TrainingArgument객체를 생성. 조정가능한 모든 하이퍼파라미터, 지원하는 훈련옵션을 실행하기 위한 플래그가 속해있음.
  트레이너에 compute_metrics를 넣은 뒤 evaluation_strategy="epoch"로 epoch마다 정확도를 보고하게 할 수 있으며, 이 외에도 learning_rate, num_trian_epochs등 다양한 옵션이 있음.
- metrics : 트레이너가 metrics를 계산하고 report를 하게 하기 위해서는 예측과 labels를 가지고 {metric명(str): metric(float)}을 반환하는 compute_metric함수를 주어야 함.
- trianer.evaluate() : 인자로 넣은 compute_metrics를 이용해 정확도 출력.

- transformers.Trainer(model=model, args=traininig_args, trian_dataset=데이터셋, eval_dataset=데이터셋, compute_metrics=정확도 계산함수) : 트레이너 생성. 

- trainer.train() : 파인튜닝. 학습/평가의 모든 과정이 사용자가 원하는 인자에 맞게 실행됨. pytorch lightning과 비슷하게 공통적으로 사용되는 학습스크립트가 모듈화 되어있음. GPU필요.


# Tokenizers | 토크나이저들
- Tokenizers : 허깅페이스가 제작한 토크나이저 라이브러리. Rust로 구현되어있어 매우 빠름. 

### 토크나이저 사용
- output = tokenizer.encode(sent) : sent를 토큰화. 문장쌍을 토큰화 하고 싶다면 sent, sent형식으로 넣으면 됨.
- output = tokenizer.encode_batch(sents) : 문장들을 토큰화. sents는 리스트 형식이며, 문장 쌍의 경우는 [[sent, sent\]\]형식으로 넣으면 됨. 라이브러리의 최대속도를 얻을 수 있음.
- output.tokens : 토큰들을 확인.
- output.ids : 정수화된 토큰들을 확인.
- output.type_ids : type_ids를 확인.
- output.attention_mask : attention_mask를 확인.
- output.offsets[id(토큰)인덱스\] : 원래 문장에서 해당 토큰(id)의 위치를 반환. (시작, 끝)형태의 튜플로 반환함.
##### decode
- tokenizer.decode(ids) : id들을 다시 텍스트로 변환.
- tokenizer.decode_batch(ids) : id batch를 다시 텍스트로 변환.

- WordPiece/metaspace등 단어의 하위토큰을 나타내기 위해 특수문자를 추가한 모델의 경우는 티코더를 사용자정의해야 함. 
- tokenizers.decoders.WordPiece() : WordPiece모델을 디코딩. WordPiece모델은 단어의 하위토큰을 식별하는 ##를 사용해, 해당 하위단어를 디코딩하는데 도움.
- tokenizers.decoders.Metaspace() : Metaspace PreTokenizer를 디코딩. 특수 식별자 _를 사용 하여 공백을 식별해, 공백을 디코딩하는데 도움.
- tokenizers.decoders.ByteLevel() : ByteLevel PreTokenizer를 디코딩. 각 바이트를 나타내는 보이는 유니코드 문자를 사용해 바이트 수준에서 인코딩해, 되돌리는데 도움.
##### custom
- 토크나이저 사전훈련시 추 후 사용을 위해 훈련된 <unused\>토큰을 바꿔준 뒤 저장하는 방식으로 커스텀 토큰 사용 가능. "additional_special_tokens": ["[NAME]", "[RELIGION]"]식.
  따로 디렉토리를 만들어 vocab.txt, tokenizer_config.json, special_tokens_map.json를 관리, vocab.txt에서 unused를 원하는 토큰으로 바꾸고 특별토큰이면 special에 넣어줌.
- tokenizer.save_pretrained('토크나이저이름', legacy_format=False) : 불러온 토크나이저를 저장. legacy_format=True로 저장하면 버전이 0.10.1 전 형식으로 저장돼 오류발생가능.

### 토크나이저 빌드
- tokenizers.Tokenizer(모델) : 토크나이저 생성.
- tokenziers.Tokenizer.from_file(path) : 토크나이저 로드.
##### 토크나이저 설정
- tokenizer.pre_tokenzier = 사전토크나이저 : 토크나이저의 토큰화 기준(pre토크나이저)지정.
- tokenizer.normalizer = 노멀라이저 : 토크나이저의 노멀라이저 사용자정의.
- tokenizer.post_processor = 사후토크나이저 : 자동 특별 토큰 추가를 위한 후처리 사용. 토큰화 후 자동으로 지정한 형식이 됨.
- tokenizer.save(path) : 토크나이저 저장. json파일로 저장해야 함.
- tokenizer.token_to_id(토큰) : 토큰의 id를 반환.
- tokenizer.enable_padding(pad_id, pad_token) : 여러 문장 인코딩시 가장 긴것에 맞게 패딩. direction(기본 우측)/length등의 인자 사용가능.
##### 토크나이저 훈련
- tokenizer.train_from_iterator(data, trainer) : 이터레이터의 데이터를 기반으로 토크나이저 훈련. length 매개변수에 길이를 넣어 표시줄의 모양을 개선할 수 있음.
- tokenizer.train(path, trainer) : txt 파일을 기반으로 토크나이저 훈련. path는 리스트형태로, 여러 파일을 훈련하고 싶다면 파일들의 경로를 넣으면 됨.
###### 트레이너
- 토크나이저 인자 : special_tokens=[토큰리스트\](특별토큰들의 지정), vocab_size(vocab size)등의 사용이 가능함.
- tokenizers.BpeTrainer() : BPE토크나이저를 위한 트레이너 생성. 
- tokenizers.WordPieceTrainer() : WordPiece토크나이저를 위한 트레이너 생성. 
- tokenizers.UnigramTrainer() : Unigram토크나이저를 위한 트레이너 생성. 
- tokenizers.WordLevelTrainer() : WordLevel토크나이저를 위한 트레이너 생성. 
### 사전훈련 토크나이저 사용
- tokenizers.BertWordPieceTokenizer(vocab파일(txt)) : 파일을 기반으로 사전훈련된 Bert토크나이저 로드. encode등의 함수를 전부 사용가능. 

### 토큰화 파이프라인
- 파이프라인 : 정규화 -> 사전토큰화 -> 모델 -> 후처리 의 단계를 거침.
#### 정규화
- 정규화 : 원시 문자열을 보다 덜 무작위로 만들거나, 더 깔끔하게 일련의 작업. 공백제거/악센트가 있는 문자제거/소문자화 등이 포함됨. 유니코드 정규화 또한 매우 일반적인 정규화 작업임.
- normailzer = tokenizers.normalizers.Sequence([Normalizer 리스트\]) : 여러 정규화 작업을 결합한 노멀라이저 생성. 순차적으로 실행되며, 인스턴스(NFD()형식)으로 넣어야 함.
- normalizer.normalize_str(str) : 문자열 정규화. 
##### 노멀라이저
- tokenizers.normalizers.NFD() : NFD유니코드 정규화.
- tokenizers.normalizers.NFKD() : NFKD유니코드 정규화.
- tokenizers.normalizers.NFC() : NFC유니코드 정규화.
- tokenizers.normalizers.NFKC() : NFKC유니코드 정규화.
- tokenizers.normalizers.Lowercase() : 모든 대문자 소문자화.
- tokenizers.normalizers.Strip() : 입력의 지정된 측면(좌/우/둘다)의 모든 공백 제거.
- tokenizers.normalizers.StripAccents() : 악센트제거 정규화(일관성을 위해 NFD와 함께 사용).
- tokenizers.normalizers.Replace() : 문자열/정규표현식을 지정된 문자로 대체(Replace("a", "E")식으로 사용).
- tokenizers.normalizers.BertNormalizer() : 원래 BERT에서 사용된 정규화. clean_text/handle_chinese_chars/strip_accents/lowercase 옵션 설정가능.

#### 사전토큰화
- 사전토큰화 : 텍스트를 토큰(교육종료시 토큰이 무엇인지에 대한 상한선 제공)으로 분할하는 작업. 토큰으로 나눌 규칙(기준)을 정의함.
- preTokenizer = tokenizers.pre_tokenizers.Sequence([사전토크나이저 리스트\]) : 사전토크나이저를 여러개로 결합한 사전토크나이저 생성. 주어진 순서대로 실행.
- preTokenizer.pre_tokenize_str(str) : 문자열 전처리. 반환값은 [(토큰, (위치)), (토큰, (위치))\]의 형태로 되어있음.
##### preTokenizer
- tokenizers.pre_tokenizers.ByteLevel() : 모든 바이트를 보이는 문자집합으로 다시 매핑하는 동안 공백으로 분할. 바이트에 매핑되어 초기 알파벳으로 256자만 필요하고,
  256개의 토큰으로 무엇이든 나타낼 수 있어 OOV가 없음. 비 ASCII문자의 경우는 완전히 읽을 수 없지만, 그래도 작동함. 첫 단어를 제외하면 앞에 Ġ 가 붙게 됨.
- tokenizers.pre_tokenizers.Whitespace() : whiteSpace를 기준(구두점도 따로 분리됨)으로 토큰화. 단어경계에서 분할. `\w+|[^\w\s]+`의 정규식 사용.
- tokenizers.pre_tokenizers.WhitespaceSplit() : 모든 whiteSpace에서 분할(구두점은 그대로 유지).
- tokenizers.pre_tokenizers.Punctuation() : 모든 구두점을 분리함. 
- tokenizers.pre_tokenizers.Metaspace() : 공백으로 분할 후 특수문자로 바꿈("$A $B" -> "$A", "_$B"식).
- tokenizers.pre_tokenizers.CharDelimiterSplit() : 주어진 문자로 분할함.
- tokenizers.pre_tokenizers.Digits() : 다른 문자에서 숫자를 나눔("call 911!" -> "call", "911", "!"식).
- tokenizers.pre_tokenizers.Split() : 제공된 패턴과 동작에 따라 분할됨. pattern=(나눠질)문자열/정규표현식, invert=bool(True시 삭제될것과 존재할것이 반전됨(removed)),
  behavior=removed(구분문자 제거)/isolated(구분문자 독립토큰화)/merged_with_previous(앞쪽토큰에 병합)/merged_with_next(뒤쪽토큰에 병합)/contiguous(isolated)의 형식.

#### 모델
- 모델 : 입력이 정규화/사전토큰화 되면 모델적용. 학습한 규칙을 사용해 단어를 토큰으로 나누며, 해당 토큰을 vocab의 ID에 매핑함. unk_token="[UNK\]"등으로 특별토큰의 지정이 가능함.
##### models
- tokenizers.models.BPE() : BPE(바이트 페어 인코딩)모델 생성. 글자에서 시작해 가장 흔한 글자들을 병합해 새 토큰을 생성, 이를 반복. OOV의 가능성이 적고 vocab이 덜 필요.
- tokenizers.models.WordPiece() : 긴 단어에서 시작해 어휘에 없으면 분할. ##접두사를 사용해 단어의 일부인 토큰을 식별. BERT등에서 Google등이 사용하는 BPE와 유사한 알고리즘. 
- tokenizers.models.Unigram() : Unigram 모델. 하위단어 토큰화 알고리즘. 문장에 대한 확률을 최대화 하기 위해 최상의 하위 단어 토큰 세트를 식별하려 노력함.  
- tokenizers.models.WordLevel() : 고전적인 토큰화 알고리즘. 단순히 단어를 ID에 매핑. 사용과 이해가 간단하나 매우 큰 vocab이 필요하단 단점이 있음.

#### 후처리
- 후처리 : 반환 전 추가 변환을 수행하기 위한 토큰화 파이프라인의 마지막 단계. 후처리기는 변경 후 토크나이저를 재 훈련할 필요 없음.
##### post_processors
- tokenizers.processors.TemplateProcessing(single, pair, special_tokens) : 탬플릿에 따른 후처리 사용. 하나의 문장과 여러 문장을 위한 템플릿을 지정해 줘야 함. 
- single : "[CLS] $A(문장1) [SEP]" | pair : "[CLS] $A [SEP] $B(문장2) [SEP]" 형태로, 각 토큰이 쓰이는 형식. 각 토큰/문장표현에 :1 등을 붙여 typeID를 지정해 줄 수 있음. 
- special_tokens : [(토큰, id)\]형태로, 각 토큰과 해당 토큰의 id. tokenizer.token_to_id(토큰)를 사용해 더욱 확실히 할 수 있음.

# datasets | 데이터셋
- datasets : 데이터셋 로드/사용을 위한 허깅페이스 라이브러리. 허깅페이스 허브의 데이터셋을 사용할 수 있음.  
- 데이터셋 업로드 : 프로필 -> New Dataset에서 레포지토리 생성 -> Files and versions에서 파일(CSV/JSON/JSON라인/txt/Parquet지원)업로드 
  -> Dataset card에서 데이터셋 카드 제작. 이후 load_dataset으로 로드가능. 데이터가 비공개면 `huggingface-cli login ->  (use_auth_token=True)`로 가능.

- builder = datasets.load_dataset_builder(데이터셋) : 데이터세트의 정보를 얻기 위한 데이터셋 빌더 생성.
- builder.cache_dir : 데이터셋이 저장될 캐쉬폴더.
- builder.info.features : 데이터셋 각 features의 정보.
- builder.info.splits : 각 데이터셋의 크기(나눠진 후의)정보.
- datasets.get_dataset_config_names(데이터셋) : 여러 구성(데이터셋)으로 구성되는 일부 데이터셋 속 구성들의 정보를 반환. 
- datasets.get_dataset_split_names(데이터셋) : 데이터셋 또는 구성의 분할 이름을 반환.

- dataset.shape/num_columns/num_rows/column_names/features : 데이터셋의 데이터관련 속성들. 각 속성의 이름과 맞는 값을 가지고 있음. 이외에도 다양한 속성이 있음.
- dataset.info/split/description/citation/homepage : 데이터셋의 정보/데이터셋종류/설명/저작권정보/홈페이지.
- dataset.to_tf_dataset(columns, shuffle, batch_size, collate_fn) : 
- dataset.map(func) : 데이터셋을 형식화하는 방법. 함수는 `lambda examples: {'labels': examples['label']}`식이며, batched=True를 통해 배치설정이 가능. 
- dataset.set_format(type, columns) : 형식([각 데이터셋의 이름\]형식)설정 후 tensorflow/torch의 데이터셋으로 래핑.   
- dataset.cleanup_cache_files() : 캐시 디렉토리의 캐시파일을 정리함. 

- datasets.list_metrics() : datasets에서 지원하는 메트릭들의 리스트를 반환.
- metric = datasets.load_metric(데이터셋) : 데이터셋에 연결된 메트릭 로드. 가져오려는 데이터셋이 구성일 경우 (데이터셋, 구성)형태로 써줘야 함.
- metric.compute(predictions=pred_y, references=y) : 메트릭 계산.
- metric.inputs_description : 메트릭의 설명 확인 가능.

- datasets.load_dataset(데이터셋) : 데이터셋 로드. 불러오려는 데이터셋이 구성이라면 (데이터셋, 구성)형태로 써야 함. split="train"등으로 특정 데이터셋만 가져올 수 있음.

# gensim | word2vec, FastText
- gensim : 통계적의미론에 초점이 맞춰져, 문서의 구조를 분석한 후 유사성을 기준으로 다른 문서에 점수를 주는
  W2V, D2V, FastText, LDA등과 많이 사용하는 알고리즘에 최적화 되있는 모듈.  
- gensim.models.Word2Vec(sentences, size, window, min_count, workers, sg) : WordToVector학습. 
  size(임베딩 벡터 차원), window(윈도우 크기), min_count(최소빈도수), workers(프로세스 수), sg(0-CBOW, 1-Skip_gram)등의 매개변수 사용가능.
- gensim.models.FastText(corpus, size, window, min_count, workers, sg) : FastText학습. 한국어에서 이걸 사용하려면 음절단위가 아닌 자모(ㄱ,ㅏ,ㄴ,ㅓ)단위로 사용함. 

- model.get_vector(token) : token의 wordVector반환. token이 vocab에 없다면 KeyError발생. word_vec()도 같은 역할을 함.
- model.get_index(token) : token의 vocab내 인덱스를 반환. 
- model.similarity(w1, w2) : 두 단어간 유사도 계산.
- model.wv : 모델의 wordVector확인. {토큰: 벡터}형태의 딕셔너리로 되어있음. 
- model.wv.most_similar(word) : word와 유사한 단어들, 유사도(확률)출력.

- model.save(모델명.bin) : 모델저장.
- model.wv.save(모델명.wordvectors) : 모델에서 생성된 벡터 저장.
- model.wv.save_word2vec_format(name) : 모델에서 생성된 벡터 W2V형식으로 저장.

- gensim.models.Word2Vec.load(모델명) : 모델로드.
- gensim.models.KeyedVectors.load(모델명.wordvectors) : 모델에서 생성된 벡터 로드.
- gensim.models.KeyedVectors.load_word2vec_format(path) : 저장된 text/bin형식의 모델에서 모델벡터(wv)로드.

- gensim.downloader.load(모델명) : 사전훈련된 모델 로드. 
- gensim.downloader.info() : 다운로드 가능한 것들의 정보 확인. list(gensim.downloader.info()['models'\].keys())로 모델의 정보만 확인 가능.

- [google_W2V](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) : 구글 제공 3백만개의 사전훈련된 W2V 단어 벡터 로드(model.bin). 

- [embedding_Projector](https://projector.tensorflow.org/) : 임베딩 벡터 시각화 사이트. `!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름` 
  명령어를 사용해 모델명_metadata.tsv파일과 모델명_tensor.tsv 파일을 생성한 후 사용할 수 있음.

# SentencePiece | subword
- sentencepiece : BPE를 포함한 기타 서브워트 토크나이징(내부단어분리)알고리즘 내장 패키지. 사전 토큰화 작업 없이 단어분리 토큰화를 수행해 언어무관 사용가능.
  
- sentencepiece.SentencePieceTraner.Train() : 서브워드 토큰화. 단어 집합과 각 단어를 정수 인코딩. 
  결과로 설정파일명.model 과 파일명.vocap 의 파일이 생성, vocab 파일에서 학습된 서브워드들을 확인할 수 있음.
- 사용가능 매개변수 : input(학습시킬 파일), model_prefix(만들어질 파일(모델) 이름), 
  vocab_size(단어 집합의 크기), model_type(사용할 모델 (unigram(default), bpe, char, word)).
- 사용가능 매개변수 : pad_id, pad_piece(pad token id, 값), unk_id, 
  unk_piece(unknown token id, 값), os_id, bos_piece(begin of sentence token id, 값), eos_id, eos_piece(end of sequence token id, 값).
- 사용가능 매개변수 : max_sentence_length(문장의 최대 길이), user_defined_symbols(사용자 정의 토큰).

- sentencepiece.SentencePieceProcessor() : 센텐스피스 프로세서 로드. .load(파일명.model)로 저장한 모델 로드.
- 모델 메서드 : .encode_as_pieces(line)로 문장을 서브워드 시퀀스로, .encode_as_ids(line)로 문장을 정수 시퀀스로 변경할 수 있음.
- 모델 메서드 : .GetPieceSize()(단어집합크기), .idToPiece(int)(정수 > 서브워드), .PieceToId(subword)(서브워드 > 정수)
- 모델 메서드 : .DecodeIds(int list)(정수시퀀스>문장), .DecodePieces(서브워드 시퀀스)(서브워드시퀀스>문장), .encode(문장, output=str/int)(문장 > 서브워드/정수 시퀀스) 

# glove | GloVe
- glove : pip install glove_python 으로 다운로드 가능. 워드 임베딩의 방법 중 하나인 glove를 사용할 수 있음.
- glove.Corpus() : 글로브 동시 등장 행렬 생성기 로드. .fit(corpus, window)로 사용가능. 
- glove.Glove(no_components, learning_rate) : GloVe를 수행 클래스 로드. .fit(등시등장행렬.matrix, epochs, no_threads, verbose)로 사용가능.
- glove 메서드 : .add_dictionary(동시등장행렬.dictionary) > 사전 추가 | .most_similar(word) > 비슷한 단어들과 유사도 반환. 

# NLTK | NLP(영어토큰화, 전처리 도구들)
- nltk.download() : NLTK 세트 다운로드. 특정 세트의 이름을 넣으면 그것만 다운로드한다.
- nltk.Text(tokens) : 토큰들을 다시 문장(iter가능)화. .tokens{토큰확인}, .plot(), 
  .concordance(word){비슷한단어추출}, .vocab(){단어빈도수체크, .most_common(i)사용가능.} 

### tokenize
- nltk.tokenize.sent_tokenize(text) : 문장 토큰화함수. 문서를 문장단위로 나눠준다. 
  PunktSentenceTokenizer인스턴스(문장의 시작과 끝을 표시하는 문자나 문장 기호에 기초해 다른 유럽언어로 토큰화를 수행)를 사용해 punkt의 다운로드를 필요로 함. 
- 영어가 아닌 언어를 토큰화 하려면 'tokenizer/punkt/언어.pickle' 파일을 로드하고 사용하면 된다. 로드한 언어.pickle에
  .tokenize(text) 매서드를 사용해서도 토큰화를 사용할 수 있다.
  
- nltk.tokenize.word_tokenize(sentence) : 단어 토큰화 함수. 문장을 단어 단위로 나눠주며 축약형의 경우 단어의 의미가 유지되게(do , n't)분리한다. 
  TreebankWordTokenizer 를 사용한다.
- nltk.tokenize.sent_Tokenizer(text) : 문장 토큰화 함수. 문장 내부에 구두점이 포함되어 있어도 잘 작동한다.
- nltk.tokenize.WordPunctTokenizer() : 또 다른 단어 토크나이저. 구두점을 별도로 분리한다. 축약형의 경우 '전, ', '후 총 세가지로 나눈다.
- nltk.tokenize.TreebankWordTokenizer() : 트리뱅크워드 토큰화 함수 로드. .tokenize(Sentence) 로 토큰화를 수행할 수 있다. 분리된 축약형('Do', 'n't')으로 작동된다. 
- nltk.tokenize.WhitespaceTokenizer() : 화이트 스페이스(탭 제외?) 으로 단어 토큰화. 토크나이저.span_tokenize(sent)로 토큰의 오프셋인 튜플의 순서를 받을 수 있다.
- nltk.tokenize.util.string_span_tokenize(문자열, "separator(구분자)") : 구분자대로 분할해 전송된 토큰의 오프셋을 반환.

- nltk.tokenizer.RegexpTokenizer(정규 표현식) : 정규 표현식을 이용한 단어 토큰화 클래스 로드. .tokenize(String)으로 사용할 수 있다. 
  gaps = True 로 화이트 스페이스를 사용한 토큰화를 할 수 있다. 
- nltk.tokenizer.regexp_tokenize(sentence, patten='정규 표현식') : 정규 표현식 단어 토큰화 함수 로드. 

- nltk.ngrams(words, i) : i-gram으로 words내의 단어들을 튜플형태로 만들어 반환해줌.

### stem
- nltk.stem.WordNetLemmatizer() : 표제어 추출기 생성. .lemmatize(word)로 표제어 추출 사용 가능. 정확한 추출을 위해선 (word, 품사)식으로 넣어주어야 한다.
- nltk.stem.PorterStemmer() : PorterStemmer 알고리즘의 어간 추출기 생성. .stem(토큰.norm_.lower()) 으로 어간(용언에서 뜻을 나타내는 불변인 부분)을 찾음.
- nltk.stem.LancasterStemmer() : Lancaster 어간 추출기 로드. .stem(word) 로 사용할 수 있다.

### Freq
- nltk.FreqDist(단어집합) : 단어를 키로, 빈도수를 값으로 저장해 리턴. 결과.most_common(i)로 상위 i개의 단어만 보존가능. 아래와 똑같은 효과를 지님.
- collections.Counter(단어집합) : 단어 집합에서 중복을 제거하고 단어의 모든 빈도를 쉽게 계산함. 이 메서드로 간단하게 등장단어와 빈도 파악이 가능.

- nltk.tag.pos_tag(토큰 리스트) : 품사 태깅을 수행. (단어, 품사)의 형태로 나타남. PRP(인칭대명사),VBP(동사),RB(부사),VBG(현재부사),IN(전치사),NNP(고유명사),
  NNS(복수명사),CC(접속사),DT(관사)를 의미함.
- nltk.ne_chunk(품사태깅([(토큰,품사)]형태)) : 개체명 인식을 수행. 개체명 인식이 된것(명사(NNP)만 인식)은 ()로 묶여있고, 
  최종(S (개체명, 단어) 단어 단어 단어 (개체명, 단어))식으로 배치된다.

### corpus
- nltk.corpus.stopwords : 불용어 처리 클래스 로드. .words('언어')로 불용어 리스트를 받아올 수 있으며 if w not in words 롤 거를 수 있고, 
  .fileids()로 불용어 목록이 있는 언어를 확인할 수 있다. 한국어 등은 직접 txt 파일등에 목록을 만들어 제거하는게 편함.
- nltk.corpus.wordnet : 프린스턴 대학의 동의어 집합 세트. synsets(word).definition()으로 단어의 유사어 확인 가능, .path_similarity(synsets)로 단어의 유사도 확인 가능. 
- nltk.corpus.alpino : 알피노 코퍼스(네덜란드 신문에 나오는 단어 모음) 로드. .word()로 내부의 단어들을 꺼낼 수 있다. 다른 것들도 사용가능. 
  전부 nltk.download(corpus)뒤에 사용 가능하다.

### metrics
- nltk.metrics.accuracy(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정확도(같은 정도)반환.
- nltk.metrics.precision(sentence1, sentence2) : 두 토큰화된 단어 리스트의 정밀도(TP/(TP+FP))반환.
- nltk.metrics.recall(sentence1, sentence2) : 두 토큰화된 단어 리스트의 재현율(TP/(TP+FN))반환.
- nltk.metrics.f_measure(sentence1, sentence2) : 두 토큰화된 단어 리스트의 f1점수(정밀도와 재현율의 조화 평균(역수의 평균의 역수,곱/합))반환.

- nltk.metrics.edit_distance(word1, word2) : 두 단어간 편집거리 반환.
- nltk.metrics.jaccard_distance(set1, set2) : 두 세트간 자카드 계수 반환.
- nltk.metrics.binary_distance(set1, set2) : 두 세트간 이진거리 계수 반환.
- nltk.metrics.masi_distance(set1, set2) : 두 세트간 매시거리 계수 반환.

### ngrams
- nltk.util.ngrams(단어 리스트, n) : n개의 토큰이 연결되어 있는 n그램 생성. 
- nltk.collocations.BigramCollocationFinder : 바이그램 탐색기. .from_words(tokens)로 토큰을 저장할 수 있다.
- 토큰저장탐색기.nbest(nltk.metrics.BigramAssocMeasures.likelihood_ratio, n) : n개의 바이그램을 찾아 리스트를 받아볼 수 있다.
- 토큰저장탐색기.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq) : 바이그램을 찾는 또다른 방법.
- nltk.probability.LidstoneProbDist(fd, gamma=f, bins = n) = 최대 우도 추정 사용. fd(빈도분포)를 기반으로 f(0~1)를 사용해 n개의 샘플을 생성. 샘플들의 총 합은 1. 

### translate
- nltk.translate.bleu_score(candidate.split(), refrences.split()) : BLEU score 측정. 

# SpaCy | 토큰화(영어)
- SpaCy : 산업에 강한 NLP. 속도와 모델크기, 정확성 측면에서 다른 라이브러리를 앞선다고 주장. 여러 언어용 모델이 포함 되어 있으나 완전한 건 16개.
- spacy.load(언어) : 해당 언어의 처리를 위한 sapcy로드. 언어는 'en'등으로 사용.
- 로드된spacy.tokenizer(sent) : 문장을 단어단위로 토큰화. 각 토큰들은 .text 로 문자열 형태로 받을 수 있음.

# TextBlob | NLTK, Pattern
- TextBlob : Pattern과 NLTK라이브러리의 친화적 프론트엔드. 고수준의, 사용하기 쉬운 인터페이스로 포장함.
- Pattern : 인기 웹서비스와 소스를 스크래핑 할 수 있는 도구 탑재. 직접 NLP함수, n-gram검색, 벡터, 그래프등도 사용할 수 있으며, DB를 다룰 수 있는 헬퍼라이브러리도 내장됨.

# replacers | 텍스트 대체, 반복 삭제
- replacers.RegexpReplacer() : 텍스트 대체 클래스 로드. .replace(text)로 사용, 축약을 해제하고 단어 토큰화까지 진행해 리스트로 반환.
- replacers.RepeatReplacer() : 반복 문자 삭제 클래스 로드. 위와 동일하게 사용할 수 있으며, 반복된 단어를 일반 단어로 바꿔 반환한다. 
  nltk 의 wordnet.synsets(word)에 이미 있다면 처리하지 않도록 하면 일반 단어는 반복을 삭제하지 않는다.
- replacers.WordReplacer({'바꿀단어':'바뀔단어'}) : 단어를 동의어로 변환하는 클래스 로드. 마찬가지로 사용, 목록에 있는 단어는 바꿔서, 아니면 그대로 반환한다.

# inflect | 숫자 영어로 변환
- inf = inflect.engine() : inflect 엔진 생성.
- inf.number_to_words(number) : 숫자를 영어로 변환.

# unidecode | 다른 언어 영어 발음대로 변환
- unidecode.unidecode(other_lang) : 다른 언어를 발음대로 영어로 변환함.

# KoNLpy | 한글 분석(토큰화, 태깅)
- konlpy : 한글 분석을 가능하게 함. 자바로 이뤄져 있어 JDK 1.7 이상과 JPype 가 설치되어 있어야 함. 각 분석기는 성능과 결과가 다르게 나와 용도에 따라 적절한 것을 선택해야 함.
  python 3.9이상에선, JPype1-py3를 설치한 후 jvm.py에서 `convertStrings=True(line 64)`를 지워야 사용할 수 있다.
- 형태소 분석기 종류 : 메캅(MeCab), Okt(Open Korea Text(Twitter)), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)등의 형태소 분석기 사용 가능.
- 함수 : konlpy.tag.분석기명()으로 분석기사용, .morphs(text){토큰화}, .pos(text){토큰화 후 품사 태깅(.tagset으로 종류확인)}, .nouns(text){명사만 추출}에 
  Okt.phrases(text){구문별로 나눔}, Kkma.sentences(text){문장별로 나눔}, 한나눔.analyze(text){형태소후보 모두반환}등이 사용가능.
- 매개변수 : .pos(), .morphs() 사용시 norm=bool(일정 수준의 정규화), stem=bool(표제어(원본글자)로 변형)등의 매개변수를 사용할 수 있음.
  
- MeCab : 띄어쓰기에서 속도/정확도 모두 뛰어남. 지능형 형태소 분석기(결과 수작업 수정가능), 단어 추가가능. 성능이 가장 뛰어나나 사용이 번거롭단 문제가 있음.
  C/C++로 개발, CRF채용. mecab폴더는 무조건 C에 있어야(C:\mecab)에 있어야 함.
  [기본사전](https://github.com/Pusnow/mecab-ko-dic-msvc/releases/tag/mecab-ko-dic-2.1.1-20180720-msvc) 과
  [Mecab실행기](https://github.com/Pusnow/mecab-ko-msvc/releases/tag/release-0.9.2-msvc-3) 를 다운로드 받고,
  [Mecab python_wheel](https://github.com/Pusnow/mecab-python-msvc/releases/tag/mecab_python-0.996_ko_0.9.2_msvc-2) 에서 맞는 버전(3.7이 최신)을 설치 후
  [pip install whl파일] > [import MeCab] 혹은 [import konlpy.tag.Mecab > mecab = Mecab(mecab-ko-dic파일경로)]로 Mecab의 사용이 가능함.
- Okt : 띄어쓰기에서 가장 좋은 성능, 정제되지 않은 데이터에 대해 강점. 분석 범주가 다소 적으나 이모티콘/해쉬태그 등 인터넷텍스트에 특화된 범주가 추가.
  어근화, 정규화, 토큰화등이 가능, 미등록어 처리/등음이의어처리/분석범주적음 등의 문제가 있음. 스칼라/java로 개발.    
- Kkma : 띄어쓰기 오류에 덜 민감함. 분석시간이 꽤 길고, 정제된 언어가 사용되지 않는 문서에 대한 정확도가 낮음. 세종품사태그에 가장 가깝고, 분석범주 또한 다양.
  java로 개발, 동적프로그래밍을 이용해 모든 형태소분석 후보를 생성 후 적합한 순서대로 정렬, 기분석 사전을 이용한 인접조건검사방식(속도)과 HMM에 기반한 확률모델(품질)이용 최적화.
- Komoran : 빠른 속도와 보통의 분석품질. 여러 어절을 하나의 품사로 분석가능해, 공백이 포함된 고유명사를 더 정확히 분석가능. 개발자가 지속적으로 업데이트.
  자소분리/오탈자 문장에 대해서도 괜찮음. 로딩시간이 기나 분석속도는 빠름. 띄어쓰기 없는 문장엔 취약. java로 개발, HMM알고리즘 사용.
- Hannanum : 로딩시간이 빠른편. 전체시스템이 각 모듈들의 조합(입력필터/문장분리/형태소분석/미등록어처리/형태소분석후처리/태거)으로 구성됨. 
  띄어쓰기 없는 문장과 정제된 언어가 사용되지 않는 문서에 대한 정확도가 낮음. 상위 6개 태그에 대해 20개의 태그를 세분화해 사용. java로 개발, HMM알고리즘 사용.


### ckonlpy
- `pip install customized_konlpy` : 사용자사전 추가를 위한 패키지 설치. 
- 클래스 : konlpy와 같은 .tag.Twitter()등을 사용해 형태소분석기 등을 사용할 수 있음.
- 형태소분석기.add_dictionary(word, 품사) : 형태소분석기 사전에 단어 추가.

# PORORO | (?)
- (?)

# soynlp | 반복 제어, 품사태깅, 단어 토큰화
- SOYNLP : 품사 태깅, 단어 토큰화 등을 지원. 비지도 학습으로 토큰화. 어느정도 규모가 있으면서 비슷한 문서집합에서 잘 작동.
- 형태소 분석기 : 데이터의 통계량을 확인해 만든 단어점수표로 작동. 응집확률(문자열이 유기적으로 연결되 자주 등장)과 브랜칭 엔트로피(앞뒤로 다양한 단어 등장)를 사용함.
- soynlp.DoublespaceLineCorpus("txt파일.txt") : 데이터를 다수의 문서로 분리함.
- soynlp.normalizer.emoticon_normalize(sent, num_repeats=i) : ㅋㅋ,ㅎㅎ 등의 이모티콘을 i개 까지만 반복되도록 변환.
- soynlp.normalizer.repeat_normalize(sent, num_repeats=i) : 의미없이 반복되는 글자를 i개 까지만 반복되도록 변환.

# khaiii | Khaiii 한글 분석기 사용
- khaiii : 카카오가 제작한 한글 분석기. [자세한 내용](https://github.com/kakao/khaiii)
- tokenizer = khaiii.KhaiiiApi() : 카이 토크나이저 로드.
- tokenizer.analyze(sent) : 문장분석. 반환값의 요소를 빼내어 word.morphs로 토큰/품사 형태의 결과를 볼 수 있다(str으로 변환 필요).

# hgtk | 한국어 자소단위로 쪼개기
- 공식 : Code = 0xAC00(가(유니코드)) + (초성 인덱스 * 중성개수 * 종성개수) + (중성 인덱스 * 종성 개수) + (종성 인덱스)
- hgtk.text.decompose(sent, compose_code) : 문장을 자소단위로 쪼갬. 글자간 경계 문자는 생략시 기본값인 ᴥ가 사용됨.
- hgtk.text.compose(sent, compose_code) : 자소단위의 문장을 다시 합침. compose_code에 넣은 문자는 글자간 경계 문자를 알려주는 역할.

# 한국어 전처리 | 띄어쓰기, 맞춤법, 문장 토큰화
- PyKoSpacing : 한국어 띄어쓰기 패키지. 띄어쓰기가 없는 문장을 띄어쓰기를 한 문장으로 변환해줌. pykospacing.spacing(문장)으로 사용할 수 있음. 
- Py-Hanspell : 네이버 맞춤법 검사기를 바탕으로 제작된 맞춤법(띄어쓰기 포함)보정 패키지. hanspell.spell_checker.check(문장).checked 로 개선된 문장을 볼 수 있음.
- kss.split_sentences(text) :  kss 모듈의 한국어 문장 토큰화 함수. 


# re | 정규표현식
- 정규 표현식 사용을 지원.
- 정규표현식
> [] : []안의 문자들 중 하나와 매치. [abc]면 a,b,c중 하나와 매치를 뜻함. [a-c],[a-zA-Z]식으로 범위를 지정할 수 도 있고, 
> []안, 패턴 앞에 ^ 를 붙혀 패턴 부정을 나타낼 수 도 있음. []가 없으면 그 패턴과 정확히 일치.
> 특수 문자 : .(임의 문자 1개), ?(앞 문자 0 또는 1개), *(앞 문자 0개 이상), +(앞 문자 1개 이상), ^(뒤 문자로 문자열 시작), $(앞 문자로 문자열 종료). 
> 패턴의 바로 앞/뒤에 위치해야 함.
> 메타문자 : 정규 표현식 내에서 특수한 역할을 하는 문자. 앞에 \를 붙여야 패턴으로 인식됨. |`$ *+.?([\^{ | 총 12개. []안에선 | \^-] | 만이 메타문자.
> 특수기호 : \d(숫자), \D(숫자가 아닌것), \s(whitespace, \t\n\r\f\v), \S(부정), \w(문자+숫자), \W(문자+숫자의 부정), \b(단어경계(단어-비단어)), \B(부정,(단어-단어|비단어-비단어))등이 있다.
> 범위 : 문자{n,m}은 n번 부터 m이하 반복, {n}은 반드시 n번 반복으로 사용된다. a|b|c 는 여러 문자중 하나에 매칭임. 
> 캡쳐그룹 : 패턴을 ()로 감싸면 캡쳐그룹으로 만듦. \1,\2 등으로 그 순서의 그룹을 사용 가능하며, 매칭되지 않았다면 반환하지 않음. 사용하려면 r''으로 사용해야 한다(그렇지 않으면 이상한 값이 반환됨.

- re.compile() : 정규 표현식 컴파일. 결과 객체 반환. re.S(.이 \n을 포함하게 함) 등을 매개변수로 줄 수도 있다.
- re.sub('패턴', 바꿀문자, 바뀔 문장) : 바뀔 문장에서 패턴에 일치하는 부분을 바꿀 문자로 바꿈. 바꿀 문자를 함수로 줄 수 도 있으며, 해당 함수에는 Match객체가 들어가게 됨. 매치된 모든 부분이 함수에 들어갔다 나오게 됨.
- re.subn() : sub와 같은 기능을 하지만, 튜플((new_string, number_of_subs_made))을 반환함.
- re.match(패턴, 문자열) : 컴파일을 거치지 않고 매치 사용. .group()으로 매치된 문자열을 볼 수 있다. 
- re.search(패턴, 문자열) : match 와 동일하지만 문장의 경우 문장 전체를 검색한다. .start()로 시작 위치, .end()로 끝 위치, .span()으로 (시작, 끝)을 받을 수 있다.
- re.split(패턴, 문자열) : 정규 표현식을 기준으로 문자열을 구분해 리스트로 리턴.
- re.findall(패턴, 문자열) : 정규 표현식에 맞는 단어만 리스트로 반환. 
- re.finditer(패턴, 문자열) : 정규 표현식에 맞는 단어만 각각을 매치 객체의 형태로 하여 반복 가능한 개채의 형태로 돌려준다. 
- re의 메서드는 전부 re.compile(표현식)후 컴파일.메서드(문자열) 형태로 사용할 수 있다.


